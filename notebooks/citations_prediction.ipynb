{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook to predict citations count\n",
    "* Using arXiv data\n",
    "* Using pubmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pysrc.config import *\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.prediction.predict_analyzer import PredictAnalyzer\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "\n",
    "loader = PubmedPostgresLoader(config)\n",
    "# Analyzer configures progress in loader,\n",
    "analyzer = PredictAnalyzer(loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = analyzer.search(limit=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.analyze(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create balanced dataset with respect to cytations count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Cytations histogram\n",
    "analyzer.df['total'].hist(bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Balance dataset of papers without any citations and with citations.\n",
    "papers_df_not_null = analyzer.df[analyzer.df.total > 0]\n",
    "papers_df_null = analyzer.df[analyzer.df.total == 0].head(n=10000)\n",
    "papers_df = pd.concat([papers_df_not_null, papers_df_null]).drop(columns=['aux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Balanced cytations histogram\n",
    "papers_df['total'].hist(bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram by year\n",
    "papers_df['year'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Filter only 1975-2015 years\n",
    "papers_df_years = papers_df[np.logical_and(1975 <= papers_df.year, papers_df.year <= 2015)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add topics using LDA algorithm to dataframe only based on texts, without graph information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "topic_names = [f'topic{i}' for i in range(n_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_year, end_year = 1995, 2016  # end year exclusive\n",
    "topics_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_topic(row):\n",
    "    index = np.argmax(np.array(row))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pysrc.papers.analysis.text import vectorize_corpus\n",
    "\n",
    "\n",
    "# Note: this method was restored in papers.utils to restore notebook functionality.\n",
    "# TODO: Refactor with newer code.\n",
    "def lda_topics_df(df, n_words, n_topics):\n",
    "    logging.info(f'Building corpus from {len(df)} articles')\n",
    "    corpus, corpus_tokens, corpus_counts = vectorize_corpus(\n",
    "            df,\n",
    "            max_features=VECTOR_WORDS,\n",
    "            min_df=VECTOR_MIN_DF,\n",
    "            max_df=VECTOR_MAX_DF\n",
    "        )\n",
    "\n",
    "    logging.info(f'Performing LDA topic analysis')\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(corpus_counts)\n",
    "\n",
    "    topics = lda.transform(corpus_counts)\n",
    "    logging.info('Done')\n",
    "    return corpus_tokens, corpus_counts, topics, lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def explain_lda_topics(corpus_tokens, corpus_counts, lda, n_top_words=20):\n",
    "    explanations = {}\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        explanations[i] = [(topic[i], corpus_tokens[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Find topics for year {year}\")\n",
    "    corpus_tokens, corpus_counts, topics, lda = lda_topics_df(\n",
    "        papers_df_years[papers_df_years.year <= year], n_words=1000, n_topics=n_topics\n",
    "    )\n",
    "\n",
    "    topics_df = pd.DataFrame(data=topics, columns=topic_names)\n",
    "    topics_df.index = papers_df_years[papers_df_years.year <= year].index\n",
    "    topics_df['main_topic'] = topics_df[topic_names].apply(lambda row: find_topic(row), axis=1)\n",
    "\n",
    "    explanations = explain_lda_topics(corpus_tokens, corpus_counts, lda, n_top_words=20)\n",
    "\n",
    "    topics_info[year] = {'topics': topics_df, 'lda': lda, \n",
    "                         'corpus_tokens': corpus_tokens, 'corpus_counts': corpus_counts, \n",
    "                         'explanations': explanations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_info[2000]['explanations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# count citations before each year (including this year)\n",
    "def before_year_citations(df):\n",
    "    df[f'before_{start_year}'] = df[start_year]\n",
    "    for year in range(start_year + 1, end_year):\n",
    "        df[f'before_{year}'] = df[f'before_{year - 1}'] + df[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "before_year_citations(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topic_citations = {}\n",
    "topic_ranks = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Counting topics citations for year {year}\")\n",
    "    # count citations of each topic\n",
    "    topic_citations[year] = []\n",
    "    for i in range(n_topics):\n",
    "        p_topic_i = topics_info[year]['topics'][f'topic{i}']\n",
    "\n",
    "        cit_documents = papers_df_years[papers_df_years.year <= year][f'before_{year}']\n",
    "        assert p_topic_i.shape[0] == cit_documents.shape[0]\n",
    "\n",
    "        topic_citations[year].append(np.dot(p_topic_i, cit_documents))\n",
    "    topic_ranks[year] = pd.Series(topic_citations[year]).rank(ascending=False, method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_topic_rank(row):\n",
    "    df = topics_info[row.year]['topics']\n",
    "    main_topic = df.loc[row.name, :]['main_topic']\n",
    "    topics_rank = topic_ranks[row.year][main_topic]\n",
    "    return topics_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "papers_df_years['topic_rank'] = papers_df_years[papers_df_years.year >= 1995].apply(lambda row: get_topic_rank(row),\n",
    "                                                                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_diversity(row):\n",
    "    topic_probs = topics_info[row.year]['topics'].loc[row.name, :][topic_names]\n",
    "    return np.dot(list(topic_probs), np.log(list(topic_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "papers_df_years['diversity'] = papers_df_years[papers_df_years.year >= 1995].apply(lambda row: get_diversity(row),\n",
    "                                                                                   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing (aurhors and journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install scholarmetrics pbr\n",
    "from statistics import mean\n",
    "from scholarmetrics import hindex, gindex\n",
    "# An h-index of x means that the author has at least x publications that have been cited at least x times.\n",
    "# An g-index of x means that the author’s top x publications together accumulated at least x2 citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def citations_after_n_years(row, n):\n",
    "    paper_year = row['year']\n",
    "    cit = 0\n",
    "    for cur_year in range(paper_year, paper_year + n):\n",
    "        if cur_year in row:\n",
    "            cit += row[cur_year]\n",
    "    return cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def journal_rank_and_mean(df):\n",
    "    journals_citations_years = pd.DataFrame()\n",
    "    for year in range(start_year, end_year):\n",
    "        journals_citations_cur = df[df.year <= year][['journal', 'total']].groupby(['journal']).agg(\n",
    "            {'total': 'mean'}).reset_index().rename(columns={'total': 'journal_citations'})\n",
    "\n",
    "        journals_citations_cur['journal'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "        journals_citations_cur.dropna(subset=['journal'], inplace=True)\n",
    "\n",
    "        journals_citations_cur['rank'] = journals_citations_cur['journal_citations'].rank(ascending=False,\n",
    "                                                                                          method='min')\n",
    "        journals_citations_cur['year'] = year\n",
    "        journals_citations_years = pd.concat([journals_citations_years, journals_citations_cur], axis=0)\n",
    "\n",
    "    return journals_citations_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_journals_graph(df, cit_df):\n",
    "    with_journal_out = pd.merge(cit_df, df[['id', 'journal']], left_on='id_out', right_on='id').rename(\n",
    "        columns={'journal': 'journal_out'}).drop(columns=['id'])\n",
    "\n",
    "    journal_edges = pd.merge(with_journal_out, df[['id', 'journal']], left_on='id_in', right_on='id').rename(\n",
    "        columns={'journal': 'journal_in'}).drop(columns=['id'])[['journal_out', 'journal_in']]\n",
    "    # clear empty journals\n",
    "    journal_edges.replace({'': np.nan}, inplace=True)\n",
    "    journal_edges.dropna(inplace=True)\n",
    "\n",
    "    journal_edges = journal_edges.groupby(['journal_out', 'journal_in']).size().reset_index(name='weight')\n",
    "\n",
    "    # build graph\n",
    "    journal_graph = nx.from_pandas_edgelist(journal_edges, 'journal_out', 'journal_in', 'weight')\n",
    "\n",
    "    return journal_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pagerank_journals = {}\n",
    "pagerank_journals_df = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Count pagerank of each journal for year {year}\")\n",
    "    journals_graph = build_journals_graph(papers_df_years[papers_df_years.year <= year], analyzer.cit_df)\n",
    "    pagerank_journals[year] = nx.pagerank(journals_graph, alpha=0.85)\n",
    "    pagerank_journals_df[year] = pd.DataFrame([pagerank_journals[year]]).transpose().reset_index()\n",
    "    pagerank_journals_df[year].columns = ['journal', 'pagerank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "journals_citations_years = journal_rank_and_mean(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.metadata import split_df_list\n",
    "\n",
    "\n",
    "def build_authors_graph(df, cit_df):\n",
    "    authors_df = df[['authors', 'id']]\n",
    "    authors_df['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    authors_df.dropna(subset=['authors'], inplace=True)\n",
    "\n",
    "    authors_df = split_df_list(authors_df, target_column='authors', separator=', ').rename(\n",
    "        columns={'authors': 'author'})\n",
    "\n",
    "    with_author_out = pd.merge(cit_df, authors_df, left_on='id_out', right_on='id').rename(\n",
    "        columns={'author': 'author_out'}).drop(columns=['id'])\n",
    "\n",
    "    author_edges = pd.merge(with_author_out, authors_df, left_on='id_in', right_on='id').rename(\n",
    "        columns={'author': 'author_in'}).drop(columns=['id'])[['author_out', 'author_in']]\n",
    "\n",
    "    # clear empty authors\n",
    "    author_edges.replace({'': np.nan}, inplace=True)\n",
    "    author_edges.dropna(inplace=True)\n",
    "\n",
    "    author_edges = author_edges.groupby(['author_out', 'author_in']).size().reset_index(name='weight')\n",
    "\n",
    "    # build graph\n",
    "    author_graph = nx.from_pandas_edgelist(author_edges, 'author_out', 'author_in', 'weight')\n",
    "\n",
    "    return author_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Count pagerank for graph of authors citation and productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "author_graph_features = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Started counting graph of authors citations for year {year}\")\n",
    "    authors_graph = build_authors_graph(papers_df_years[papers_df_years.year <= year], analyzer.cit_df)\n",
    "    author_graph_features[year] = nx.pagerank(authors_graph, alpha=0.85)\n",
    "\n",
    "    author_graph_features[year] = {k: {'pagerank': v, 'productivity': 0}\n",
    "                                   for k, v in author_graph_features[year].items()}\n",
    "    for author, _, weight in nx.selfloop_edges(authors_graph, data='weight'):\n",
    "        author_graph_features[year][author]['productivity'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def author_features(df):\n",
    "    author_total = df[['authors', 'total', 'year']]\n",
    "    author_total['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_total.dropna(subset=['authors'], inplace=True)\n",
    "    author_total['co_authors'] = author_total['authors'].apply(lambda authors: len(authors.split(', ')) - 1)\n",
    "\n",
    "    author_total = split_df_list(author_total, target_column='authors', separator=', ')\n",
    "\n",
    "    authors_dict_years = {}\n",
    "    for year in range(start_year, end_year):\n",
    "        logging.info(f\"Started counting authors ranks and mean number citations for year {year}\")\n",
    "        authors_citations_groupped = author_total[author_total.year <= year].groupby(['authors'])\n",
    "        authors_citations = authors_citations_groupped.agg(\n",
    "            {'total': ['mean', hindex, gindex], 'co_authors': 'mean'}).reset_index()\n",
    "        authors_citations.columns = authors_citations.columns.droplevel(level=1)\n",
    "        authors_citations.columns = ['author', 'total', 'hindex', 'gindex', 'co_authors']\n",
    "\n",
    "        authors_citations = authors_citations.loc[authors_citations['author'] != '']\n",
    "        authors_citations['rank'] = authors_citations['total'].rank(ascending=False, method='min')\n",
    "        cur_authors_dict = authors_citations.set_index('author')[\n",
    "            ['total', 'rank', 'hindex', 'gindex', 'co_authors']].to_dict(orient='index')\n",
    "\n",
    "        authors_dict_years[year] = cur_authors_dict\n",
    "\n",
    "    return authors_dict_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "authors_dict_years = author_features(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None, None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    default_features = {'rank': None, 'hindex': None, 'gindex': None, 'co_authors': None}\n",
    "    features_of_given_authors = list(map(lambda author: authors_dict_years[year][author]\n",
    "    if author in authors_dict_years[year] else default_features,\n",
    "                                         authors_list))\n",
    "\n",
    "    ranks = filter(None.__ne__, map(lambda features: features['rank'], features_of_given_authors))\n",
    "    hindexes = filter(None.__ne__, map(lambda features: features['hindex'], features_of_given_authors))\n",
    "    gindexes = filter(None.__ne__, map(lambda features: features['gindex'], features_of_given_authors))\n",
    "    socialities = filter(None.__ne__, map(lambda features: features['co_authors'], features_of_given_authors))\n",
    "\n",
    "    return pd.Series([mean(ranks), mean(hindexes), mean(gindexes), mean(socialities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors_graph_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "\n",
    "    default_features = {'pagerank': np.nan, 'productivity': np.nan}\n",
    "    features_of_given_authors = list(map(lambda author: author_graph_features[year][author]\n",
    "    if author in author_graph_features[year] else default_features,\n",
    "                                         authors_list))\n",
    "\n",
    "    pageranks = filter(None.__ne__, map(lambda features: features['pagerank'], features_of_given_authors))\n",
    "    productivities = filter(None.__ne__, map(lambda features: features['productivity'], features_of_given_authors))\n",
    "    return pd.Series([mean(pageranks), mean(productivities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors_papers(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    papers_of_given_authors = list(map(lambda author: authors_papers_dict[author]\n",
    "    if author in authors_papers_dict else 1,\n",
    "                                       authors_list))\n",
    "\n",
    "    return pd.Series([mean(papers_of_given_authors), max(papers_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_authors_citations(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    cit_of_given_authors = list(map(lambda author: authors_dict[author] if author in authors_dict else 0,\n",
    "                                    authors_list))\n",
    "\n",
    "    return pd.Series([mean(cit_of_given_authors), max(cit_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_as_in_paper(df2, step=5, current_year=2020):\n",
    "    df = df2.copy()\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['recency'] = current_year - df['year']\n",
    "    final_features = ['recency', 'topic_rank', 'diversity']\n",
    "\n",
    "    #   early citations (cumulative)\n",
    "    for i in range(1, step + 2):\n",
    "        feature_name = f'c{i}'\n",
    "        df[feature_name] = df.apply(lambda row: citations_after_n_years(row, n=i), axis=1)\n",
    "\n",
    "    logging.info(\"Done counting early citations\")\n",
    "\n",
    "    features_to_add = ['authors_mean_rank', 'authors_mean_hindex', 'authors_mean_gindex', 'authors_mean_sociality']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_features(row), axis=1)\n",
    "\n",
    "    logging.info(\"Done counting author rank, h-indexes, g-indexes and sociality\")\n",
    "\n",
    "    features_to_add = ['authors_mean_pagerank', 'authors_mean_productivity']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_graph_features(row), axis=1)\n",
    "\n",
    "    logging.info(\"Done counting author pagerank and productivity\")\n",
    "\n",
    "    df = pd.merge(df, pagerank_journals_df[year], on='journal', how='left').rename(\n",
    "        columns={'pagerank': 'journal_pagerank'})\n",
    "    df = pd.merge(df, journals_citations_years[['journal', 'rank', 'year']], on=['journal', 'year'], how='left').rename(\n",
    "        columns={'rank': 'journal_rank'})\n",
    "    final_features += ['journal_pagerank', 'journal_rank']\n",
    "    logging.info(\"Done counting rank and pagerank of each journal\")\n",
    "\n",
    "    #   extra features\n",
    "    df['title_len'] = df['title'].apply(lambda title: 0 if pd.isnull(title) else len(title))\n",
    "    df['abstract_len'] = df['abstract'].apply(lambda abstract: 0 if pd.isnull(abstract) else len(abstract))\n",
    "    df['n_authors'] = df['authors'].apply(lambda authors: len(authors.split(', ')))\n",
    "    final_features += ['title_len', 'abstract_len', 'n_authors']\n",
    "\n",
    "    final_targets = ['c1', 'c5']\n",
    "\n",
    "    return df[final_features + final_targets], final_features, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df, features, targets = preprocess_as_in_paper(papers_df_years[papers_df_years.year >= 1995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ten_features = ['recency', 'topic_rank', 'diversity', 'authors_mean_rank', 'authors_mean_hindex',\n",
    "                'authors_mean_sociality', 'authors_mean_pagerank', 'authors_mean_productivity',\n",
    "                'journal_pagerank', 'journal_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df.hist(figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_to_log = ['authors_mean_pagerank', 'authors_mean_sociality']\n",
    "logged_features = []\n",
    "for f in features_to_log:\n",
    "    if test_df[f].min(skipna=True) >= 0:\n",
    "        test_df['log_' + f] = test_df[f].apply(lambda x: np.log1p(x))\n",
    "        logged_features.append('log_' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "df = test_df.astype(np.float32)\n",
    "print(\"Shape before imputing nans\", df.shape)\n",
    "# Imputer silently removes columns, with empty values    \n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "df = pd.DataFrame(imp.fit_transform(df))\n",
    "print(\"Shape after\", df.shape)\n",
    "imputed_features = [c for c in test_df.columns if not test_df[c].isnull().values.all()]\n",
    "df.columns = imputed_features\n",
    "print(\"tranform done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models with different features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(df, features, target, model=LinearRegression(), show_plt=False, log_target=False, n=1):\n",
    "    features = [f for f in features + [target] if f in imputed_features]\n",
    "    train_validate = df[df.recency > step + 1][features]\n",
    "    train = train_validate[train_validate.recency > 11]\n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "\n",
    "    train.recency = train.recency\n",
    "    validate.recency = validate.recency\n",
    "\n",
    "    coefs = []\n",
    "    r_squared = []\n",
    "    rmse = []\n",
    "    for i in range(n):\n",
    "        frac = 0.8\n",
    "        train_sample = train.sample(frac=frac)\n",
    "        X = train_sample.iloc[:, :-1]\n",
    "        y = train_sample.iloc[:, -1]\n",
    "        val_sample = validate.sample(frac=frac)\n",
    "        X_validate = val_sample.iloc[:, :-1]\n",
    "        y_validate = val_sample.iloc[:, -1]\n",
    "\n",
    "        if log_target:\n",
    "            y = np.log(y + 1)\n",
    "            y_validate = np.log(y_validate + 1)\n",
    "\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        X_validate = scaler.transform(X_validate)\n",
    "\n",
    "        reg = model.fit(X, y)\n",
    "        coefs.append(reg.coef_)\n",
    "\n",
    "        if show_plt:\n",
    "            x = []\n",
    "            for a, b in zip(list(y_validate), list(reg.predict(X_validate))):\n",
    "                if a != 0:\n",
    "                    x.append(b - a)\n",
    "            plt.hist(x, bins=50)\n",
    "            plt.show()\n",
    "\n",
    "        r_squared.append((reg.score(X, y), reg.score(X_validate, y_validate)))\n",
    "        rmse.append((sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate))))\n",
    "        print(f\"R^2 train: {reg.score(X, y)} validate: {reg.score(X_validate, y_validate)}\")\n",
    "        print(f\"RMSE train: {sqrt(mse(reg.predict(X), y))} validate: {sqrt(mse(reg.predict(X_validate), y_validate))}\")\n",
    "\n",
    "    return reg, (X, y), (X_validate, y_validate), (coefs, r_squared, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_top_influencers(df, features, reg, coefs=None, n=10):\n",
    "    if coefs:\n",
    "        maxcoef = np.argsort(-np.abs(coefs))\n",
    "        coef = np.array(coefs)[maxcoef]\n",
    "    else:\n",
    "        maxcoef = np.argsort(-np.abs(reg.coef_))\n",
    "        coef = reg.coef_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Predict c5 given c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target = 'c5'\n",
    "all_features = features + ['c1']  # + logged_features\n",
    "print('All features', len(all_features))\n",
    "print('Test DF shape', test_df.shape)\n",
    "all_features.remove('authors_mean_gindex')\n",
    "reg, (X, y), (X_val, y_val), extras = predict(df, all_features, target, model=LassoCV(cv=5), show_plt=True,\n",
    "                                              log_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_top_influencers(test_df, all_features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(reg.predict(X_val), y_val, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Predict c_5 without any early citations info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_features.remove('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reg, (X, y), (X_val, y_val), extras = predict(df, all_features, 'c5', show_plt=True, log_target=True)\n",
    "print_top_influencers(test_df, all_features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df.drop(['c1'], axis=1).to_csv('~/predict.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Or with regularisation (L1 or L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Lasso regularisation; target {target}\")\n",
    "Ls, (X, y), (X_val, y_val), extras = predict(df, all_features, target, model=LassoCV(cv=5), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(Ls.predict(X_val), y_val, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Ridge regularisation; target {target}\")\n",
    "Rr, (X, y), (X_val, y_val), extras = predict(df, all_features, target, model=RidgeCV(), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg, (X, y), (X_val, y_val), (coefs, r_squared, rmse) = predict(df, all_features, target,\n",
    "                                                                model=LassoCV(cv=5, tol=0.4), log_target=True, n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(all_features)):\n",
    "    print(all_features[j])\n",
    "    plt.hist(np.transpose(coefs)[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "average_coef = []\n",
    "for j in range(len(all_features)):\n",
    "    average_coef.append(mean(np.transpose(coefs)[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_features = print_top_influencers(df, all_features, reg, coefs=average_coef, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class linear_regression:\n",
    "    def __init__(self, coef=[]):\n",
    "        self.coef = coef\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = preprocessing.scale(X)\n",
    "        self.y = y\n",
    "        self.coef = []\n",
    "        b = sum(y) / len(y)\n",
    "        a = np.dot(np.dot(np.linalg.pinv(np.dot(self.X.transpose(), self.X)), self.X.transpose()), y)\n",
    "        self.coef = [b] + a.tolist()\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted_y = np.dot(x, self.coef[1:]) + self.coef[0]\n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_validate(df2):\n",
    "    df = df2[all_features + [target]].copy()\n",
    "    df = df.astype(np.float32)\n",
    "    print(\"start fill nans\")\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp.fit(df)\n",
    "    df = pd.DataFrame(imp.transform(df))\n",
    "    df.columns = all_features + [target]\n",
    "\n",
    "    print(\"tranform done\")\n",
    "\n",
    "    train_validate = df[df.recency > step + 1][all_features + [target]]\n",
    "    train = train_validate[train_validate.recency > 11]\n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "    return train, validate\n",
    "\n",
    "\n",
    "train, validate = get_train_validate(test_df)\n",
    "X = train.iloc[:, :-1]\n",
    "y = np.log1p(train.iloc[:, -1])\n",
    "X_validate = validate.iloc[:, :-1]\n",
    "y_validate = np.log1p(validate.iloc[:, -1])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_validate = scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(all_features)):\n",
    "    if abs(average_coef[i]) < 0.002:\n",
    "        average_coef[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_average_coef = [mean(y)] + average_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reg_test = linear_regression(coef=all_average_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = reg_test.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(y_predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(r2_score(y, reg_test.predict(X)), r2_score(y_validate, reg_test.predict(X_validate)))\n",
    "print(r2_score(np.exp(y), np.exp(reg_test.predict(X))),\n",
    "      r2_score(np.exp(y_validate), np.exp(reg_test.predict(X_validate))))\n",
    "print(sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(reg_test.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use desicion tree instead of linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = test_df[all_features + ['c5']].astype(np.float32)\n",
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_validate = df[df.recency > step + 1][all_features + [target]]\n",
    "train = train_validate[train_validate.recency > 11]\n",
    "validate = train_validate[train_validate.recency <= 11]\n",
    "\n",
    "train_sample = train.sample(frac=0.7)\n",
    "X = train_sample.iloc[:, :-1]\n",
    "y = np.log(train_sample.iloc[:, -1] + 1)\n",
    "validate_sample = validate.sample(frac=0.7)\n",
    "X_validate = validate_sample.iloc[:, :-1]\n",
    "y_validate = np.log(validate_sample.iloc[:, -1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=6, min_samples_split=40, n_estimators=1000, n_jobs=-1, verbose=4)\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"RMSE train: {sqrt(mse(regr.predict(X), y))} validate: {sqrt(mse(regr.predict(X_validate), y_validate))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"R^2: train {regr.score(X, y)} validate: {regr.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(r2_score(np.exp(y_validate), np.exp(regr.predict(X_validate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(regr.predict(X_validate), y_validate, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"R^2: train {r2_score(y, regr.predict(X))} validate: {r2_score(y_validate, regr.predict(X_validate))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_top_influencers_tree(df, features, reg, n=10):\n",
    "    maxcoef = np.argsort(-np.abs(reg.feature_importances_))\n",
    "    coef = reg.feature_importances_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_features = print_top_influencers_tree(test_df, all_features, regr, n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install catboost\n",
    "from catboost import Pool, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Pool(data=X, label=y)\n",
    "eval_dataset = Pool(data=X_validate, label=y_validate)\n",
    "model = CatBoostRegressor(iterations=1400, use_best_model=True, learning_rate=0.02, max_depth=6, loss_function='RMSE')\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          use_best_model=True,\n",
    "          eval_set=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"R^2: {model.score(X, y)} validate: {model.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(model.predict(X_validate), y_validate, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "importances = model.get_feature_importance(data=train_dataset,\n",
    "                                           prettified=True,\n",
    "                                           thread_count=-1,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_validate(train, validate):\n",
    "    X = train[all_features + ['is_top']].iloc[:, :-1]\n",
    "    y = train[all_features + ['is_top']].iloc[:, -1]\n",
    "    X_validate = validate[all_features + ['is_top']].iloc[:, :-1]\n",
    "    y_validate = validate[all_features + ['is_top']].iloc[:, -1]\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_validate = scaler.transform(X_validate)\n",
    "    return X, y, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "part = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_best = train.nlargest(columns=['c5'], n=int(part * train.shape[0]))\n",
    "train_min = train_best['c5'].min()\n",
    "validate_best = validate.nlargest(columns=['c5'], n=int(part * validate.shape[0]))\n",
    "val_min = validate_best['c5'].min()\n",
    "train['is_top'] = train['c5'].apply(lambda x: 1 if x > train_min else 0)\n",
    "validate['is_top'] = validate['c5'].apply(lambda x: 1 if x > val_min else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_best.groupby(by=['recency'])['c5'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top = train[train['is_top'] == 1]\n",
    "not_top = train[train['is_top'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"top size: {top.shape[0]} not top size: {not_top.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "not_top_downsampled = resample(not_top, replace=False, n_samples=len(top), random_state=27)\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_top_downsampled, top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_upsampled = resample(top, replace=True, n_samples=len(not_top), random_state=27)\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_top, top_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in [downsampled, upsampled]:\n",
    "    X, y, X_validate, y_validate = train_validate(dataset, validate)\n",
    "    target = 'is_top'\n",
    "    for weight in ([1, 3, 5]):\n",
    "        print(\"weight =\", weight)\n",
    "        clf = RandomForestClassifier(max_depth=7, n_estimators=1000, class_weight={0: 1, 1: weight})\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print(\"score train: \", clf.score(X, y), \"validate :\", clf.score(X_validate, y_validate))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_validate, clf.predict(X_validate)).ravel()\n",
    "        print(tn, fp, fn, tp)\n",
    "        print(\"recall :\", recall_score(y_validate, clf.predict(X_validate)))\n",
    "        print(\"precision :\", precision_score(y_validate, clf.predict(X_validate)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
