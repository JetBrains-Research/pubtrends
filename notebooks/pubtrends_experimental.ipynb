{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pubtrends-experimental\n",
    "\n",
    "Experimental notebook for hypothesis testing and development purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from bokeh.plotting import show, figure, output_file, save, reset_output, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pysrc.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.utils import SORT_MOST_CITED\n",
    "\n",
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 10_000\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "# Avoid info message about compilation flags\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "\n",
    "config = PubtrendsConfig(test=False)\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Title1', 'Title2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pysrc.papers.db.postgres_utils import preprocess_quotes, preprocess_search_query_for_postgres\n",
    "from pysrc.papers.utils import SORT_MOST_RECENT\n",
    "\n",
    "pmids = []\n",
    "for title in tqdm(titles):\n",
    "    paperids = loader.search_key_value('title', title)\n",
    "    if paperids:\n",
    "        pmids.extend(paperids)\n",
    "    else:\n",
    "        print(f'NOT FOUND: {title}')\n",
    "\n",
    "print('Found papers', len(pmids), 'of', len(titles))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By DOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pysrc.papers.utils import cut_authors_list, crc32, \\\n",
    "#     preprocess_doi, preprocess_search_title, rgb2hex\n",
    "# dois = [preprocess_doi(d) for d in dois]\n",
    "# pmids = []\n",
    "# for doi in tqdm(dois):\n",
    "#    paperids = loader.find('doi', doi)\n",
    "#    if paperids:\n",
    "#        pmids.extend(paperids)\n",
    "#    else:\n",
    "#        print(doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Pubmed syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from Bio import Entrez\n",
    "# Entrez.email = 'os@jetbrains.com'\n",
    "# QUERY = '((Aging) NOT (Review[Publication Type])) AND ((\"2015\"[Date - Publication] : \"2025\"[Date - Publication]))'\n",
    "# handle = Entrez.esearch(db='pubmed', retmax='1000', retmode='xml', term=QUERY)\n",
    "# pmids = Entrez.read(handle)['IdList']\n",
    "# print(f'Found {len(pmids)} papers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     pmids = analyzer.search_terms('Human Immune Aging', 1000, SORT_MOST_CITED)\n",
    "#     analyzer.analyze_papers(pmids, 'bci', 20)\n",
    "# finally:\n",
    "#     loader.close_connection()\n",
    "#     analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config.topic_min_size=5\n",
    "try:\n",
    "    analyzer.analyze_papers(pmids, 'Papers', 'Pubmed', SEARCH_PAPERS, SORT_MOST_RECENT, 10)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.plot.plotter import Plotter\n",
    "analyzer.search_ids = pmids\n",
    "plotter = Plotter(config, analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_papers_by_year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_top_cited_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_most_cited_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_fastest_growth_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens\n",
    "from itertools import chain\n",
    "\n",
    "freq_kwds = get_frequent_tokens(chain(*chain(*plotter.data.corpus)))\n",
    "show(plotter.plot_keywords_frequencies(freq_kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_papers_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.topics_hierarchy_with_keywords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tokens embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pysrc.papers.analysis.text import texts_embeddings, vectorize_corpus, tokens_embeddings\n",
    "\n",
    "print('Compute global embeddings')\n",
    "embeddings = tokens_embeddings(analyzer.corpus, analyzer.corpus_tokens)\n",
    "analyzer.corpus_tokens = analyzer.corpus_tokens\n",
    "print(f'Embeddings shape {embeddings.shape}')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logger.debug('Computing PCA projection')\n",
    "pca = PCA(n_components=15)\n",
    "t = StandardScaler().fit_transform(embeddings)\n",
    "pca_coords = pca.fit_transform(t)\n",
    "logger.debug(f'Explained variation {int(np.sum(pca.explained_variance_ratio_) * 100)}%')\n",
    "\n",
    "logger.debug('Apply TSNE transformation on papers PCA coords')\n",
    "tsne_embeddings_2d = TSNE(n_components=2, random_state=42).fit_transform(pca_coords)\n",
    "xs = tsne_embeddings_2d[:, 0]\n",
    "ys = tsne_embeddings_2d[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(xs, ys, linewidths=0.1, color='black', alpha=0.1)\n",
    "plt.xlabel('tSNE1')\n",
    "plt.ylabel('tSNE2')\n",
    "plt.title('All tokens in global word embedding space')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.utils import factors_colormap\n",
    "import matplotlib.pyplot as plt\n",
    "from pysrc.papers.plot.plot_preprocessor import PlotPreprocessor\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "from itertools import chain\n",
    "\n",
    "data = analyzer\n",
    "\n",
    "n = 10\n",
    "\n",
    "print('Show words for components')\n",
    "topics_description = get_topics_description(\n",
    "            data.df,\n",
    "            data.corpus, data.corpus_tokens, data.corpus_counts,\n",
    "            n_words=n\n",
    "        )\n",
    "kwd_df = PlotPreprocessor.compute_kwds(topics_description, n)\n",
    "words2show = PlotPreprocessor.topics_words(kwd_df, n)\n",
    "print(words2show)\n",
    "\n",
    "words = list(chain(*words2show.values()))\n",
    "print(f'Total words {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wxs = [xs[analyzer.corpus_tokens.index(w)] for w in words]\n",
    "wys = [ys[analyzer.corpus_tokens.index(w)] for w in words]\n",
    "\n",
    "cmap = factors_colormap(len(words2show))\n",
    "colors = []\n",
    "sizes = [analyzer.corpus_counts[:, analyzer.corpus_tokens.index(w)].sum() / len(analyzer.df) * 500\n",
    "         for w in words]\n",
    "for i, ws in words2show.items():\n",
    "    colors.extend([cmap(i)] * len(ws))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(wxs, wys, sizes=sizes, color=colors, alpha=0.5)\n",
    "plt.xlabel('tSNE1')\n",
    "plt.ylabel('tSNE2')\n",
    "plt.title('Main keywords in global word embedding space')\n",
    "\n",
    "for word, x, y in zip(words, wxs, wys):\n",
    "    plt.annotate(word, xy=(x, y - 0.1), size=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Topics visualization in embedded space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comp_pids = analyzer.df[['id', 'comp']].groupby('comp')['id'].apply(list).to_dict()\n",
    "terms_freqs_per_comp = np.zeros(shape=(len(words2show), analyzer.corpus_counts.shape[1]), dtype=float)\n",
    "for comp, pids in comp_pids.items():\n",
    "    terms_freqs_per_comp[comp, :] = np.sum(analyzer.corpus_counts[np.flatnonzero(analyzer.df['id'].isin(pids)), :],\n",
    "                                           axis=0) / len(pids)\n",
    "print(terms_freqs_per_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "n = 500\n",
    "\n",
    "ncols = 4\n",
    "nrows = int(math.ceil(len(comp_pids) / ncols))\n",
    "plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, int(15 * nrows / ncols)))\n",
    "for comp, _ in comp_pids.items():\n",
    "    freqs = terms_freqs_per_comp[comp, :]\n",
    "    freq_keywords_indx = freqs.argsort()[-n:][::-1]\n",
    "    ax = plt.subplot(nrows, ncols, comp + 1)\n",
    "    wxs = [xs[i] for i in freq_keywords_indx]\n",
    "    wys = [ys[i] for i in freq_keywords_indx]\n",
    "    words = [analyzer.corpus_tokens[i] for i in freq_keywords_indx]\n",
    "    sizes = [freqs[i] * 50 for i in freq_keywords_indx]\n",
    "    ax.scatter(wxs, wys, marker='o', sizes=sizes, color=cmap(comp), alpha=0.5)\n",
    "#     plt.xlabel('tSNE1')\n",
    "#     plt.ylabel('tSNE2')\n",
    "#     plt.title(f'Topic {comp + 1} papers frequent keywords in global word embedding space')\n",
    "#     for word, x, y in zip(words, wxs, wys):\n",
    "#         plt.annotate(word, xy=(x, y-0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Authors analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.utils import cut_authors_list, color_to_rgb\n",
    "\n",
    "\n",
    "def plot_embeddings(df, clusters):\n",
    "    cmap = factors_colormap(len(set(clusters)))\n",
    "    palette = dict(zip(sorted(set(clusters)), [color_to_rgb(cmap(i)).to_hex()\n",
    "                                               for i in range(len(set(clusters)))]))\n",
    "\n",
    "    df['size'] = 5 + df['total'] / df['total'].max() * 20\n",
    "\n",
    "    # Split authors\n",
    "    df['authors'] = df['authors'].apply(lambda authors: cut_authors_list(authors))\n",
    "\n",
    "    ds = ColumnDataSource(df)\n",
    "    # Add clusters coloring\n",
    "    ds.add([palette[c] for c in clusters], 'color')\n",
    "    p = figure(width=600, height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'x'\n",
    "    p.yaxis.axis_label = 'y'\n",
    "\n",
    "    p.hover.tooltips = plotter._paper_html_tooltips([\n",
    "        (\"Author(s)\", '@authors'),\n",
    "        (\"Journal\", '@journal'),\n",
    "        (\"Year\", '@year'),\n",
    "        (\"Type\", '@type'),\n",
    "        (\"Cited by\", '@total paper(s) total'),\n",
    "        (\"Topic\", '@comp')])\n",
    "    p.scatter(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='comp')\n",
    "    p.legend.visible = False\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fa(authors, first_last_only=True):\n",
    "    return authors if len(authors) <= 2 or not first_last_only else [authors[0], authors[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_authors_citations_and_papers(df):\n",
    "    logger.debug('Compute author citations')\n",
    "    author_citations = {}\n",
    "    for i, row in tqdm(df[['authors', 'total']].iterrows()):\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            author_citations[a] = author_citations.get(a, 0) + row['total']\n",
    "\n",
    "    logger.debug('Compute number of papers per author')\n",
    "    author_papers = {}\n",
    "    for i, row in df[['id', 'title', 'authors']].iterrows():\n",
    "        pmid = row['id']\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            if a not in author_papers:\n",
    "                author_papers[a] = []    \n",
    "            author_papers[a].append(pmid)\n",
    "\n",
    "    return author_citations, author_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.info(\"Analyzing top 20% of authors\")\n",
    "authors_citations, authors_papers = compute_authors_citations_and_papers(analyzer.df)\n",
    "logging.info(f\"Total first and last authors {len(authors_papers)}\")\n",
    "min_threshold = np.percentile([len(ps) for ps in authors_papers.values()], 80)\n",
    "logging.info(f'Min papers for author {min_threshold}')\n",
    "logging.info(f'Filtered authors: {sum(len(v) >= min_threshold for v in authors_papers.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def build_authors_similarity_graph(df,\n",
    "                                   cocit_grouped_df, bibcoupling_df, cit_df,\n",
    "                                   check_author_func=lambda a: True):\n",
    "    logger.debug('Processing papers')\n",
    "    result = nx.Graph()\n",
    "    for _, row in tqdm(df[['authors']].iterrows()):\n",
    "        authors = fa(row[0].split(', '))\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                a1 = authors[i]\n",
    "                a2 = authors[j]\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'authorship', 1)\n",
    "\n",
    "    logger.debug('Processing co-citations')\n",
    "    for el in tqdm(cocit_grouped_df[['cited_1', 'cited_2', 'total']].values):\n",
    "        start, end, cocitation = str(el[0]), str(el[1]), float(el[2])\n",
    "        authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'cocitation', cocitation)\n",
    "\n",
    "    logger.debug('Bibliographic coupling')\n",
    "    if len(bibcoupling_df) > 0:\n",
    "        for el in tqdm(bibcoupling_df[['citing_1', 'citing_2', 'total']].values):\n",
    "            start, end, bibcoupling = str(el[0]), str(el[1]), float(el[2])\n",
    "            authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "            authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "            for a1, a2 in itertools.product(authors1, authors2):\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'bibcoupling', bibcoupling)\n",
    "\n",
    "    logger.debug('Citations')\n",
    "    # Citations\n",
    "    for start, end in zip(cit_df['id_out'], cit_df['id_in']):\n",
    "        authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'citation', 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_edge(graph, a1, a2, name, value):\n",
    "    if a1 == a2:\n",
    "        return\n",
    "    if a1 > a2:\n",
    "        a1, a2 = a2, a1\n",
    "    if not graph.has_edge(a1, a2):\n",
    "        graph.add_edge(a1, a2)\n",
    "    edge = graph[a1][a2]\n",
    "    edge[name] = edge.get(name, 0) + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "logger = logging.getLogger('Test')\n",
    "\n",
    "authors_similarity_graph = build_authors_similarity_graph(\n",
    "    analyzer.df, analyzer.cocit_grouped_df,\n",
    "    analyzer.bibliographic_coupling_df,\n",
    "    analyzer.cit_df,\n",
    "    check_author_func=lambda a: len(authors_papers[a]) >= min_threshold\n",
    ")\n",
    "\n",
    "logging.info(f'Built authors graph - '\n",
    "             f'{len(authors_similarity_graph.nodes())} nodes and {len(authors_similarity_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Node2vec embeddings for authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.node2vec import node2vec\n",
    "from pysrc.papers.analysis.graph import sparse_graph, similarity\n",
    "\n",
    "def to_weighted_graph(graph, weight_func, key='weight'):\n",
    "    logger.debug('Creating weighted graph')\n",
    "    g = nx.Graph()\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        w = weight_func(data)\n",
    "        if np.isnan(w):\n",
    "            raise Exception(f'Weight is NaN {w}')\n",
    "        elif w < 0:\n",
    "            raise Exception(f'Weight is < 0 {w}')\n",
    "        elif w != 0:\n",
    "            g.add_edge(u, v, **{key: w})\n",
    "    # Ensure all the nodes present\n",
    "    for v in graph.nodes:\n",
    "        if not g.has_node(v):\n",
    "            g.add_node(v)\n",
    "    return g\n",
    "\n",
    "\n",
    "logger.debug('Compute aggregated similarity using co-authorship')\n",
    "ga = to_weighted_graph(authors_similarity_graph,\n",
    "                       weight_func=lambda d: 100 * d.get('authorship', 0) + similarity(d))\n",
    "gs = sparse_graph(ga, 10)\n",
    "authors_node_ids = list(authors_similarity_graph.nodes)\n",
    "authors_weighted_node_embeddings = node2vec(authors_node_ids, gs)\n",
    "print(authors_weighted_node_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors papers embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import texts_embeddings\n",
    "papers_text_embeddings = texts_embeddings(\n",
    "    analyzer.corpus_counts, analyzer.corpus_tokens_embedding\n",
    ")\n",
    "print(papers_text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_node_ids = list(authors_similarity_graph.nodes)\n",
    "papers_idx = {pmid: idx for idx, pmid in enumerate(analyzer.df['id'])}\n",
    "authors_papers_embeddings = np.zeros((len(authors_node_ids), papers_text_embeddings.shape[1]))\n",
    "for i, a in enumerate(authors_node_ids):\n",
    "    for pmid in authors_papers[a]:\n",
    "        authors_papers_embeddings[i, :] += papers_text_embeddings[papers_idx[pmid], :]\n",
    "    authors_papers_embeddings[i, :] /= len(authors_papers[a])\n",
    "authors_papers_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.config import *\n",
    "authors_embeddings = (authors_weighted_node_embeddings * GRAPH_EMBEDDINGS_FACTOR +\n",
    "                      authors_papers_embeddings * TEXT_EMBEDDINGS_FACTOR\n",
    "                      ) / (GRAPH_EMBEDDINGS_FACTOR + TEXT_EMBEDDINGS_FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORS_HIGHLIGTHS = ['A1', 'A2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "logger.debug('Computing PCA projection')\n",
    "pca = PCA(n_components=min(len(authors_embeddings), PCA_COMPONENTS))\n",
    "t = StandardScaler().fit_transform(authors_embeddings)\n",
    "authors_pca_coords = pca.fit_transform(t)\n",
    "logger.debug(f'Explained variation {int(np.sum(pca.explained_variance_ratio_) * 100)}%')\n",
    "\n",
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "authors_tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, authors_pca_coords.shape[0] - 1))\n",
    "authors_weighted_node_embeddings_2d = authors_tsne.fit_transform(authors_pca_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build dataframe combining information about authors and projected coordinates\n",
    "authors_df = pd.DataFrame(dict(author=authors_node_ids,\n",
    "                               x=authors_weighted_node_embeddings_2d[:, 0],\n",
    "                               y=authors_weighted_node_embeddings_2d[:, 1]))\n",
    "authors_df['cited'] = [authors_citations[a] for a in authors_df['author']]\n",
    "authors_df['papers'] = [authors_papers[a] for a in authors_df['author']]\n",
    "authors_df['size'] = [1 + 10 * np.log1p(authors_citations[a]) for a in authors_df['author']]\n",
    "# Limit max size\n",
    "authors_df['size'] = authors_df['size'] * 10 / authors_df['size'].max() + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import LabelSet, Label\n",
    "\n",
    "\n",
    "def plot_authors(authors_df, highlights=[], groups_topics=None, width=600, height=600):\n",
    "    clusters = set(authors_df['cluster'])\n",
    "    cmap = factors_colormap(len(clusters))\n",
    "    palette = dict(zip(sorted(clusters),\n",
    "                       [color_to_rgb(cmap(i)).to_hex() for i in range(len(clusters))]))\n",
    "    authors_df['color'] = [palette[c] for c in authors_df['cluster']]\n",
    "    authors_df['line_width'] = [3 if a in highlights else 1 for a in authors_df['author']]\n",
    "    ds = ColumnDataSource(authors_df)\n",
    "    del authors_df['color'], authors_df['line_width']\n",
    "    x = authors_df['x']\n",
    "    y = authors_df['y']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(width=width, height=height,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange),\n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\",\n",
    "               tooltips=[(\"Author\", '@author'),\n",
    "                         (\"Papers\", '@papers'),\n",
    "                         (\"Cited by\", '@cited'),\n",
    "                         (\"Cluster\", '@cluster'),\n",
    "                         (\"Tags\", '@tags')])\n",
    "\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    p.scatter(source=ds, x='x', y='y', fill_alpha=0.8, size='size', line_width='line_width',\n",
    "             line_color='black', fill_color='color')\n",
    "\n",
    "    lxs = [authors_df.loc[authors_df['cluster'] == c]['x'].mean() for c in sorted(clusters)]\n",
    "    lys = [authors_df.loc[authors_df['cluster'] == c]['y'].mean() for c in sorted(clusters)]\n",
    "    cluster_labels = [f'#{c + 1}' for c in sorted(clusters)]\n",
    "    source = ColumnDataSource({'x': lxs, 'y': lys, 'name': cluster_labels})\n",
    "    labels = LabelSet(x='x', y='y', text='name', source=source,\n",
    "                      background_fill_color='white', text_font_size='11px', background_fill_alpha=.9)\n",
    "    p.renderers.append(labels)\n",
    "    for i, c in enumerate(sorted(clusters)):\n",
    "        p.rect(x=min(x), y=max(y) - i * 2, width=1, height=2, fill_color=palette[c], line_color=None)\n",
    "        if groups_topics is not None:\n",
    "            text = f\"#{c + 1} {', '.join(t for t, _ in groups_topics[c][:5])}\"\n",
    "        else:\n",
    "            text = f\"#{c + 1}\"\n",
    "        p.add_layout(Label(\n",
    "                    x=min(x) + 1, y=max(y) - 1 - i * 2,\n",
    "                    text=text,\n",
    "                    text_font_size='11px',\n",
    "                    text_align=\"left\",\n",
    "                    background_fill_color=\"white\",\n",
    "                    background_fill_alpha=0.7,\n",
    "                ))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authors_df['cluster'] = 0\n",
    "authors_df['tags'] = 'n/a'\n",
    "show(plot_authors(authors_df, highlights=AUTHORS_HIGHLIGTHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Authors clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.topics import cluster_and_sort\n",
    "\n",
    "author_clusters, _ = cluster_and_sort(authors_pca_coords, 10, 5)\n",
    "authors_df['cluster'] = author_clusters\n",
    "\n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'cluster': author_clusters,\n",
    "                  'size': np.ones(len(author_clusters))}).groupby(['cluster']).sum().astype(int).reset_index()\n",
    "sns.barplot(data=t, x='cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plot_authors(authors_df, highlights=AUTHORS_HIGHLIGTHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analyze authors group topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def compute_tfidf(corpus_counts):\n",
    "    logger.debug('Compute TF-IDF on tokens counts')\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(corpus_counts)\n",
    "    logger.debug(f'TFIDF shape {tfidf.shape}')\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def compute_groups_topics(authors_df):\n",
    "    logging.info('Computing groups of authors topics')\n",
    "    groups_counts = np.zeros(shape=(len(set(authors_df['cluster'])), analyzer.corpus_counts.shape[1]), dtype=np.float64)\n",
    "\n",
    "    part_sizes = Counter(authors_df['cluster'])\n",
    "    authors_clusters_map = dict(zip(authors_df['author'], authors_df['cluster']))\n",
    "\n",
    "    for i, row in tqdm(analyzer.df[['authors']].iterrows()):\n",
    "        for a in row[0].split(', '):\n",
    "            if a in authors_clusters_map:\n",
    "                group = authors_clusters_map[a]\n",
    "                groups_counts[group, :] += analyzer.corpus_counts[i, :] / part_sizes[group]\n",
    "\n",
    "    tfidf = compute_tfidf(groups_counts)\n",
    "\n",
    "    logging.info('Take terms with the largest tfidf for topics')\n",
    "    result = {}\n",
    "    for g in range(groups_counts.shape[0]):\n",
    "        counter = Counter()\n",
    "        for i, t in enumerate(analyzer.corpus_tokens):\n",
    "            counter[t] += tfidf[g, i]\n",
    "        # Ignore terms with insignificant frequencies\n",
    "        result[g] = [(t, f) for t, f in counter.most_common(10) if f > 0]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "groups_topics = compute_groups_topics(authors_df)\n",
    "kwds = [(g, ','.join(f'{t}:{v:.3f}' for t, v in vs)) for g, vs in groups_topics.items()]\n",
    "logging.info('Description\\n' + '\\n'.join(f'{g}: {kwd}' for g, kwd in kwds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groups_df = pd.DataFrame(columns=['group', 'authors', 'keywords'], dtype=object)\n",
    "for g in sorted(set(authors_df['cluster'])):\n",
    "    authors = ', '.join(authors_df.loc[authors_df['cluster'] == g]['author'])\n",
    "    groups_df.loc[len(groups_df)] = (g, authors, ','.join(t for t, _ in groups_topics[g]))\n",
    "\n",
    "display(groups_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "authors_df['tags'] = [', '.join(f'{t}:{v:.3f}' for t, v in groups_topics[c][:5]) for c in authors_df['cluster']]\n",
    "show(plot_authors(authors_df, highlights=AUTHORS_HIGHLIGTHS, groups_topics=groups_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#logging.info('Saving author groups graph for bokeh')\n",
    "#output_file(filename=os.path.expanduser(\"~/authors.html\"), title=\"Authors similarity graph\")\n",
    "#save(plot_authors(authors_df, width=1600, height=1200))\n",
    "#reset_output()\n",
    "#output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use ego-splitting to compute possible overlapping groups of authors\n",
    "Taken from https://github.com/benedekrozemberczki/EgoSplitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import community\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class EgoNetSplitter(object):\n",
    "    \"\"\"An implementation of `\"Ego-Splitting\" see:\n",
    "    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n",
    "    From the KDD '17 paper \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\".\n",
    "    The tool first creates the egonets of nodes.\n",
    "    A persona-graph is created which is clustered by the Louvain method.\n",
    "    The resulting overlapping cluster memberships are stored as a dictionary.\n",
    "    Args:\n",
    "        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resolution=1.0):\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def _create_egonet(self, node):\n",
    "        \"\"\"\n",
    "        Creating an ego net, extracting personas and partitioning it.\n",
    "\n",
    "        Args:\n",
    "            node: Node ID for egonet (ego node).\n",
    "        \"\"\"\n",
    "        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n",
    "        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n",
    "        new_mapping = {}\n",
    "        personalities = []\n",
    "        for k, v in components.items():\n",
    "            personalities.append(self.index)\n",
    "            for other_node in v:\n",
    "                new_mapping[other_node] = self.index\n",
    "            self.index = self.index + 1\n",
    "        self.components[node] = new_mapping\n",
    "        self.personalities[node] = personalities\n",
    "\n",
    "    def _create_egonets(self):\n",
    "        \"\"\"\n",
    "        Creating an egonet for each node.\n",
    "        \"\"\"\n",
    "        self.components = {}\n",
    "        self.personalities = {}\n",
    "        self.index = 0\n",
    "        print(\"Creating egonets.\")\n",
    "        for node in tqdm(self.graph.nodes()):\n",
    "            self._create_egonet(node)\n",
    "\n",
    "    def _map_personalities(self):\n",
    "        \"\"\"\n",
    "        Mapping the personas to new nodes.\n",
    "        \"\"\"\n",
    "        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n",
    "\n",
    "    def _get_new_edge_ids(self, edge):\n",
    "        \"\"\"\n",
    "        Getting the new edge identifiers.\n",
    "        Args:\n",
    "            edge: Edge being mapped to the new identifiers.\n",
    "        \"\"\"\n",
    "        return self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]]\n",
    "\n",
    "    def _create_persona_graph(self):\n",
    "        \"\"\"\n",
    "        Create a persona graph using the egonet components.\n",
    "        \"\"\"\n",
    "        print(\"Creating the persona graph.\")\n",
    "        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n",
    "        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n",
    "\n",
    "    def _create_partitions(self):\n",
    "        \"\"\"\n",
    "        Creating a non-overlapping clustering of nodes in the persona graph.\n",
    "        \"\"\"\n",
    "        print(\"Clustering the persona graph.\")\n",
    "        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n",
    "        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n",
    "        for node, membership in self.partitions.items():\n",
    "            self.overlapping_partitions[self.personality_map[node]].append(membership)\n",
    "\n",
    "    def fit(self, graph):\n",
    "        \"\"\"\n",
    "        Fitting an Ego-Splitter clustering model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be clustered.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self._create_egonets()\n",
    "        self._map_personalities()\n",
    "        self._create_persona_graph()\n",
    "        self._create_partitions()\n",
    "\n",
    "    def get_memberships(self):\n",
    "        r\"\"\"Getting the cluster membership of nodes.\n",
    "        Return types:\n",
    "            * **memberships** *(dictionary of lists)* - Cluster memberships.\n",
    "        \"\"\"\n",
    "        return self.overlapping_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = EgoNetSplitter(0.8)\n",
    "splitter.fit(authors_similarity_graph)\n",
    "\n",
    "ego_clusters = []\n",
    "for a, cs in splitter.overlapping_partitions.items():\n",
    "    ego_clusters.extend(cs)\n",
    "print('Total clusters', len(set(ego_clusters)))\n",
    "print('Clusters', Counter(ego_clusters))\n",
    "print(len(authors_similarity_graph.nodes()))\n",
    "print(len(ego_clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
