{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubtrends-experimental\n",
    "\n",
    "Experimental notebook for hypothesis testing and development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "Entrez.email = 'os@jetbrains.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = '((Aging) NOT (Review[Publication Type])) AND ((\"2015\"[Date - Publication] : \"2018\"[Date - Publication]))'\n",
    "handle = Entrez.esearch(db='pubmed', retmax='1000', retmode='xml', term=QUERY)\n",
    "pmids = Entrez.read(handle)['IdList']\n",
    "print(f'Found {len(pmids)} papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pysrc.papers.utils import SORT_MOST_CITED\n",
    "\n",
    "SEARCH_QUERY = 'Aging 2018+'\n",
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 1000\n",
    "OUTPUT = os.path.expanduser(f'~/pubtrends/{SEARCH_QUERY}')\n",
    "! mkdir -p \"{OUTPUT}\"\n",
    "\n",
    "# File with ids to analyze\n",
    "FILE = os.path.expanduser(f'~/pubtrends/{SEARCH_QUERY}/pmid.itxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE, 'w') as f:\n",
    "    f.write('\\n'.join(pmids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from bokeh.plotting import show, figure, output_file, save, reset_output, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "# Avoid info message about compilation flags\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analyzer_files import AnalyzerFiles\n",
    "\n",
    "config = PubtrendsConfig(test=False)\n",
    "config.feature_evolution_enabled = True\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = AnalyzerFiles(loader, config)\n",
    "\n",
    "try:\n",
    "    analyzer.analyze_ids(pmids, 'Pubmed', SEARCH_QUERY, SEARCH_PAPERS, SORT_MOST_CITED, 'medium')\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers text similarities analysis\n",
    "We hope that the distribution of similarities edge weights illustrates that majority of linked nodes are insignificantly similar in terms of their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_similarities_features():\n",
    "    bibcoupling_array = np.zeros(len(analyzer.papers_graph.edges))\n",
    "    cocitations_array = np.zeros(len(analyzer.papers_graph.edges))\n",
    "    citations_array = np.zeros(len(analyzer.papers_graph.edges))\n",
    "    similarities_array = np.zeros(len(analyzer.papers_graph.edges))\n",
    "    text_similarities_array = np.zeros(len(analyzer.papers_graph.edges))\n",
    "\n",
    "    for i, (u, v, data) in enumerate(analyzer.papers_graph.edges(data=True)):\n",
    "        bibcoupling_array[i] = np.log1p(data.get('bibcoupling', 0))\n",
    "        cocitations_array[i] = np.log1p(data.get('cocitation', 0))\n",
    "        citations_array[i] = data.get('citation', 0)\n",
    "        text_similarities_array[i] = data.get('text', 0)\n",
    "        similarities_array[i] = PapersAnalyzer.similarity(data)\n",
    "\n",
    "    fig = plt.figure(figsize=(5 * 4, 5))\n",
    "    ax = plt.subplot(1, 4, 1)\n",
    "    print(f'Bibcoupling, non-zero {np.count_nonzero(bibcoupling_array)} of {len(bibcoupling_array)}')\n",
    "    bibcoupling_array = bibcoupling_array[np.nonzero(bibcoupling_array)]\n",
    "    print(stats.describe(bibcoupling_array))\n",
    "    sns.kdeplot(bibcoupling_array)\n",
    "    plt.title('Bibcoupling')\n",
    "    # plt.show()\n",
    "\n",
    "    ax = plt.subplot(1, 4, 2)\n",
    "    print(f'Co-citations, non-zero {np.count_nonzero(cocitations_array)} of {len(cocitations_array)}')\n",
    "    cocitations_array = cocitations_array[np.nonzero(cocitations_array)]\n",
    "    print(stats.describe(cocitations_array))\n",
    "    sns.kdeplot(cocitations_array)\n",
    "    plt.title('Co-citations')\n",
    "    # plt.show()\n",
    "\n",
    "    ax = plt.subplot(1, 4, 3)\n",
    "    print(f'Text similarities, non-zero {np.count_nonzero(text_similarities_array)} of {len(text_similarities_array)}')\n",
    "    text_similarities_array = text_similarities_array[np.nonzero(text_similarities_array)]\n",
    "    print(stats.describe(text_similarities_array))\n",
    "    sns.kdeplot(text_similarities_array)\n",
    "    plt.title('Text')\n",
    "    # plt.show\n",
    "\n",
    "    ax = plt.subplot(1, 4, 4)\n",
    "    print(f'Similarities, non-zero {np.count_nonzero(similarities_array)} of {len(similarities_array)}')\n",
    "    print(stats.describe(similarities_array))\n",
    "    sns.kdeplot(similarities_array)\n",
    "    plt.title('Similarity')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Citations, non-zero {np.count_nonzero(citations_array)} of {len(citations_array)}')\n",
    "    \n",
    "analyze_similarities_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def analyze_similarities():\n",
    "    print('Analyze similarities between tokens counts for all papers')\n",
    "    cos_similarities = cosine_similarity(analyzer.corpus_counts)\n",
    "    cos_similarities_array = cos_similarities.reshape(-1)\n",
    "    print(stats.describe(cos_similarities_array))\n",
    "    print('Q1', np.percentile(cos_similarities_array, 25), \n",
    "          'Q2', np.percentile(cos_similarities_array, 50), \n",
    "          'Q3', np.percentile(cos_similarities_array, 75))\n",
    "\n",
    "    fig = plt.figure(figsize=(5 * 2, 5))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(cos_similarities_array)\n",
    "    plt.title('Cosine similarities among all papers')\n",
    "    # plt.show()\n",
    "\n",
    "    print('Analyze similarities between papers with direct citations')\n",
    "    pid_indx = {pid: i for i, pid in enumerate(analyzer.df['id'])}\n",
    "    cited_cos_similarities = []\n",
    "    for i, (u, v, data) in enumerate(analyzer.papers_graph.edges(data=True)):\n",
    "        if data.get('citation', 0) != 0:\n",
    "            cited_cos_similarities.append(cos_similarities[pid_indx[u], pid_indx[v]])\n",
    "\n",
    "    print(stats.describe(cited_cos_similarities))\n",
    "    print('Q1', np.percentile(cited_cos_similarities, 25), \n",
    "          'Q2', np.percentile(cited_cos_similarities, 50), \n",
    "          'Q3', np.percentile(cited_cos_similarities, 75))\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    sns.kdeplot(cited_cos_similarities)\n",
    "    plt.title('Cosine similarity between cited papers')\n",
    "\n",
    "    plt.show()   \n",
    "    \n",
    "analyze_similarities()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = analyzer.papers_graph\n",
    "degrees = [d for (n, d) in G.degree()]\n",
    "plt.title('Similarity graph degrees')\n",
    "sns.kdeplot(data=degrees)          \n",
    "plt.show()  \n",
    "print('Average degree', sum(degrees) / float(G.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pysrc.papers.utils import cut_authors_list\n",
    "\n",
    "\n",
    "def plot_embeddings(df, clusters):\n",
    "    cmap = Plotter.factors_colormap(len(set(clusters)))\n",
    "    palette = dict(zip(sorted(set(clusters)), [Plotter.color_to_rgb(cmap(i)).to_hex()\n",
    "                                               for i in range(len(set(clusters)))]))\n",
    "\n",
    "    df['size'] = 5 + df['total'] / df['total'].max() * 20\n",
    "\n",
    "    # Split authors\n",
    "    df['authors'] = df['authors'].apply(lambda authors: cut_authors_list(authors))\n",
    "\n",
    "    ds = ColumnDataSource(df)\n",
    "    # Add clusters coloring\n",
    "    ds.add([palette[c] for c in clusters], 'color')\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'x'\n",
    "    p.yaxis.axis_label = 'y'\n",
    "\n",
    "    p.hover.tooltips = plotter._paper_html_tooltips([\n",
    "        (\"Author(s)\", '@authors'),\n",
    "        (\"Journal\", '@journal'),\n",
    "        (\"Year\", '@year'),\n",
    "        (\"Type\", '@type'),\n",
    "        (\"Cited by\", '@total paper(s) total'),\n",
    "        (\"Topic\", '@comp')])\n",
    "    p.circle(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='comp')\n",
    "    p.legend.visible = False\n",
    "    show(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(authors, first_last_only=True):\n",
    "    return authors if len(authors) <= 2 or not first_last_only else [authors[0], authors[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_authors_citations_and_papers(df):\n",
    "    logger.debug('Compute author citations')\n",
    "    author_citations = {}\n",
    "    for i, row in tqdm(df[['authors', 'total']].iterrows()):\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            author_citations[a] = author_citations.get(a, 0) + row['total']\n",
    "\n",
    "    logger.debug('Compute number of papers per author')\n",
    "    author_papers = {}\n",
    "    for i, row in df[['title', 'authors']].iterrows():\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            author_papers[a] = author_papers.get(a, 0) + 1\n",
    "\n",
    "    return author_citations, author_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "logging.info(\"Analyzing groups of similar authors\")\n",
    "authors_citations, authors_papers = compute_authors_citations_and_papers(analyzer.df)\n",
    "logging.info(f\"Authors {len(authors_papers)}\")\n",
    "min_threshold = np.percentile(list(authors_papers.values()), 90)\n",
    "logging.info(f'Min papers for author {min_threshold}')\n",
    "logging.info(f'Filtered authors: {sum(v >= min_threshold for v in authors_papers.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_authors_similarity_graph(df,\n",
    "                                   cocit_grouped_df, bibcoupling_df, cit_df,\n",
    "                                   check_author_func=lambda a: True):\n",
    "    logger.debug('Processing papers')\n",
    "    result = nx.Graph()\n",
    "    for _, row in tqdm(df[['authors']].iterrows()):\n",
    "        authors = fa(row[0].split(', '))\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                a1 = authors[i]\n",
    "                a2 = authors[j]\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'authorship', 1)\n",
    "\n",
    "    logger.debug('Processing co-citations')\n",
    "    for el in tqdm(cocit_grouped_df[['cited_1', 'cited_2', 'total']].values):\n",
    "        start, end, cocitation = str(el[0]), str(el[1]), float(el[2])\n",
    "        authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'cocitation', cocitation)\n",
    "\n",
    "    logger.debug('Bibliographic coupling')\n",
    "    if len(bibcoupling_df) > 0:\n",
    "        for el in tqdm(bibcoupling_df[['citing_1', 'citing_2', 'total']].values):\n",
    "            start, end, bibcoupling = str(el[0]), str(el[1]), float(el[2])\n",
    "            authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "            authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "            for a1, a2 in itertools.product(authors1, authors2):\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'bibcoupling', bibcoupling)\n",
    "\n",
    "    logger.debug('Citations')\n",
    "    # Citations\n",
    "    for start, end in zip(cit_df['id_out'], cit_df['id_in']):\n",
    "        authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'citation', 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_edge(graph, a1, a2, name, value):\n",
    "    if a1 == a2:\n",
    "        return\n",
    "    if a1 > a2:\n",
    "        a1, a2 = a2, a1\n",
    "    if not graph.has_edge(a1, a2):\n",
    "        graph.add_edge(a1, a2)\n",
    "    edge = graph[a1][a2]\n",
    "    edge[name] = edge.get(name, 0) + value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "logger = logging.getLogger('Test')\n",
    "\n",
    "authors_similarity_graph = build_authors_similarity_graph(\n",
    "    analyzer.df, analyzer.cocit_grouped_df,\n",
    "    analyzer.bibliographic_coupling_df,\n",
    "    analyzer.cit_df,\n",
    "    check_author_func=lambda a: authors_papers[a] >= min_threshold\n",
    ")\n",
    "\n",
    "logging.info(f'Built authors graph - '\n",
    "             f'{len(authors_similarity_graph.nodes())} nodes and {len(authors_similarity_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec embeddings for authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.node2vec import node2vec\n",
    "from pysrc.papers.analysis.graph import to_weighted_graph, sparse_graph\n",
    "\n",
    "logger.debug('Compute aggregated similarity using co-authorship')\n",
    "ga = to_weighted_graph(authors_similarity_graph, \n",
    "                       weight_func=lambda d: 100 * d.get('authorship', 0) + PapersAnalyzer.similarity(d))\n",
    "gs = sparse_graph(ga, 10)\n",
    "authors_node_ids = list(authors_similarity_graph.nodes)\n",
    "authors_weighted_node_embeddings = node2vec(authors_node_ids, gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "authors_tsne = TSNE(n_components=2, random_state=42)\n",
    "authors_weighted_node_embeddings_2d = authors_tsne.fit_transform(authors_weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about authors and projected coordinates\n",
    "authors_df = pd.DataFrame(dict(author=authors_node_ids, \n",
    "                               x=authors_weighted_node_embeddings_2d[:, 0],\n",
    "                               y=authors_weighted_node_embeddings_2d[:, 1]))\n",
    "authors_df['cited'] = [authors_citations[a] for a in authors_df['author']]\n",
    "authors_df['papers'] = [authors_papers[a] for a in authors_df['author']]\n",
    "authors_df['size'] = [1 + 10 * np.log1p(authors_citations[a]) for a in authors_df['author']]\n",
    "# Limit max size\n",
    "authors_df['size'] = authors_df['size'] * 10 / authors_df['size'].max() + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = 0\n",
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LabelSet\n",
    "\n",
    "\n",
    "def plot_authors(authors_df, plot_width=600, plot_height=600):\n",
    "    clusters = set(authors_df['cluster'])\n",
    "    cmap = Plotter.factors_colormap(len(clusters))\n",
    "    palette = dict(zip(sorted(clusters), \n",
    "                       [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(clusters))]))\n",
    "    authors_df['color'] = [palette[c] for c in authors_df['cluster']]\n",
    "\n",
    "    ds = ColumnDataSource(authors_df)\n",
    "    del authors_df['color']\n",
    "    x = authors_df['x']\n",
    "    y = authors_df['y']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(plot_width=plot_width, plot_height=plot_height,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange), \n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),    \n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\",\n",
    "               tooltips=[(\"Author\", '@author'),\n",
    "                         (\"Papers\", '@papers'),\n",
    "                         (\"Cited by\", '@cited'),\n",
    "                         (\"Cluster\", '@cluster'),\n",
    "                         (\"Tags\", '@tags')])\n",
    "\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    p.circle(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color')\n",
    "\n",
    "    lxs = [authors_df.loc[authors_df['cluster'] == c]['x'].mean() for c in sorted(clusters)]\n",
    "    lys = [authors_df.loc[authors_df['cluster'] == c]['y'].mean() for c in sorted(clusters)]\n",
    "    cluster_labels = [f'#{c}' for c in sorted(clusters)]\n",
    "    source = ColumnDataSource({'x': lxs, 'y': lys, 'name': cluster_labels})\n",
    "    labels = LabelSet(x='x', y='y', text='name', source=source, \n",
    "                      background_fill_color='white', text_font_size='11px', background_fill_alpha=.9)\n",
    "    p.renderers.append(labels)\n",
    "\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = 0\n",
    "authors_df['tags'] = 'n/a'\n",
    "show(plot_authors(authors_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.topics import cluster_and_sort\n",
    "\n",
    "author_clusters, _ = cluster_and_sort(authors_weighted_node_embeddings, 10, 100)\n",
    "\n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'cluster': author_clusters, \n",
    "                  'size': np.ones(len(author_clusters))}).groupby(['cluster']).sum().astype(int).reset_index()    \n",
    "sns.barplot(data=t, x='cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = author_clusters\n",
    "display(authors_df.head())\n",
    "\n",
    "logging.info('Saving authors and groups dataframes')\n",
    "authors_df.to_csv(f'{OUTPUT}/authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ego-splitting to compute possible overlapping groups of authors\n",
    "Taken from https://github.com/benedekrozemberczki/EgoSplitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EgoNetSplitter(object):\n",
    "    \"\"\"An implementation of `\"Ego-Splitting\" see:\n",
    "    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n",
    "    From the KDD '17 paper \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\".\n",
    "    The tool first creates the egonets of nodes.\n",
    "    A persona-graph is created which is clustered by the Louvain method.\n",
    "    The resulting overlapping cluster memberships are stored as a dictionary.\n",
    "    Args:\n",
    "        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution=1.0):\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def _create_egonet(self, node):\n",
    "        \"\"\"\n",
    "        Creating an ego net, extracting personas and partitioning it.\n",
    "\n",
    "        Args:\n",
    "            node: Node ID for egonet (ego node).\n",
    "        \"\"\"\n",
    "        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n",
    "        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n",
    "        new_mapping = {}\n",
    "        personalities = []\n",
    "        for k, v in components.items():\n",
    "            personalities.append(self.index)\n",
    "            for other_node in v:\n",
    "                new_mapping[other_node] = self.index\n",
    "            self.index = self.index+1\n",
    "        self.components[node] = new_mapping\n",
    "        self.personalities[node] = personalities\n",
    "\n",
    "    def _create_egonets(self):\n",
    "        \"\"\"\n",
    "        Creating an egonet for each node.\n",
    "        \"\"\"\n",
    "        self.components = {}\n",
    "        self.personalities = {}\n",
    "        self.index = 0\n",
    "        print(\"Creating egonets.\")\n",
    "        for node in tqdm(self.graph.nodes()):\n",
    "            self._create_egonet(node)\n",
    "\n",
    "    def _map_personalities(self):\n",
    "        \"\"\"\n",
    "        Mapping the personas to new nodes.\n",
    "        \"\"\"\n",
    "        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n",
    "\n",
    "    def _get_new_edge_ids(self, edge):\n",
    "        \"\"\"\n",
    "        Getting the new edge identifiers.\n",
    "        Args:\n",
    "            edge: Edge being mapped to the new identifiers.\n",
    "        \"\"\"\n",
    "        return (self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]])\n",
    "\n",
    "    def _create_persona_graph(self):\n",
    "        \"\"\"\n",
    "        Create a persona graph using the egonet components.\n",
    "        \"\"\"\n",
    "        print(\"Creating the persona graph.\")\n",
    "        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n",
    "        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n",
    "\n",
    "    def _create_partitions(self):\n",
    "        \"\"\"\n",
    "        Creating a non-overlapping clustering of nodes in the persona graph.\n",
    "        \"\"\"\n",
    "        print(\"Clustering the persona graph.\")\n",
    "        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n",
    "        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n",
    "        for node, membership in self.partitions.items():\n",
    "            self.overlapping_partitions[self.personality_map[node]].append(membership)\n",
    "\n",
    "    def fit(self, graph):\n",
    "        \"\"\"\n",
    "        Fitting an Ego-Splitter clustering model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be clustered.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self._create_egonets()\n",
    "        self._map_personalities()\n",
    "        self._create_persona_graph()\n",
    "        self._create_partitions()\n",
    "\n",
    "    def get_memberships(self):\n",
    "        r\"\"\"Getting the cluster membership of nodes.\n",
    "        Return types:\n",
    "            * **memberships** *(dictionary of lists)* - Cluster memberships.\n",
    "        \"\"\"\n",
    "        return self.overlapping_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = EgoNetSplitter(0.8)\n",
    "splitter.fit(authors_similarity_graph)\n",
    "\n",
    "ego_clusters = []\n",
    "for a, cs in splitter.overlapping_partitions.items():\n",
    "    ego_clusters.extend(cs)\n",
    "print('Total clusters', len(set(ego_clusters)))\n",
    "print('Clusters', Counter(ego_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze authors group topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def compute_tfidf(corpus_counts):\n",
    "    logger.debug('Compute TF-IDF on tokens counts')\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(corpus_counts)\n",
    "    logger.debug(f'TFIDF shape {tfidf.shape}')\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def compute_groups_topics(authors_df):\n",
    "    logging.info('Computing groups of authors topics')\n",
    "    groups_counts = \\\n",
    "        np.zeros(shape=(len(set(authors_df['cluster'])), analyzer.corpus_counts.shape[1]), dtype=np.float64)\n",
    "\n",
    "    part_sizes = Counter(authors_df['cluster'])\n",
    "    authors_clusters_map=dict(zip(authors_df['author'], authors_df['cluster']))\n",
    "\n",
    "    for i, row in tqdm(analyzer.df[['authors']].iterrows()):\n",
    "        for a in row[0].split(', '):\n",
    "            if a in authors_clusters_map:\n",
    "                group = authors_clusters_map[a]\n",
    "                groups_counts[group, :] += analyzer.corpus_counts[i, :] / part_sizes[group]\n",
    "\n",
    "    tfidf = compute_tfidf(groups_counts)\n",
    "\n",
    "    logging.info('Take terms with the largest tfidf for topics')\n",
    "    result = {}\n",
    "    for g in range(groups_counts.shape[0]):\n",
    "        counter = Counter()\n",
    "        for i, t in enumerate(analyzer.corpus_tokens):\n",
    "            counter[t] += tfidf[g, i]\n",
    "        # Ignore terms with insignificant frequencies\n",
    "        result[g] = [(t, f) for t, f in counter.most_common(10) if f > 0]\n",
    "    return result\n",
    "\n",
    "groups_topics = compute_groups_topics(authors_df)\n",
    "kwds = [(g, ','.join(f'{t}:{v:.3f}' for t, v in vs)) for g, vs in groups_topics.items()]\n",
    "logging.info('Description\\n' + '\\n'.join(f'{g}: {kwd}' for g, kwd in kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.DataFrame(columns=['group', 'authors', 'keywords'], dtype=object)\n",
    "for g in sorted(set(authors_df['cluster'])):\n",
    "    authors = ', '.join(authors_df.loc[authors_df['cluster'] == g]['author'])\n",
    "    groups_df.loc[len(groups_df)] = (g, authors, ','.join(t for t, _ in groups_topics[g]))\n",
    "\n",
    "display(groups_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Saving groups of authors with keywords')\n",
    "groups_df.to_csv(f'{OUTPUT}/groups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['tags'] = [', '.join(f'{t}:{v:.3f}' for t, v in groups_topics[c][:5]) for c in authors_df['cluster']]\n",
    "show(plot_authors(authors_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Saving author groups graph for bokeh')\n",
    "output_file(filename=f\"{OUTPUT}/authors.html\", title=\"Authors similarity graph\")\n",
    "save(plot_authors(authors_df, plot_width=1600, plot_height=1200))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "citations_graph = nx.Graph()\n",
    "for start, end in zip(analyzer.cit_df['id_out'], analyzer.cit_df['id_in']):\n",
    "    citations_graph.add_edge(start, end)\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(citations_graph, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(citations_graph, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in citations_graph:\n",
    "    for u in citations_graph[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['id'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "titles = []\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    sel = analyzer.df[analyzer.df['id'] == node]\n",
    "    year = sel['year'].values[0]\n",
    "    \n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    \n",
    "    titles.append(sel['title'].values[0])\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PMID</span>\n",
    "            <span style=\"font-size: 10px;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PageRank</span>\n",
    "            <span style=\"font-size: 10px;\">@pagerank</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['title'] = titles\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "# graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['id'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['id'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "# graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "# graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.min_border_left = 75\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Papers by PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pmid, pagerank in sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)[:10]:\n",
    "    print(f\"{(100*pagerank):.2f} {analyzer.df[analyzer.df['id'] == pmid]['title'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank and citation ranking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "analyzer.df['citation_rank'] = analyzer.df['total'].rank(method='first', ascending=False)\n",
    "pagerank_rank = sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)\n",
    "\n",
    "r = np.zeros((len(pagerank_rank), 2))\n",
    "for i, (pmid, pr) in enumerate(pagerank_rank):\n",
    "    sel = analyzer.df[analyzer.df['id'] == pmid]\n",
    "    if len(sel) > 0:\n",
    "        r[i, 0] = i\n",
    "        r[i, 1] = int(sel['citation_rank'].values[0])\n",
    "        \n",
    "TOP_X = [5, 10, 30, 50, 100]\n",
    "for x in TOP_X:\n",
    "    rho, _ = spearmanr(r[:x, 0], r[:x, 1])\n",
    "    print(f'Spearman correlation coefficient for top {x}: {rho}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}