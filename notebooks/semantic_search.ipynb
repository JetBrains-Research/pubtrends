{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic search with fasttext embeddings and postgresql\n",
    "\n",
    "* Create DB Postgresql + pgvector\n",
    "```\n",
    "docker run --rm --name pubtrends-postgres -p 5432:5432 \\\n",
    "        -m 32G \\\n",
    "        -e POSTGRES_USER=biolabs -e POSTGRES_PASSWORD=mysecretpassword \\\n",
    "        -e POSTGRES_DB=pubtrends \\\n",
    "        -v ~/postgres/:/var/lib/postgresql/data \\\n",
    "        -e PGDATA=/var/lib/postgresql/data/pgdata \\\n",
    "        -d pgvector/pgvector:pg17\n",
    "```\n",
    "\n",
    "* Launch fasttext endpoint API\n",
    "  `python pysrc/fasttext/fasttext_apy.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connections with main PubTrends database"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.config import PubtrendsConfig\n",
    "config = PubtrendsConfig(test=False)\n",
    "\n",
    "connection_string_full_db = f\"\"\"\n",
    "                    host={config.postgres_host} \\\n",
    "                    port={config.postgres_port} \\\n",
    "                    dbname={config.postgres_database} \\\n",
    "                    user={config.postgres_username} \\\n",
    "                    password={config.postgres_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.postgres_utils import ints_to_vals\n",
    "\n",
    "\n",
    "def load_publications(pids):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "    vals = ints_to_vals(pids)\n",
    "    query = f'''\n",
    "                SELECT P.pmid as id, title, abstract, year\n",
    "                FROM PMPublications P\n",
    "                WHERE P.pmid IN (VALUES {vals});\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        df = pd.DataFrame(cursor.fetchall(),\n",
    "                          columns=['id', 'title', 'abstract', 'year'],\n",
    "                          dtype=object)\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications_year(year):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "        query = f'''\n",
    "                SELECT P.pmid as id, title, abstract\n",
    "                FROM PMPublications P\n",
    "                WHERE year = {year};\n",
    "                '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = pd.DataFrame(cursor.fetchall(),\n",
    "                              columns=['id', 'title', 'abstract'],\n",
    "                              dtype=object)\n",
    "            return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# load_publications_year(2025).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import universal_chunk\n",
    "\n",
    "text = \"Staphylococcus aureus is a rare cause of postinfectious glomerulonephritis, and Staphylococcus-related glo-merulonephritis primarily occurs in middle-aged or elderly patients. Patients with Staphylococcus-related glomerulonephritis also present with hematuria, proteinuria of varying degrees, rising serum creatinine levels, and/or edema. The severity of renal insufficiency is proportional to the degree of proliferation and crescent formation. Here, we present a diabetic patient admitted with a history of 1 week of left elbow pain. Laboratory results revealed that erythrocyte sedimentation rate was 110 mm/hour, serum creatinine level was 1 mg/dL, C-reactive protein level was 150 mg/L, and magnetic resonance imaging showed signal changes in favor of osteomyelitis at the olecranon level, with diffuse edematous appearance in the elbow skin tissue and increased intra-articular effusion. After diagnosis of osteomyelitis, ampicillin/sulbactam and teicoplanin were administered. After day 7 of admission, the patient developed acute kidney injury requiring hemodialysis under antibiotic treatment. Kidney biopsy was performed to determine the underlying cause, which showed Staphylococcus-related glomerulonephritis. Recovery of renal func-tions was observed after antibiotic and supportive treatment.\"\n",
    "\n",
    "chunks = universal_chunk(text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import process_paper_chunks\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "def parallel_collect_chunks(pids, texts):\n",
    "    # Default to number of CPUs for max workers\n",
    "    max_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    chunks = []\n",
    "    chunk_idx = []\n",
    "\n",
    "    # Process texts in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create a list of (pid, text) tuples for processing\n",
    "        # Using index as pid for demonstration\n",
    "        text_args = [(pid, text) for pid, text in zip(pids, texts)]\n",
    "\n",
    "        # Submit all tasks and wait for results\n",
    "        results = list(executor.map(process_paper_chunks, text_args))\n",
    "\n",
    "    # Combine results\n",
    "    for text_chunks, text_chunk_idx in results:\n",
    "        chunks.extend(text_chunks)\n",
    "        chunk_idx.extend(text_chunk_idx)\n",
    "\n",
    "    return chunks, chunk_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare Postgresql + pgvector for embeddings search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "semantics_search_host = 'localhost'\n",
    "semantics_search_port = 5432\n",
    "semantics_search_database = 'pubtrends'\n",
    "semantics_search_username = 'biolabs'\n",
    "semantics_search_password = 'mysecretpassword'\n",
    "\n",
    "semantics_search_connection_string = f\"\"\"\n",
    "                    host={semantics_search_host} \\\n",
    "                    port={semantics_search_port} \\\n",
    "                    dbname={semantics_search_database} \\\n",
    "                    user={semantics_search_username} \\\n",
    "                    password={semantics_search_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "drop table if exists PMPublicationsSmall;\n",
    "create table PMPublicationsSmall(\n",
    "pmid    integer,\n",
    "title   varchar(1023),\n",
    "abstract text\n",
    ");\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            drop table if exists PMPublicationsEmbeddings;\n",
    "            create table PMPublicationsEmbeddings(\n",
    "                                                pmid    integer,\n",
    "                                                chunk   integer,\n",
    "                                                embedding vector(200)\n",
    "            );\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an index for fast vector similarity search\n",
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "            CREATE INDEX texts_embedding_idx\n",
    "                ON PMPublicationsEmbeddings\n",
    "                USING ivfflat (embedding vector_cosine_ops)\n",
    "                WITH (lists = 100);\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute embeddings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.fasttext.fasttext import PretrainedModelCache\n",
    "\n",
    "# Use a local embeddings model, and dispose it after\n",
    "model_cache = PretrainedModelCache()\n",
    "model_cache.download_and_load_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_embedding_fasttext(text, model_instance):\n",
    "    tokens = text.split()\n",
    "    vectors = [\n",
    "            model_instance.get_vector(t) if model_instance.has_index_for(t)\n",
    "            else np.zeros(model_instance.vector_size)  # Support out-of-dictionary missing embeddings\n",
    "            for t in tokens\n",
    "        ]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model_instance.vector_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(text_embedding_fasttext(text, model_instance=model_cache.download_and_load_model))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import concurrent\n",
    "\n",
    "def parallel_texts_embeddings(texts, model_instance):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(\n",
    "            lambda t: text_embedding_fasttext(t, model_instance), t\n",
    "        ) for t in texts]\n",
    "        return [future.result() for future in concurrent.futures.as_completed(futures)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Insert embeddings into Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from more_itertools import sliced\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "for year in range(2025, 2024, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    df = load_publications_year(year)\n",
    "    print('Storing embeddings into DB')\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for index_slice in tqdm(list(index_slices)):\n",
    "        chunk = df.iloc[index_slice]\n",
    "        print('\\rCollecting chunks           ', end='')\n",
    "        pids = list(chunk['id'])\n",
    "        texts = [f'{title}. {abstract}' for title, abstract in zip(chunk['title'], chunk['abstract'])]\n",
    "        chunks, chunk_idx = parallel_collect_chunks(pids, texts)\n",
    "        print('\\rCompute embeddings          ', end='')\n",
    "        chunk_embeddings = parallel_texts_embeddings(chunks, model_cache.download_and_load_model)\n",
    "        print(f'\\rStoring {len(chunk_embeddings)} embeddings', end='')\n",
    "        data = [(pmid, chunk, e) for (pmid, chunk), e in zip(chunk_idx, chunk_embeddings)]\n",
    "        with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                execute_values(\n",
    "                    cursor,\n",
    "                    \"INSERT INTO PMPublicationsEmbeddings (pmid, chunk, embedding) VALUES %s\",\n",
    "                    data\n",
    "                )\n",
    "            connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Cleanup memory\n",
    "del model_cache"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import is_fasttext_endpoint_ready\n",
    "import time\n",
    "\n",
    "i = 0\n",
    "while not is_fasttext_endpoint_ready():\n",
    "    print('\\rWaiting for fasttext endpoint to be ready' + '.' * i, end='')\n",
    "    i += 1\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import fetch_fasttext_text_embedding\n",
    "\n",
    "len(fetch_fasttext_text_embedding(text))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_postgresql(query, k=5):\n",
    "    embedding = fetch_fasttext_text_embedding(query)\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                   SELECT pmid, chunk, embedding <=> %s::vector AS distance\n",
    "                   FROM PMPublicationsEmbeddings\n",
    "                   ORDER BY distance ASC\n",
    "                       LIMIT %s\n",
    "                   \"\"\", (embedding, k))\n",
    "\n",
    "            results = cursor.fetchall()\n",
    "            return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search = semantic_search_postgresql(\"epigenetic human aging\", 1000)\n",
    "pmids = [pid for pid, _, _ in search]\n",
    "load_publications(pmids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config.topic_min_size = 5\n",
    "try:\n",
    "    analyzer.analyze_papers(pmids, 10)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bokeh.plotting import show\n",
    "from itertools import chain\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "\n",
    "analyzer.search_ids = pmids\n",
    "plotter = Plotter(config, analyzer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.plot_papers_graph())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.topics_hierarchy_with_keywords())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Insert embeddings into Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use a local embeddings model, and dispose it after\n",
    "model_cache = PretrainedModelCache()\n",
    "model_cache.download_and_load_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! mkdir -p ~/faiss\n",
    "import faiss\n",
    "\n",
    "FAISS_INDEX_FILE = os.path.expanduser('~/faiss/embeddings.index')\n",
    "PIDS_INDEX_FILE = os.path.expanduser('~/faiss/pids.csv.gz')\n",
    "\n",
    "def create_or_load_faiss():\n",
    "    if os.path.exists(FAISS_INDEX_FILE):\n",
    "        print('Creating Faiss index from existing file')\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "    else:\n",
    "        print('Creating empty Faiss index')\n",
    "        # Using HNSW index (good performance at scale)\n",
    "        index = faiss.IndexHNSWFlat(200, 32)  # 32 = number of neighbors in HNSW graph\n",
    "        index.hnsw.efConstruction = 100  # higher value improves accuracy (at expense of index build time)\n",
    "    if os.path.exists(PIDS_INDEX_FILE):\n",
    "        pids_idx = pd.read_csv(PIDS_INDEX_FILE, compression='gzip')\n",
    "    else:\n",
    "        pids_idx = pd.DataFrame(data=[], columns=['pmid', 'chunk'], dtype=int)\n",
    "    return index, pids_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from more_itertools import sliced\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "faiss_index, pids_idx = None, None\n",
    "\n",
    "for year in range(2025, 2024, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    df = load_publications_year(year)\n",
    "    if faiss_index is None:\n",
    "        faiss_index, pids_idx = create_or_load_faiss()\n",
    "\n",
    "    print('Computing embeddings')\n",
    "    chunk_idx_all = []\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for i, index_slice in tqdm(list(enumerate(index_slices))):\n",
    "        chunk = df.iloc[index_slice]\n",
    "        print('\\rCollecting chunks           ', end='')\n",
    "        pids = list(chunk['id'])\n",
    "        texts = [f'{title}. {abstract}' for title, abstract in zip(chunk['title'], chunk['abstract'])]\n",
    "        chunks, chunk_idx = parallel_collect_chunks(pids, texts)\n",
    "        print('\\rCompute embeddings          ', end='')\n",
    "        chunk_embeddings = parallel_texts_embeddings(chunks, model_cache.download_and_load_model)\n",
    "        print(f'\\rStoring {len(chunk_embeddings)} embeddings', end='')\n",
    "        embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "        # Normalize embeddings if using cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        # Add embeddings to the index\n",
    "        faiss_index.add(embeddings)\n",
    "        chunk_idx_all.extend(chunk_idx)\n",
    "\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print('Storing FAISS index')\n",
    "            faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "            print('Storing Ids index')\n",
    "            new_pids_idx = pd.DataFrame(chunk_idx_all, columns=['pmid', 'chunk'])\n",
    "            t = pd.concat([pids_idx, new_pids_idx]).reset_index(drop=True)\n",
    "            t.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "    print('Storing FAISS index')\n",
    "    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "    print('Storing Ids index')\n",
    "    new_pids_idx = pd.DataFrame(chunk_idx_all, columns=['pmid', 'chunk'])\n",
    "    t = pd.concat([pids_idx, new_pids_idx]).reset_index(drop=True)\n",
    "    t.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "    pids_idx = t\n",
    "    print('Done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del model_cache\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "faiss_index, pids_idx = create_or_load_faiss()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_faiss(query_text, faiss_index, df, top_k=5):\n",
    "    query_vector = text_embedding_fasttext(query_text, model_cache.download_and_load_model).astype('float32')\n",
    "    faiss.normalize_L2(query_vector.reshape(1, -1))\n",
    "    distances, indices = faiss_index.search(query_vector.reshape(1, -1), top_k)\n",
    "    return df.iloc[indices[0]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search = semantic_search_faiss(\"epigenetic human aging\", faiss_index, pids_idx, 1000)\n",
    "pmids = search['pmid']\n",
    "load_publications(pmids)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
