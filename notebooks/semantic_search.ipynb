{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic search\n",
    "\n",
    "Ready to go models.\n",
    "\n",
    "| Model Name                                                               | Type                | Dim | Quality (Biomedical)                | Speed (CPU)    | Memory Usage            | Sentence-Level Optimized |\n",
    "| ------------------------------------------------------------------------ | ------------------- | --- | ----------------------------------- | -------------- | ----------------------- | ------------------------ |\n",
    "| **BioWordVec (BioSentVec)**<br>`BioWordVec_PubMed_MIMICIII_d200.vec.bin` | Static (word-level) | 200 | ‚ö†Ô∏è Low‚ÄìModerate                     | ‚úÖ‚úÖ‚úÖ Very Fast  | ‚úÖ Very Low (\\~1 GB RAM) | ‚ùå No                     |\n",
    "| **`all-MiniLM-L6-v2`**                                                   | SBERT (MiniLM)      | 384 | ‚úÖ Moderate (general)                | ‚úÖ‚úÖ‚úÖ Very Fast  | ‚úÖ Low (\\~80 MB)         | ‚úÖ Yes                    |\n",
    "| **`pritamdeka/S-PubMedBert-MS-MARCO`**                                   | SBERT (PubMedBERT)  | 768 | ‚úÖ‚úÖ‚úÖ Excellent                       | ‚ö†Ô∏è Medium      | ‚ö†Ô∏è Moderate-High        | ‚úÖ Yes                    |\n",
    "| **`thenlper/gte-base`**                                                  | GTE (BERT)          | 768 | ‚úÖ‚úÖ Good                             | ‚úÖ‚úÖ Fast        | ‚úÖ Moderate (\\~400 MB)   | ‚úÖ Yes                    |\n",
    "| **`nomic-ai/nomic-embed-text-v1.5`**                                     | OpenCLIP-style      | 768 | ‚úÖ‚úÖ Very Good (general + scientific) | ‚ö†Ô∏è Medium-Slow | ‚ùó High (\\~1 GB+)        | ‚ö†Ô∏è Partial (CLS token)   |\n",
    "| **`microsoft/BiomedNLP-PubMedBERT...`**                                  | Raw BERT            | 768 | ‚úÖ‚úÖ‚úÖ Best-in-domain                  | üê¢ Slow        | ‚ùó High (\\~1.2 GB)       | ‚ùå No (needs pooling)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import psycopg2\n",
    "from more_itertools import sliced\n",
    "from math import ceil\n",
    "import concurrent\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "from pysrc.papers.db.postgres_utils import ints_to_vals\n",
    "from pysrc.config import PubtrendsConfig\n",
    "config = PubtrendsConfig(test=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configures weather to use Postgres index or use Fast index in Faiss\n",
    "EXACT_SEARCH = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connections with main PubTrends database"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "connection_string_full_db = f\"\"\"\n",
    "                    host={config.postgres_host} \\\n",
    "                    port={config.postgres_port} \\\n",
    "                    dbname={config.postgres_database} \\\n",
    "                    user={config.postgres_username} \\\n",
    "                    password={config.postgres_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications(pids):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "    vals = ints_to_vals(pids)\n",
    "    query = f'''\n",
    "                SELECT P.pmid as id, title, abstract, type\n",
    "                FROM PMPublications P\n",
    "                WHERE P.pmid IN (VALUES {vals});\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        df = pd.DataFrame(cursor.fetchall(),\n",
    "                          columns=['id', 'title', 'abstract', 'type'],\n",
    "                          dtype=object)\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications_year(year):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "        query = f'''\n",
    "                SELECT P.pmid as id, title, abstract\n",
    "                FROM PMPublications P\n",
    "                WHERE year = {year}\n",
    "                ORDER BY pmid;\n",
    "                '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = pd.DataFrame(cursor.fetchall(),\n",
    "                              columns=['id', 'title', 'abstract'],\n",
    "                              dtype=object)\n",
    "            return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "! mkdir -p ~/pubtrends_years\n",
    "\n",
    "def fetch_year(year):\n",
    "    path = os.path.expanduser(f'~/pubtrends_years/{year}.csv.gz')\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            return pd.read_csv(path, compression='gzip')\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error reading {path}: {e}')\n",
    "            os.path.remove(path)\n",
    "            return fetch_year(year)\n",
    "    else:\n",
    "        df = load_publications_year(year)\n",
    "        df.to_csv(path, index=None, compression='gzip')\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# load_publications_year(2025).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import get_chunks\n",
    "\n",
    "MAX_TOKENS = 128\n",
    "\n",
    "text = \"Staphylococcus aureus is a rare cause of postinfectious glomerulonephritis, and Staphylococcus-related glo-merulonephritis primarily occurs in middle-aged or elderly patients. Patients with Staphylococcus-related glomerulonephritis also present with hematuria, proteinuria of varying degrees, rising serum creatinine levels, and/or edema. The severity of renal insufficiency is proportional to the degree of proliferation and crescent formation. Here, we present a diabetic patient admitted with a history of 1 week of left elbow pain. Laboratory results revealed that erythrocyte sedimentation rate was 110 mm/hour, serum creatinine level was 1 mg/dL, C-reactive protein level was 150 mg/L, and magnetic resonance imaging showed signal changes in favor of osteomyelitis at the olecranon level, with diffuse edematous appearance in the elbow skin tissue and increased intra-articular effusion. After diagnosis of osteomyelitis, ampicillin/sulbactam and teicoplanin were administered. After day 7 of admission, the patient developed acute kidney injury requiring hemodialysis under antibiotic treatment. Kidney biopsy was performed to determine the underlying cause, which showed Staphylococcus-related glomerulonephritis. Recovery of renal func-tions was observed after antibiotic and supportive treatment.\"\n",
    "\n",
    "chunks = get_chunks(text, MAX_TOKENS)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings with Sentence Transformer"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.embeddings.sentence_transformer.sentence_transformer import SentenceTransformerModel\n",
    "\n",
    "sentence_transformer_model = SentenceTransformerModel()\n",
    "sentence_transformer_model.download_and_load_model\n",
    "emb = sentence_transformer_model.encode(['This is a test.', 'This is a test2'])\n",
    "print(emb.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_dimension = emb.shape[1]\n",
    "text_embedding = lambda t: sentence_transformer_model.encode(t)\n",
    "batch_texts_embeddings = sentence_transformer_model.encode_parallel\n",
    "embeddings_model = sentence_transformer_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = sentence_transformer_model.device\n",
    "embeddings_model_name = 'all_MiniLM_L6_v2'\n",
    "embedding_dimension = 384"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings with HugginFace Wrapper model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from more_itertools import sliced\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "#\n",
    "# if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = 'mps'\n",
    "# elif torch.cuda.is_available():\n",
    "#     device = 'gpu'\n",
    "# else:\n",
    "#     device = 'cpu'\n",
    "#\n",
    "# class SentenceTransformerWrapper:\n",
    "#     def __init__(self, model_name, attention):\n",
    "#         print(f'Loading model into {device}')\n",
    "#         self.device = device\n",
    "#         self.attention = attention\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "#         self.model.eval()\n",
    "#\n",
    "#     @staticmethod\n",
    "#     def mean_pooling(model_output, attention_mask):\n",
    "#         token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#         summed = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "#         summed_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "#         return summed / summed_mask\n",
    "#\n",
    "#     def encode(self, sentences, batch_size=32):\n",
    "#         all_embeddings = []\n",
    "#\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(list(sliced(sentences, batch_size))):\n",
    "#                 inputs = self.tokenizer(\n",
    "#                     batch,\n",
    "#                     return_tensors=\"pt\",\n",
    "#                     padding=True,\n",
    "#                     truncation=True,\n",
    "#                     max_length=1024,\n",
    "#                 ).to(self.device)\n",
    "#\n",
    "#                 outputs = self.model(**inputs)\n",
    "#                 if self.attention:\n",
    "#                     embeddings = SentenceTransformerWrapper.mean_pooling(outputs, inputs['attention_mask'])\n",
    "#                 else:\n",
    "#                     embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#\n",
    "#                 all_embeddings.append(embeddings.cpu().numpy())\n",
    "#\n",
    "#         return np.vstack(all_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Decent model for biomedical embeddings\n",
    "# # wrapped_model = SentenceTransformerWrapper(\"nomic-ai/nomic-embed-text-v1.5\", False)\n",
    "# # Also good, and slightly faster than nomic-embed\n",
    "# wrapped_model = SentenceTransformerWrapper(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", True)\n",
    "# embeddings = wrapped_model.encode('Test sentence')\n",
    "# embeddings.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from more_itertools import sliced\n",
    "# from math import ceil\n",
    "# import concurrent\n",
    "# import multiprocessing\n",
    "# import numpy as np\n",
    "#\n",
    "# def parallel_texts_embeddings_wrapper(texts):\n",
    "#     if device != 'cpu':\n",
    "#         return wrapped_model.encode(texts)\n",
    "#     # Default to number of CPUs for max workers\n",
    "#     max_workers = multiprocessing.cpu_count()\n",
    "#     # Compute parallel on different threads, since we use the same fasttext model\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         futures = [\n",
    "#             executor.submit(lambda ts: wrapped_model.encode(ts), ts)\n",
    "#                    for ts in sliced(texts, int(ceil(len(texts) / max_workers)))\n",
    "#         ]\n",
    "#         # Important: keep order of results!!!\n",
    "#         return np.vstack([future.result() for future in futures])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# embeddings_model_name = BiomedNLP_PubMedBERT\n",
    "# text_embedding = lambda t: wrapped_model.encode([t])\n",
    "# batch_texts_embeddings = parallel_texts_embeddings_wrapper\n",
    "# embeddings_model = wrapped_model\n",
    "# embedding_dimension = embeddings.shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prepare Postgresql + pgvector for embeddings search\n",
    "\n",
    "Create DB Postgresql + pgvector\n",
    "```\n",
    "docker run --rm --name pubtrends-postgres -p 5432:5432 \\\n",
    "        -m 32G \\\n",
    "        -e POSTGRES_USER=biolabs -e POSTGRES_PASSWORD=mysecretpassword \\\n",
    "        -e POSTGRES_DB=pubtrends \\\n",
    "        -v ~/pgvector/:/var/lib/postgresql/data \\\n",
    "        -e PGDATA=/var/lib/postgresql/data/pgdata \\\n",
    "        -d pgvector/pgvector:pg17\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "semantics_search_host = 'localhost'\n",
    "semantics_search_port = 5432\n",
    "semantics_search_database = 'pubtrends'\n",
    "semantics_search_username = 'biolabs'\n",
    "semantics_search_password = 'mysecretpassword'\n",
    "\n",
    "semantics_search_connection_string = f\"\"\"\n",
    "                    host={semantics_search_host} \\\n",
    "                    port={semantics_search_port} \\\n",
    "                    dbname={semantics_search_database} \\\n",
    "                    user={semantics_search_username} \\\n",
    "                    password={semantics_search_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embeddings DB initialization\n",
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = f'''\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            create table {embeddings_model_name}(\n",
    "                pmid    integer,\n",
    "                chunk   integer,\n",
    "                embedding vector({embedding_dimension})\n",
    "            );\n",
    "            CREATE INDEX pmid_chunk_idx_{embeddings_model_name}\n",
    "            ON {embeddings_model_name}(pmid, chunk);\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not EXACT_SEARCH:\n",
    "    # Create an index for fast vector similarity search using cosine distance\n",
    "    # Index may slightly change results vs exact match search, but it's much faster!\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        connection.set_session(readonly=False)\n",
    "        query = f'''\n",
    "                CREATE INDEX embedding_idx_{embeddings_model_name}\n",
    "                ON {embeddings_model_name}\n",
    "                USING ivfflat (embedding vector_cosine_ops)\n",
    "                WITH (lists = 100);\n",
    "            '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "        connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collect_ids_without_embeddings(pids):\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "    vals = ints_to_vals(pids)\n",
    "    query = f'''\n",
    "                SELECT pmid\n",
    "                FROM {embeddings_model_name} P\n",
    "                WHERE P.pmid IN (VALUES {vals});\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        df = pd.DataFrame(cursor.fetchall(), columns=['pmid'], dtype=object)\n",
    "        pids_with_embeddings = set(df['pmid'])\n",
    "        return [pid for pid in pids if pid not in pids_with_embeddings]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Store embeddings into Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def l2norm(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        norm = np.finfo(v.dtype).eps\n",
    "    v /= norm\n",
    "    return v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def store_embeddings_to_postgresql(chunk_embeddings, chunk_idx):\n",
    "    # Normalize embeddings if using cosine similarity\n",
    "    data = [(pmid, chunk, l2norm(e).tolist())\n",
    "            for (pmid, chunk), e in zip(chunk_idx, chunk_embeddings)]\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            execute_values(\n",
    "                cursor,\n",
    "                f\"INSERT INTO {embeddings_model_name} (pmid, chunk, embedding) VALUES %s\",\n",
    "                data\n",
    "            )\n",
    "        connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import parallel_collect_chunks\n",
    "\n",
    "# Thread safe queue to store chunks\n",
    "chunks_queue = queue.Queue()\n",
    "\n",
    "# Thread safe queue to store embeddings\n",
    "embeddings_queue = queue.Queue()\n",
    "\n",
    "def empty_embeddings_queue():\n",
    "    while not chunks_queue.empty():\n",
    "        chunks_queue.get()\n",
    "    while not embeddings_queue.empty():\n",
    "        embeddings_queue.get()\n",
    "\n",
    "def collect_chunks_work(pids, texts):\n",
    "    if pids is None or texts is None:\n",
    "        return\n",
    "    chunks, chunk_idx = parallel_collect_chunks(pids, texts, MAX_TOKENS, 1)\n",
    "    chunks_queue.put((chunks, chunk_idx))\n",
    "\n",
    "def compute_embeddings_work():\n",
    "    try:\n",
    "        chunks, chunk_idx = chunks_queue.get_nowait()  # Non-blocking\n",
    "        chunk_embeddings = batch_texts_embeddings(chunks)\n",
    "        embeddings_queue.put((chunk_embeddings, chunk_idx))\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "\n",
    "def store_embeddings_work():\n",
    "    try:\n",
    "        chunk_embeddings, chunk_idx = embeddings_queue.get_nowait()  # Non-blocking\n",
    "        store_embeddings_to_postgresql(chunk_embeddings, chunk_idx)\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "\n",
    "def compute_embeddings_and_store_work():\n",
    "    try:\n",
    "        chunks, chunk_idx = chunks_queue.get_nowait()  # Non-blocking\n",
    "        chunk_embeddings = batch_texts_embeddings(chunks)\n",
    "        store_embeddings_to_postgresql(chunk_embeddings, chunk_idx)\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "\n",
    "def process_cpu_work(pids, texts):\n",
    "    # Create threads\n",
    "    threads = [\n",
    "        threading.Thread(target=collect_chunks_work, args=(pids, texts)),\n",
    "        threading.Thread(target=compute_embeddings_and_store_work, args=()),\n",
    "    ]\n",
    "    # Start the threads\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    # Wait for both threads to complete\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "def process_gpu_work(pids, texts):\n",
    "    # Create threads\n",
    "    threads = [\n",
    "        threading.Thread(target=collect_chunks_work, args=(pids, texts)),\n",
    "        threading.Thread(target=compute_embeddings_work, args=()),\n",
    "        threading.Thread(target=store_embeddings_work, args=())\n",
    "    ]\n",
    "    # Start the threads\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    # Wait for both threads to complete\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "def process_work(pids, texts):\n",
    "    if device == 'cpu':\n",
    "        # Parallel chunking + embeddins | store\n",
    "        process_cpu_work(pids, texts)\n",
    "    else:\n",
    "        # Parallel chunkin | embeddings | store\n",
    "        process_gpu_work(pids, texts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from more_itertools import sliced\n",
    "\n",
    "CHUNK_SIZE = 10_000\n",
    "\n",
    "empty_embeddings_queue()\n",
    "\n",
    "for year in range(2025, 1969, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    df = fetch_year(year)\n",
    "    pids_to_process = set(collect_ids_without_embeddings(df['id']))\n",
    "    print(f'To process {len(pids_to_process)}')\n",
    "    df = df[df['id'].isin(pids_to_process)]\n",
    "\n",
    "    print('Storing embeddings into DB')\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for index_slice in tqdm(list(index_slices)):\n",
    "        print(f'\\rProcessing chunks {index_slice[0]}-{index_slice[-1]}          ', end='')\n",
    "        chunk_df = df.iloc[index_slice]\n",
    "        pids_chunk = list(chunk_df['id'])\n",
    "        texts = [f'{title}. {abstract}' for title, abstract in zip(chunk_df['title'], chunk_df['abstract'])]\n",
    "        process_work(pids, texts)\n",
    "    # Finally, process the work left in the queue\n",
    "    for _ in range(3):\n",
    "        process_work(None, None)\n",
    "\n",
    "    print('Done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(f'SELECT COUNT(*) FROM {embeddings_model_name}')\n",
    "        total_rows = cursor.fetchone()[0]\n",
    "        print(f'Total embeddings: {total_rows}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_postgresql(query, k):\n",
    "    query_vector = text_embedding(query)\n",
    "    # Normalize embeddings if using cosine similarity\n",
    "    embedding = l2norm(query_vector).tolist()\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(f\"\"\"\n",
    "                   SELECT pmid, chunk, embedding <=> %s::vector AS distance\n",
    "                   FROM {embeddings_model_name}\n",
    "                   ORDER BY distance\n",
    "                   LIMIT %s\n",
    "                   \"\"\", (embedding, k))\n",
    "\n",
    "            results = cursor.fetchall()\n",
    "            return pd.DataFrame(data=results, columns=['pmid', 'chunk', 'distance'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_pg = semantic_search_postgresql(\"epigenetic human aging\", 1000)\n",
    "search_pg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pmids_pg = search_pg['pmid']\n",
    "len(pmids_pg.unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Store embeddings into Faiss from Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "\n",
    "! mkdir -p ~/faiss_{embeddings_model_name}\n",
    "\n",
    "FAISS_INDEX_FILE = os.path.expanduser(f'~/faiss_{embeddings_model_name}/embeddings.index')\n",
    "PIDS_INDEX_FILE = os.path.expanduser(f'~/faiss_{embeddings_model_name}/pids.csv.gz')\n",
    "\n",
    "def create_faiss():\n",
    "    if EXACT_SEARCH:\n",
    "        print('Exact search index')\n",
    "        index = faiss.IndexFlatIP(embedding_dimension)\n",
    "    else:\n",
    "        print('Approximate search index')\n",
    "        quantizer = faiss.IndexFlatL2(embedding_dimension)\n",
    "        index = faiss.IndexIVFPQ(quantizer, embedding_dimension, 200, 16, 8)\n",
    "    return index\n",
    "\n",
    "def create_or_load_faiss():\n",
    "    if os.path.exists(FAISS_INDEX_FILE):\n",
    "        print('Loading Faiss index from existing file')\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        # For accurate search\n",
    "        index.nprobe = 200\n",
    "    else:\n",
    "        print('Creating empty Faiss index')\n",
    "        index = create_faiss()\n",
    "    if os.path.exists(PIDS_INDEX_FILE):\n",
    "        pids_idx = pd.read_csv(PIDS_INDEX_FILE, compression='gzip')\n",
    "    else:\n",
    "        pids_idx = pd.DataFrame(data=[], columns=['pmid', 'chunk', 'year', 'noreview'], dtype=int)\n",
    "    return index, pids_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import ast  # For safely converting string to list\n",
    "\n",
    "def collect_pids_types_year(year):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "        query = f'''SELECT pmid\n",
    "                FROM PMPublications P\n",
    "                WHERE year = {year}\n",
    "                ORDER BY pmid;\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        return [v[0] for v in cursor.fetchall()]\n",
    "\n",
    "def sample_embeddings(n=10_000):\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            print('Sampling training data')\n",
    "            query = f\"\"\"\n",
    "                    SELECT embedding FROM {embeddings_model_name}\n",
    "                    TABLESAMPLE SYSTEM (0.1)  -- Approx. 0.1% of table\n",
    "                    LIMIT {n};\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            embeddings = [np.array(ast.literal_eval(row[0])) for row in cursor.fetchall()]\n",
    "            return embeddings\n",
    "\n",
    "def load_embeddings_by_ids(pids):\n",
    "    vals = ints_to_vals(pids)\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            query = f\"\"\"\n",
    "                    SELECT pmid, chunk, embedding FROM {embeddings_model_name}\n",
    "                    WHERE pmid IN (VALUES {vals})\n",
    "                    ORDER BY pmid, chunk;\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            result = cursor.fetchall()\n",
    "            index = [(pmid, chunk) for pmid, chunk, _ in result]\n",
    "            embeddings = [np.array(ast.literal_eval(row[2])) for row in result]\n",
    "            return index, embeddings\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Thread safe queue to store embeddings\n",
    "embeddings_pg_queue = queue.Queue()\n",
    "\n",
    "def empty_to_fs_queue():\n",
    "    # Empty the queue safely\n",
    "    while not embeddings_pg_queue.empty():\n",
    "        embeddings_pg_queue.get()\n",
    "\n",
    "def load_embeddings_pg_work(pids):\n",
    "    if len(pids) == 0:\n",
    "        return\n",
    "    index, embeddings = load_embeddings_by_ids(pids)\n",
    "    embeddings_pg_queue.put((index, embeddings))\n",
    "\n",
    "def store_embeddings_fs_work():\n",
    "    global faiss_index\n",
    "    global pids_idx\n",
    "    try:\n",
    "        index, embeddings = embeddings_pg_queue.get_nowait()  # Non-blocking\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        if (len(embeddings.shape) == 1 or\n",
    "                embeddings.shape[1] != embedding_dimension or\n",
    "                len(index) != embeddings.shape[0]):\n",
    "            print(f'Problematic chunk embeddings, {embeddings.shape}')\n",
    "            return\n",
    "        t = pd.DataFrame(data=index, columns=['pmid', 'chunk'])\n",
    "        pids_idx = pd.concat([pids_idx, t], ignore_index=True).reset_index(drop=True)\n",
    "        faiss_index.add(embeddings)\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "\n",
    "def process_store_embeddings_work(pids):\n",
    "    # Create threads\n",
    "    threads = [\n",
    "        threading.Thread(target=load_embeddings_pg_work, args=([pids])),\n",
    "        threading.Thread(target=store_embeddings_fs_work, args=())\n",
    "    ]\n",
    "    # Start the threads\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    # Wait for both threads to complete\n",
    "    for t in threads:\n",
    "        t.join()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "faiss_index, pids_idx = create_or_load_faiss()\n",
    "\n",
    "if not EXACT_SEARCH and len(pids_idx) == 0:\n",
    "    embeddings = sample_embeddings()\n",
    "    embeddings = np.array(embeddings).astype('float32')\n",
    "    print(f'Training index on {embeddings.shape[0]} embeddings')\n",
    "    faiss_index.train(embeddings)\n",
    "\n",
    "assert len(pids_idx) == faiss_index.ntotal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CHUNK_SIZE = 1000\n",
    "\n",
    "empty_to_fs_queue()\n",
    "\n",
    "for year in range(2025, 2015, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    pids_year = collect_pids_types_year(year)\n",
    "    pids_year = list(set(pids_year) - set(pids_idx['pmid']))\n",
    "    print(f'To process {len(pids_year)}')\n",
    "    if len(pids_year) == 0:\n",
    "        continue\n",
    "\n",
    "    for i, chunk in tqdm(list(enumerate(sliced(pids_year, CHUNK_SIZE)))):\n",
    "        chunk_offset = i * CHUNK_SIZE\n",
    "        print(f'\\rProcessing embeddings {chunk_offset}-{min(len(pids_year), chunk_offset + CHUNK_SIZE)}      ', end='')\n",
    "        process_store_embeddings_work(chunk)\n",
    "\n",
    "    # Finally, process the work left in the queue\n",
    "    for _ in range(3):\n",
    "        process_store_embeddings_work([])\n",
    "\n",
    "    print('Storing FAISS index')\n",
    "    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "    print('Storing Ids index')\n",
    "    pids_idx.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "\n",
    "print('Done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(pids_idx))\n",
    "print(faiss_index.ntotal)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "faiss_index, pids_idx = create_or_load_faiss()\n",
    "print(len(pids_idx))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_faiss(query_text, k):\n",
    "    query_vector = text_embedding(query_text).reshape(1, -1)\n",
    "    # Normalize embeddings if using cosine similarity\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    similarities, indices = faiss_index.search(query_vector.astype('float32'), k)\n",
    "    t = pids_idx.iloc[indices[0]].copy().reset_index(drop=True)\n",
    "    t['similarity'] = similarities[0]\n",
    "    return t"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_fs = semantic_search_faiss(\"epigenetic human aging\", 10_000)\n",
    "search_fs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# At some point, vectors are similar and too far away from the request vector, which limits the number of results\n",
    "pmids_fs = search_fs['pmid']\n",
    "len(pmids_fs.unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_publications(pmids_fs.unique())['title']",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison Postgresql vs Faiss semantic search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(f'Postgresql {len(pmids_pg.unique())}')\n",
    "# print(f'Faiss {len(pmids_fs.unique())}')\n",
    "# overlap = set(list(pmids_pg)) & set(list(pmids_fs))\n",
    "# print(f'Overlap {len(overlap)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Apply additional semantic filtering on search results"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search = semantic_search_faiss(\n",
    "    \"epigenetic changes in stem cell differentiation in human\",\n",
    "    1000\n",
    ")\n",
    "search_ids = search['pmid']\n",
    "print(len(search_ids.unique()))\n",
    "search"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "publications = load_publications(search_ids)\n",
    "search_ids = publications['id']\n",
    "publications.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collect_chunks_embeddings(df):\n",
    "    print('\\rCollecting chunks           ', end='')\n",
    "    pids = list(df['id'])\n",
    "    texts = [f'{title}. {abstract}' for title, abstract in zip(df['title'], df['abstract'])]\n",
    "    chunks, chunk_idx = parallel_collect_chunks(pids, texts, MAX_TOKENS)\n",
    "    print(f'\\rComputing {len(chunks)} embeddings   ', end='')\n",
    "    chunk_embeddings = batch_texts_embeddings(chunks)\n",
    "    return chunk_embeddings, chunk_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Compute documents embeddings')\n",
    "embeddings, chunk_idx = collect_chunks_embeddings(publications)\n",
    "embeddings = [l2norm(e) for e in embeddings]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Compute filters embeddings')\n",
    "\n",
    "positive_filters = ['homo sapience', 'human', 'mammal', 'human cell']\n",
    "negative_filters = ['cancer', 'tumor', 'tumor genesis', 'adenoma', 'carcinoma', 'mouse']\n",
    "\n",
    "print(f'Computing filters embeddings embeddings')\n",
    "negative_filters_embeddings = [l2norm(e) for e in batch_texts_embeddings(positive_filters)]\n",
    "positive_filters_embeddings = [l2norm(e) for e in batch_texts_embeddings(negative_filters)]\n",
    "\n",
    "negative_filters_scores = [(pmid, max([np.dot(e, ne) for ne in negative_filters_embeddings]))\n",
    "                           for (pmid,_), e in zip(chunk_idx, embeddings)]\n",
    "positive_filters_scores = [(pmid, min([np.dot(e, ne) for ne in positive_filters_embeddings]))\n",
    "                           for (pmid,_), e in zip(chunk_idx, embeddings)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "axes = [plt.subplot(1, 3, i+1) for i in range(3)]\n",
    "ax = axes[0]\n",
    "ns = [s for _, s in negative_filters_scores]\n",
    "sns.histplot(ns, kde=True, ax=ax)\n",
    "ax.set_title('Negative filters')\n",
    "\n",
    "ax = axes[1]\n",
    "ps = [s for _, s in positive_filters_scores]\n",
    "sns.histplot(ps, kde=True, ax=ax)\n",
    "ax.set_title('Positive filters')\n",
    "\n",
    "ax = axes[2]\n",
    "sns.scatterplot(x=ns, y=ps, ax=ax)\n",
    "sns.rugplot(x=ns, y=ps, height=.1, alpha=0.01, ax=ax)\n",
    "ax.set_xlabel('Negative filters')\n",
    "ax.set_ylabel('Positive filters')\n",
    "ax.set_title('Positive filters vs negative filters')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_negative_filter_score = 0.1\n",
    "min_positive_filter_score = 0.05\n",
    "\n",
    "filtered_ids = [\n",
    "    pmid for (pmid, ps), (_, ns) in zip(positive_filters_scores, negative_filters_scores)\n",
    "    if ps > min_positive_filter_score and ns < max_negative_filter_score\n",
    "]\n",
    "\n",
    "filtered_publications = load_publications(filtered_ids)\n",
    "filtered_publications['title']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualization of semantic search results\n",
    "\n",
    "Launch fasttext endpoint API so that analyzer can use it\n",
    "  ```\n",
    "  conda activate pubtrends\n",
    "  export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "  python pysrc/fasttext/fasttext_app.py\n",
    "  ```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    analyzer.analyze_papers(filtered_ids, 5)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bokeh.plotting import show\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "\n",
    "analyzer.search_ids = filtered_ids\n",
    "plotter = Plotter(config, analyzer)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# show(plotter.plot_top_cited_papers())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.plot_papers_graph())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.topics_hierarchy_with_keywords())",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
