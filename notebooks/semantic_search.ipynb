{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic search with fasttext embeddings and postgresql\n",
    "\n",
    "Create DB Postgresql + pgvector\n",
    "```\n",
    "docker run --rm --name pubtrends-postgres -p 5432:5432 \\\n",
    "        -m 32G \\\n",
    "        -e POSTGRES_USER=biolabs -e POSTGRES_PASSWORD=mysecretpassword \\\n",
    "        -e POSTGRES_DB=pubtrends \\\n",
    "        -v ~/pgvector/:/var/lib/postgresql/data \\\n",
    "        -e PGDATA=/var/lib/postgresql/data/pgdata \\\n",
    "        -d pgvector/pgvector:pg17\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configures weather to use Postgres index or use Fast index in Faiss\n",
    "EXACT_SEARCH = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connections with main PubTrends database"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.config import PubtrendsConfig\n",
    "config = PubtrendsConfig(test=False)\n",
    "\n",
    "connection_string_full_db = f\"\"\"\n",
    "                    host={config.postgres_host} \\\n",
    "                    port={config.postgres_port} \\\n",
    "                    dbname={config.postgres_database} \\\n",
    "                    user={config.postgres_username} \\\n",
    "                    password={config.postgres_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.postgres_utils import ints_to_vals\n",
    "\n",
    "\n",
    "def load_publications(pids):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "    vals = ints_to_vals(pids)\n",
    "    query = f'''\n",
    "                SELECT P.pmid as id, title, abstract, year\n",
    "                FROM PMPublications P\n",
    "                WHERE P.pmid IN (VALUES {vals});\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        df = pd.DataFrame(cursor.fetchall(),\n",
    "                          columns=['id', 'title', 'abstract', 'year'],\n",
    "                          dtype=object)\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications_year(year):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "        query = f'''\n",
    "                SELECT P.pmid as id, title, abstract\n",
    "                FROM PMPublications P\n",
    "                WHERE year = {year}\n",
    "                ORDER BY pmid;\n",
    "                '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = pd.DataFrame(cursor.fetchall(),\n",
    "                              columns=['id', 'title', 'abstract'],\n",
    "                              dtype=object)\n",
    "            return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# load_publications_year(2025).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import universal_chunk\n",
    "\n",
    "text = \"Staphylococcus aureus is a rare cause of postinfectious glomerulonephritis, and Staphylococcus-related glo-merulonephritis primarily occurs in middle-aged or elderly patients. Patients with Staphylococcus-related glomerulonephritis also present with hematuria, proteinuria of varying degrees, rising serum creatinine levels, and/or edema. The severity of renal insufficiency is proportional to the degree of proliferation and crescent formation. Here, we present a diabetic patient admitted with a history of 1 week of left elbow pain. Laboratory results revealed that erythrocyte sedimentation rate was 110 mm/hour, serum creatinine level was 1 mg/dL, C-reactive protein level was 150 mg/L, and magnetic resonance imaging showed signal changes in favor of osteomyelitis at the olecranon level, with diffuse edematous appearance in the elbow skin tissue and increased intra-articular effusion. After diagnosis of osteomyelitis, ampicillin/sulbactam and teicoplanin were administered. After day 7 of admission, the patient developed acute kidney injury requiring hemodialysis under antibiotic treatment. Kidney biopsy was performed to determine the underlying cause, which showed Staphylococcus-related glomerulonephritis. Recovery of renal func-tions was observed after antibiotic and supportive treatment.\"\n",
    "\n",
    "chunks = universal_chunk(text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import process_paper_chunks\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "\n",
    "def parallel_collect_chunks(pids, texts):\n",
    "    # Default to number of CPUs for max workers\n",
    "    max_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    chunks = []\n",
    "    chunk_idx = []\n",
    "\n",
    "    # Process texts in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create a list of (pid, text) tuples for processing\n",
    "        # Using index as pid for demonstration\n",
    "        text_args = [(pid, text) for pid, text in zip(pids, texts)]\n",
    "\n",
    "        # Submit all tasks and wait for results\n",
    "        results = list(executor.map(process_paper_chunks, text_args))\n",
    "\n",
    "    # Combine results\n",
    "    for text_chunks, text_chunk_idx in results:\n",
    "        chunks.extend(text_chunks)\n",
    "        chunk_idx.extend(text_chunk_idx)\n",
    "    assert len(chunks) == len(chunk_idx)\n",
    "    return chunks, chunk_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare Postgresql + pgvector for embeddings search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "semantics_search_host = 'localhost'\n",
    "semantics_search_port = 5432\n",
    "semantics_search_database = 'pubtrends'\n",
    "semantics_search_username = 'biolabs'\n",
    "semantics_search_password = 'mysecretpassword'\n",
    "\n",
    "semantics_search_connection_string = f\"\"\"\n",
    "                    host={semantics_search_host} \\\n",
    "                    port={semantics_search_port} \\\n",
    "                    dbname={semantics_search_database} \\\n",
    "                    user={semantics_search_username} \\\n",
    "                    password={semantics_search_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embeddings DB initialization\n",
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            create table PMPublicationsEmbeddings(\n",
    "                                                     pmid    integer,\n",
    "                                                     chunk   integer,\n",
    "                                                     embedding vector(200)\n",
    "            );\n",
    "            CREATE INDEX pmid_chunk_idx\n",
    "            ON PMPublicationsEmbeddings(pmid, chunk);\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not EXACT_SEARCH:\n",
    "    # Create an index for fast vector similarity search using cosine distance\n",
    "    # Index may slightly change results vs exact match search, but it's much faster!\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        connection.set_session(readonly=False)\n",
    "        query = '''\n",
    "                CREATE INDEX embedding_idx\n",
    "                ON PMPublicationsEmbeddings\n",
    "                USING ivfflat (embedding vector_cosine_ops)\n",
    "                WITH (lists = 100);\n",
    "            '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "        connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute embeddings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.fasttext.fasttext import PretrainedModelCache\n",
    "\n",
    "# Use a local embeddings model, and dispose it after\n",
    "model_cache = PretrainedModelCache()\n",
    "model_cache.download_and_load_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_embedding_fasttext(text, model_instance):\n",
    "    tokens = text.split()\n",
    "    vectors = [\n",
    "            model_instance.get_vector(t) if model_instance.has_index_for(t)\n",
    "            else np.zeros(model_instance.vector_size)  # Support out-of-dictionary missing embeddings\n",
    "            for t in tokens\n",
    "        ]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model_instance.vector_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(text_embedding_fasttext(text, model_instance=model_cache.download_and_load_model))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import concurrent\n",
    "\n",
    "def parallel_texts_embeddings(texts, model_instance):\n",
    "    # Default to number of CPUs for max workers\n",
    "    max_workers = multiprocessing.cpu_count()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(\n",
    "            lambda t: text_embedding_fasttext(t, model_instance), t\n",
    "        ) for t in texts]\n",
    "        # Important: keep order of results!!!\n",
    "        return [future.result() for future in futures]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Insert embeddings into Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def l2norm(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0:\n",
    "        norm = np.finfo(v.dtype).eps\n",
    "    v /= norm\n",
    "    return v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from more_itertools import sliced\n",
    "CHUNK_SIZE = 10_000\n",
    "\n",
    "for year in range(2025, 2024, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    df = load_publications_year(year)\n",
    "    print('Storing embeddings into DB')\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for index_slice in tqdm(list(index_slices)):\n",
    "        chunk = df.iloc[index_slice]\n",
    "        print('\\rCollecting chunks           ', end='')\n",
    "        pids = list(chunk['id'])\n",
    "        texts = [f'{title}. {abstract}' for title, abstract in zip(chunk['title'], chunk['abstract'])]\n",
    "        chunks, chunk_idx = parallel_collect_chunks(pids, texts)\n",
    "        print('\\rCompute embeddings          ', end='')\n",
    "        chunk_embeddings = parallel_texts_embeddings(chunks, model_cache.download_and_load_model)\n",
    "        print(f'\\rStoring {len(chunk_embeddings)} embeddings', end='')\n",
    "        # Normalize embeddings if using cosine similarity\n",
    "        data = [(pmid, chunk, l2norm(e).tolist())\n",
    "                for (pmid, chunk), e in zip(chunk_idx, chunk_embeddings)]\n",
    "        with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                execute_values(\n",
    "                    cursor,\n",
    "                    \"INSERT INTO PMPublicationsEmbeddings (pmid, chunk, embedding) VALUES %s\",\n",
    "                    data\n",
    "                )\n",
    "            connection.commit()\n",
    "    print('\\rDone                      ')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Postgresql"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_postgresql(query, k):\n",
    "    query_vector = text_embedding_fasttext(query, model_cache.download_and_load_model)\n",
    "    # Normalize embeddings if using cosine similarity\n",
    "    embedding = l2norm(query_vector).tolist()\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                   SELECT pmid, chunk, embedding <=> %s::vector AS distance\n",
    "                   FROM PMPublicationsEmbeddings\n",
    "                   ORDER BY distance\n",
    "                   LIMIT %s\n",
    "                   \"\"\", (embedding, k))\n",
    "\n",
    "            results = cursor.fetchall()\n",
    "            return pd.DataFrame(data=results, columns=['pmid', 'chunk', 'distance'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_pg = semantic_search_postgresql(\"epigenetic human aging\", 1000)\n",
    "search_pg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pmids_pg = search_pg['pmid']\n",
    "len(pmids_pg.unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_publications(pmids_pg)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Insert embeddings into Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "\n",
    "# ! mkdir -p ~/faiss\n",
    "FAISS_INDEX_FILE = os.path.expanduser('~/faiss/embeddings.index')\n",
    "PIDS_INDEX_FILE = os.path.expanduser('~/faiss/pids.csv.gz')\n",
    "\n",
    "def create_faiss():\n",
    "    if EXACT_SEARCH:\n",
    "        index = faiss.IndexFlatIP(200)\n",
    "    else:\n",
    "        # Using HNSW index (good performance at scale)\n",
    "        # dimension, number of neighbors in HNSW graph\n",
    "        index = faiss.IndexHNSWFlat(200, 200)\n",
    "        # higher value improves accuracy (at expense of index build time)\n",
    "        index.hnsw.efConstruction = 500\n",
    "    return index\n",
    "\n",
    "def create_or_load_faiss():\n",
    "    if os.path.exists(FAISS_INDEX_FILE):\n",
    "        print('Loading Faiss index from existing file')\n",
    "        index = faiss.read_index(FAISS_INDEX_FILE)\n",
    "        # For accurate search\n",
    "        index.nprobe = 200\n",
    "    else:\n",
    "        print('Creating empty Faiss index')\n",
    "        index = create_faiss()\n",
    "    if os.path.exists(PIDS_INDEX_FILE):\n",
    "        pids_idx = pd.read_csv(PIDS_INDEX_FILE, compression='gzip')\n",
    "    else:\n",
    "        pids_idx = pd.DataFrame(data=[], columns=['pmid', 'chunk'], dtype=int)\n",
    "    assert index.ntotal == len(pids_idx)\n",
    "    return index, pids_idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from more_itertools import sliced\n",
    "CHUNK_SIZE = 10_000\n",
    "\n",
    "faiss_index, pids_idx = None, None\n",
    "\n",
    "for year in range(2025, 2024, -1):\n",
    "    if faiss_index is None:\n",
    "        faiss_index, pids_idx = create_or_load_faiss()\n",
    "\n",
    "    print(f'Processing year {year}')\n",
    "    df = load_publications_year(year)\n",
    "\n",
    "    print('Computing embeddings')\n",
    "    chunk_idx_all = []\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for i, index_slice in tqdm(list(enumerate(index_slices))):\n",
    "        chunk = df.iloc[index_slice]\n",
    "        print('\\rCollecting chunks           ', end='')\n",
    "        pids = list(chunk['id'])\n",
    "        texts = [f'{title}. {abstract}' for title, abstract in zip(chunk['title'], chunk['abstract'])]\n",
    "        chunks, chunk_idx = parallel_collect_chunks(pids, texts)\n",
    "        print('\\rCompute embeddings          ', end='')\n",
    "        chunk_embeddings = parallel_texts_embeddings(chunks, model_cache.download_and_load_model)\n",
    "        print(f'\\rStoring {len(chunk_embeddings)} embeddings', end='')\n",
    "        embeddings = np.array(chunk_embeddings).astype('float32')\n",
    "        # Normalize embeddings if using cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        # Add embeddings to the index\n",
    "        faiss_index.add(embeddings)\n",
    "        chunk_idx_all.extend(chunk_idx)\n",
    "        # Intermediate save\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            print('\\rStoring FAISS index          ', end='')\n",
    "            faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "            print('\\rStoring Ids index            ', end='')\n",
    "            new_pids_idx = pd.DataFrame(chunk_idx_all, columns=['pmid', 'chunk'])\n",
    "            t = pd.concat([pids_idx, new_pids_idx]).reset_index(drop=True)\n",
    "            t.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "            assert faiss_index.ntotal == len(t)\n",
    "    # Finally\n",
    "    print('\\rStoring FAISS index          ', end='')\n",
    "    faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "    print('\\rStoring Ids index            ', end='')\n",
    "    new_pids_idx = pd.DataFrame(chunk_idx_all, columns=['pmid', 'chunk'])\n",
    "    t = pd.concat([pids_idx, new_pids_idx]).reset_index(drop=True)\n",
    "    t.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "    pids_idx = t\n",
    "    assert faiss_index.ntotal == len(pids_idx)\n",
    "    print('\\rDone                        ')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Alternatively, copy embeddings from Postgresql into Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import ast  # For safely converting string to list\n",
    "#\n",
    "# CHUNK_SIZE = 50_000\n",
    "#\n",
    "# faiss_index = create_faiss()\n",
    "#\n",
    "# with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "#     with connection.cursor() as cursor:\n",
    "#         cursor.execute(f'SELECT COUNT(*) FROM PMPublicationsEmbeddings')\n",
    "#         total_rows = cursor.fetchone()[0]\n",
    "#         print(f'Total embeddings: {total_rows}')\n",
    "#         # Fetch the data in chunks\n",
    "#         for offset in tqdm(list(range(0, total_rows, CHUNK_SIZE))):\n",
    "#             print('\\rLoading embeddings          ', end='')\n",
    "#             query = f\"\"\"\n",
    "#                     SELECT embedding FROM PMPublicationsEmbeddings\n",
    "#                     ORDER BY pmid, chunk\n",
    "#                     LIMIT {CHUNK_SIZE} OFFSET {offset};\n",
    "#             \"\"\"\n",
    "#             cursor.execute(query)\n",
    "#             embeddings = [np.array(ast.literal_eval(row[0])) for row in cursor.fetchall()]\n",
    "#             embeddings = np.array(embeddings).astype('float32')\n",
    "#             print(f'\\rStoring {len(embeddings)} embeddings', end='')\n",
    "#             # Add already normalized embeddings to the index\n",
    "#             faiss_index.add(embeddings)\n",
    "#         print('Storing FAISS index')\n",
    "#         faiss.write_index(faiss_index, FAISS_INDEX_FILE)\n",
    "#         print('Storing Ids index')\n",
    "#         with connection.cursor() as cursor:\n",
    "#             cursor.execute(f\"SELECT pmid, chunk FROM PMPublicationsEmbeddings ORDER BY pmid, chunk;\")\n",
    "#             pids_idx = pd.DataFrame(cursor.fetchall(),\n",
    "#                               columns=['pmid', 'chunk'],\n",
    "#                               dtype=object)\n",
    "#             pids_idx.to_csv(PIDS_INDEX_FILE, index=False, compression='gzip')\n",
    "#     print('Done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search with Faiss"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "faiss_index, pids_idx = create_or_load_faiss()\n",
    "print(len(pids_idx))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search_faiss(query_text, faiss_index, df, k):\n",
    "    query_vector = text_embedding_fasttext(query_text, model_cache.download_and_load_model).reshape(1, -1)\n",
    "    # Normalize embeddings if using cosine similarity\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    similarities, indices = faiss_index.search(query_vector.astype('float32'), k)\n",
    "    t = df.iloc[indices[0]].copy().reset_index(drop=True)\n",
    "    t['similarity'] = similarities.reshape(1, -1)[0]\n",
    "    return t"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_fs = semantic_search_faiss(\"epigenetic human aging\", faiss_index, pids_idx, 1000)\n",
    "search_fs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# At some point, vectors are similar and too far away from the request vector, which limits the number of results\n",
    "pmids_fs = search_fs['pmid']\n",
    "len(pmids_fs.unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_publications(pmids_fs)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del faiss_index, pids_idx",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison Postgresql vs Faiss semantic search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f'Postgresql {len(pmids_pg.unique())}')\n",
    "print(f'Faiss {len(pmids_fs.unique())}')\n",
    "overlap = set(list(pmids_pg)) & set(list(pmids_fs))\n",
    "print(f'Overlap {len(overlap)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Analysis of semantic search results\n",
    "\n",
    "Launch fasttext endpoint API, so that analyzer can use it\n",
    "  ```\n",
    "  conda activate pubtrends\n",
    "  export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "  python pysrc/fasttext/fasttext_app.py\n",
    "  ```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Don't store two embedding models in RAM simultaneously\n",
    "del model_cache"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import is_fasttext_endpoint_ready\n",
    "import time\n",
    "\n",
    "i = 0\n",
    "while not is_fasttext_endpoint_ready():\n",
    "    print('\\rWaiting for fasttext endpoint to be ready' + '.' * i, end='')\n",
    "    i += 1\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.analysis.text import fetch_fasttext_text_embedding\n",
    "\n",
    "len(fetch_fasttext_text_embedding(\"epigenetic human aging\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config.topic_min_size = 5\n",
    "try:\n",
    "    analyzer.analyze_papers(pmids_fs, 10)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bokeh.plotting import show\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "\n",
    "analyzer.search_ids = pmids_pg\n",
    "plotter = Plotter(config, analyzer)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.plot_papers_graph())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show(plotter.topics_hierarchy_with_keywords())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
