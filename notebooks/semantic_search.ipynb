{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic search with fasttext embeddings and postgresql\n",
    "\n",
    "Create DB Postgresql + pgvector\n",
    "```\n",
    "docker run --rm --name pubtrends-postgres -p 5432:5432 \\\n",
    "        -m 32G \\\n",
    "        -e POSTGRES_USER=biolabs -e POSTGRES_PASSWORD=mysecretpassword \\\n",
    "        -e POSTGRES_DB=pubtrends \\\n",
    "        -v ~/postgres/:/var/lib/postgresql/data \\\n",
    "        -e PGDATA=/var/lib/postgresql/data/pgdata \\\n",
    "        -d pgvector/pgvector:pg17\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connections with main PubTrends database"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_db_host = ''\n",
    "full_db_port = 5432\n",
    "full_db_database = 'pubtrends'\n",
    "full_db_username = 'biolabs'\n",
    "full_db_password = 'mysecretpassword'\n",
    "\n",
    "connection_string_full_db = f\"\"\"\n",
    "                    host={full_db_host} \\\n",
    "                    port={full_db_port} \\\n",
    "                    dbname={full_db_database} \\\n",
    "                    user={full_db_username} \\\n",
    "                    password={full_db_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications(pids):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "    vals = ints_to_vals(pids)\n",
    "    query = f'''\n",
    "                SELECT P.pmid as id, title, abstract, year\n",
    "                FROM PMPublications P\n",
    "                WHERE P.pmid IN (VALUES {vals});\n",
    "                '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        df = pd.DataFrame(cursor.fetchall(),\n",
    "                          columns=['id', 'title', 'abstract', 'year'],\n",
    "                          dtype=object)\n",
    "        return df\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_publications_year(year):\n",
    "    with psycopg2.connect(connection_string_full_db) as connection:\n",
    "        connection.set_session(readonly=True)\n",
    "        query = f'''\n",
    "                SELECT P.pmid as id, title, abstract\n",
    "                FROM PMPublications P\n",
    "                WHERE year = {year};\n",
    "                '''\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            df = pd.DataFrame(cursor.fetchall(),\n",
    "                              columns=['id', 'title', 'abstract'],\n",
    "                              dtype=object)\n",
    "            return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_publications_year(2025).head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings with fasttext"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def universal_chunk(text, max_tokens=64, overlap_sentences=1):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        sentence_tokens = len(sentences[i].split())\n",
    "\n",
    "        if current_tokens + sentence_tokens <= max_tokens:\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_tokens += sentence_tokens\n",
    "            i += 1\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Retain overlap\n",
    "            current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences else []\n",
    "            current_tokens = sum(len(s.split()) for s in current_chunk)\n",
    "\n",
    "            # Check if we'll get stuck in an infinite loop\n",
    "            if current_chunk and current_tokens + sentence_tokens > max_tokens:\n",
    "                # If the sentence still won't fit after keeping overlap, handle it specially\n",
    "                if sentence_tokens > max_tokens:\n",
    "                    # If the sentence itself is too long, split it\n",
    "                    if current_chunk:\n",
    "                        chunks.append(\" \".join(current_chunk))\n",
    "                        current_chunk = []\n",
    "                        current_tokens = 0\n",
    "                    words = sentences[i].split()\n",
    "                    for j in range(0, len(words), max_tokens):\n",
    "                        subchunk = \" \".join(words[j:j + max_tokens])\n",
    "                        chunks.append(subchunk)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    # If the sentence is not too long but won't fit with overlap,\n",
    "                    # start a new chunk with just this sentence\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                    current_chunk = [sentences[i]]\n",
    "                    current_tokens = sentence_tokens\n",
    "                    i += 1\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "text = \"Staphylococcus aureus is a rare cause of postinfectious glomerulonephritis, and Staphylococcus-related glo-merulonephritis primarily occurs in middle-aged or elderly patients. Patients with Staphylococcus-related glomerulonephritis also present with hematuria, proteinuria of varying degrees, rising serum creatinine levels, and/or edema. The severity of renal insufficiency is proportional to the degree of proliferation and crescent formation. Here, we present a diabetic patient admitted with a history of 1 week of left elbow pain. Laboratory results revealed that erythrocyte sedimentation rate was 110 mm/hour, serum creatinine level was 1 mg/dL, C-reactive protein level was 150 mg/L, and magnetic resonance imaging showed signal changes in favor of osteomyelitis at the olecranon level, with diffuse edematous appearance in the elbow skin tissue and increased intra-articular effusion. After diagnosis of osteomyelitis, ampicillin/sulbactam and teicoplanin were administered. After day 7 of admission, the patient developed acute kidney injury requiring hemodialysis under antibiotic treatment. Kidney biopsy was performed to determine the underlying cause, which showed Staphylococcus-related glomerulonephritis. Recovery of renal func-tions was observed after antibiotic and supportive treatment.\"\n",
    "universal_chunk(text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.fasttext.fasttext import PRETRAINED_MODEL_CACHE\n",
    "\n",
    "logger.info('Compute words embeddings using pretrained fasttext model')\n",
    "# Model will be loaded when needed through the lazy property\n",
    "logger.info('Done')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokens_embeddings_fasttext(text):\n",
    "    # Access the model correctly as a lazy property\n",
    "    model_instance = PRETRAINED_MODEL_CACHE.download_and_load_model\n",
    "    return np.mean([\n",
    "        model_instance.get_vector(t) if model_instance.has_index_for(t)\n",
    "        else np.zeros(model_instance.vector_size)  # Support out-of-dictionary missing embeddings\n",
    "        for t in text.split()\n",
    "    ], axis=0).tolist()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(tokens_embeddings_fasttext(text))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare Postgresql + pgvector for embeddings search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "semantics_search_host = ''\n",
    "semantics_search_port = 5432\n",
    "semantics_search_database = 'pubtrends'\n",
    "semantics_search_username = 'biolabs'\n",
    "semantics_search_password = 'mysecretpassword'\n",
    "\n",
    "semantics_search_connection_string = f\"\"\"\n",
    "                    host={semantics_search_host} \\\n",
    "                    port={semantics_search_port} \\\n",
    "                    dbname={semantics_search_database} \\\n",
    "                    user={semantics_search_username} \\\n",
    "                    password={semantics_search_password}\n",
    "                \"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "drop table if exists PMPublicationsSmall;\n",
    "create table PMPublicationsSmall(\n",
    "pmid    integer,\n",
    "title   varchar(1023),\n",
    "abstract text\n",
    ");\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "            CREATE EXTENSION IF NOT EXISTS vector;\n",
    "            drop table if exists PMPublicationsEmbeddings;\n",
    "            create table PMPublicationsEmbeddings(\n",
    "                                                pmid    integer,\n",
    "                                                chunk   integer,\n",
    "                                                embedding vector(200)\n",
    "            );\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an index for fast vector similarity search\n",
    "with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "    connection.set_session(readonly=False)\n",
    "    query = '''\n",
    "            CREATE INDEX texts_embedding_idx\n",
    "                ON PMPublicationsEmbeddings\n",
    "                USING ivfflat (embedding vector_cosine_ops)\n",
    "                WITH (lists = 100);\n",
    "            '''\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "    connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute embeddings for publications"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from more_itertools import sliced\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "for year in range(2025, 2024, -1):\n",
    "    print(f'Processing year {year}')\n",
    "    df = load_publications_year(year)\n",
    "    print('Storing embeddings into DB')\n",
    "    index_slices = sliced(range(len(df)), CHUNK_SIZE)\n",
    "    for index_slice in tqdm(list(index_slices)):\n",
    "        chunk = df.iloc[index_slice]\n",
    "        chunk_embeddings = []\n",
    "        for (pid, title, abstract) in zip(chunk['id'], chunk['title'], chunk['abstract']):\n",
    "            if not title or not abstract:\n",
    "                continue\n",
    "            for i, text_chunk in enumerate(universal_chunk(f'{title}. {abstract}')):\n",
    "                chunk_embeddings.append(\n",
    "                    (pid, i, tokens_embeddings_fasttext(text_chunk)))\n",
    "        print(f'Storing {len(chunk_embeddings)} embeddings')\n",
    "        with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "            with connection.cursor() as cursor:\n",
    "                execute_values(\n",
    "                    cursor,\n",
    "                    \"INSERT INTO PMPublicationsEmbeddings (pmid, chunk, embedding) VALUES %s\",\n",
    "                    chunk_embeddings\n",
    "                )\n",
    "            connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic search"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search(query, k=5):\n",
    "    embedding = tokens_embeddings_fasttext(query)\n",
    "    with psycopg2.connect(semantics_search_connection_string) as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                   SELECT pmid, chunk, embedding <=> %s::vector AS distance\n",
    "                   FROM PMPublicationsEmbeddings\n",
    "                   ORDER BY distance ASC\n",
    "                       LIMIT %s\n",
    "                   \"\"\", (embedding, k))\n",
    "\n",
    "            results = cursor.fetchall()\n",
    "            return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "semantic_search(\"epigenetic modifications in healthy human aging\", 10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pysrc.papers.db.postgres_utils import ints_to_vals\n",
    "\n",
    "def find_papers_semantic_search(query, k):\n",
    "    search = semantic_search(query, k)\n",
    "    pids = [pid for pid, _, _ in search]\n",
    "    return load_publications(pids)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_papers_semantic_search(\"epigenetic modifications in healthy human aging\", 10)",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
