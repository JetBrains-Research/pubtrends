{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training&Evaluation of a developed algorithm\n",
    "\n",
    "In this python notebook you can try several possible architectures and train&evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pysrc.review.config as cfg\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "# Used memory analysis utility\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "# Print all allocated variables\n",
    "def print_mem_usage():\n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                             key= lambda x: x[1],\n",
    "                             reverse=True)[:10]:\n",
    "        print(\"Global {:>30}: {:>8}\".format(name, sizeof_fmt(size)))    \n",
    "    for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                             key= lambda x: x[1],\n",
    "                             reverse=True)[:10]:\n",
    "        print(\"Local {:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "\n",
    "def init_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "init_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a `Summarizer`&`Classifier` class.\n",
    "\n",
    "It has several options to set up: \n",
    "* with or without features (right now without features works better), \n",
    "* `BERT` or `roberta` as basis (no big difference), \n",
    "\n",
    "You can also choose `frozen_strategy`:\n",
    "* `froze_all` in case you don't want to improve bert layers but only the summarization layer, \n",
    "* `unfroze_last4` -- modifies bert weights and still training not very slow, \n",
    "* `unfroze_all` -- the training is slow, the results may better though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertModel, RobertaModel\n",
    "from collections import namedtuple\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "import pysrc.review.config as cfg\n",
    "from pysrc.review.utils import get_ids_mask\n",
    "\n",
    "\n",
    "SpecToken = namedtuple('SpecToken', ['tkn', 'idx'])\n",
    "ConvertToken2Id = lambda tokenizer, tkn: tokenizer.convert_tokens_to_ids([tkn])[0]\n",
    "\n",
    "\n",
    "class Summarizer(nn.Module):\n",
    "\n",
    "    enc_output: torch.Tensor\n",
    "    rouges_values: np.array = np.zeros(4)\n",
    "    dec_ids_mask: torch.Tensor\n",
    "    encdec_ids_mask: torch.Tensor\n",
    "\n",
    "    def __init__(self, model_type, article_len, with_features=False, num_features=10):\n",
    "        super(Summarizer, self).__init__()\n",
    "\n",
    "        self.article_len = article_len\n",
    "\n",
    "        if model_type == 'bert':\n",
    "            self.backbone, self.tokenizer, BOS, EOS, PAD = self.initialize_bert()\n",
    "        elif model_type == 'roberta':\n",
    "            self.backbone, self.tokenizer, BOS, EOS, PAD = self.initialize_roberta()\n",
    "        else:\n",
    "            raise Exception(f\"Wrong model_type argument: {model_type}\")\n",
    "            \n",
    "        if with_features:\n",
    "            self.features = nn.Sequential(nn.Linear(num_features, 100),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(100, 100),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(100, 50))\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "        self.PAD = SpecToken(PAD, ConvertToken2Id(self.tokenizer, PAD))\n",
    "        self.artBOS = SpecToken(BOS, ConvertToken2Id(self.tokenizer, BOS))\n",
    "        self.artEOS = SpecToken(EOS, ConvertToken2Id(self.tokenizer, EOS))\n",
    "\n",
    "        # add special tokens tokenizer\n",
    "        self.tokenizer.add_special_tokens({'additional_special_tokens': [\"<sum>\", \"</sent>\", \"</sum>\"]})\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "        self.sumBOS = SpecToken(\"<sum>\", ConvertToken2Id(self.tokenizer, \"<sum>\"))\n",
    "        self.sumEOS = SpecToken(\"</sent>\", ConvertToken2Id(self.tokenizer, \"</sent>\"))\n",
    "        self.sumEOA = SpecToken(\"</sum>\", ConvertToken2Id(self.tokenizer, \"</sum>\"))\n",
    "        self.backbone.resize_token_embeddings(200 + self.vocab_size)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer.PAD = self.PAD\n",
    "        self.tokenizer.artBOS = self.artBOS\n",
    "        self.tokenizer.artEOS = self.artEOS\n",
    "        self.tokenizer.sumBOS = self.sumBOS\n",
    "        self.tokenizer.sumEOS = self.sumEOS\n",
    "        self.tokenizer.sumEOA = self.sumEOA\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "        # initialize backbone emb pulling\n",
    "        def backbone_forward(input_ids, input_mask, input_segment, input_pos):\n",
    "            return self.backbone(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=input_mask,\n",
    "                token_type_ids=input_segment,\n",
    "                position_ids=input_pos,\n",
    "            )\n",
    "        self.encoder = lambda *args: backbone_forward(*args)[0]\n",
    "\n",
    "        # initialize decoder\n",
    "        if not with_features:\n",
    "            self.decoder = Classifier(cfg.d_hidden)\n",
    "        else:\n",
    "            self.decoder = Classifier(cfg.d_hidden + 50)\n",
    "\n",
    "    def expand_posembs_ifneed(self):\n",
    "        print(self.backbone.config.max_position_embeddings, self.article_len)\n",
    "        if self.article_len > self.backbone.config.max_position_embeddings:\n",
    "            print(\"OK\")\n",
    "            old_maxlen = self.backbone.config.max_position_embeddings\n",
    "            old_w = self.backbone.embeddings.position_embeddings.weight\n",
    "            logging.info(f\"Backbone pos embeddings expanded from {old_maxlen} upto {self.article_len}\")\n",
    "            self.backbone.embeddings.position_embeddings = \\\n",
    "                nn.Embedding(self.article_len, self.backbone.config.hidden_size)\n",
    "            self.backbone.embeddings.position_embeddings.weight[:old_maxlen].data.copy_(old_w)\n",
    "            self.backbone.config.max_position_embeddings = self.article_len\n",
    "        print(self.backbone.config.max_position_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_bert():\n",
    "        backbone = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=False\n",
    "        )\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        BOS = \"[CLS]\"\n",
    "        EOS = \"[SEP]\"\n",
    "        PAD = \"[PAD]\"\n",
    "        return backbone, tokenizer, BOS, EOS, PAD\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_roberta():\n",
    "        backbone = RobertaModel.from_pretrained(\n",
    "            'roberta-base', output_hidden_states=False\n",
    "        )\n",
    "        # initialize token type emb, by default roberta doesn't have it\n",
    "        backbone.config.type_vocab_size = 2\n",
    "        backbone.embeddings.token_type_embeddings = nn.Embedding(2, backbone.config.hidden_size)\n",
    "        backbone.embeddings.token_type_embeddings.weight.data.normal_(\n",
    "            mean=0.0, std=backbone.config.initializer_range\n",
    "        )\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "        BOS = \"<s>\"\n",
    "        EOS = \"</s>\"\n",
    "        PAD = \"<pad>\"\n",
    "        return backbone, tokenizer, BOS, EOS, PAD\n",
    "\n",
    "    def save(self, save_filename):\n",
    "        \"\"\" Save model in filename\n",
    "\n",
    "        :param save_filename: str\n",
    "        \"\"\"\n",
    "        if not self.features:\n",
    "            state = {\n",
    "                'encoder_dict': self.backbone.state_dict(),\n",
    "                'decoder_dict': self.decoder.state_dict(),\n",
    "            }\n",
    "        else:\n",
    "            state = {\n",
    "                'encoder_dict': self.backbone.state_dict(),\n",
    "                'decoder_dict': self.decoder.state_dict(),\n",
    "                'features_dict': self.features.state_dict(),\n",
    "            }\n",
    "        models_folder = os.path.expanduser(cfg.weights_path)\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.mkdirs(models_folder)\n",
    "        torch.save(state, f\"{models_folder}/{save_filename}.pth\")\n",
    "\n",
    "    def load(self, load_filename):\n",
    "        path = f\"{os.path.expanduser(cfg.weights_path)}/{load_filename}.pth\"\n",
    "        state = torch.load(path, map_location=lambda storage, location: storage)\n",
    "        self.backbone.load_state_dict(state['encoder_dict'])\n",
    "        self.decoder.load_state_dict(state['decoder_dict'])\n",
    "        if self.features:\n",
    "            self.features.load_state_dict(state['features_dict'])\n",
    "        \n",
    "\n",
    "    def froze_backbone(self, froze_strategy):\n",
    "\n",
    "        assert froze_strategy in ['froze_all', 'unfroze_last4', 'unfroze_all'],\\\n",
    "            f\"incorrect froze_strategy argument: {froze_strategy}\"\n",
    "\n",
    "        if froze_strategy == 'froze_all':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        elif froze_strategy == 'unfroze_last4':\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(True if (\n",
    "                    'encoder.layer.11' in name or\n",
    "                    'encoder.layer.10' in name or\n",
    "                    'encoder.layer.9' in name or\n",
    "                    'encoder.layer.8' in name\n",
    "                ) else False)\n",
    "\n",
    "        elif froze_strategy == 'unfroze_all':\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "    def unfroze_head(self):\n",
    "\n",
    "        for name, param in self.decoder.named_parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    @property\n",
    "    def rouge_1(self):\n",
    "        return self.rouges_values[0]\n",
    "\n",
    "    @property\n",
    "    def rouge_2(self):\n",
    "        return self.rouges_values[1]\n",
    "\n",
    "    @property\n",
    "    def rouge_l(self):\n",
    "        return self.rouges_values[2]\n",
    "\n",
    "    @property\n",
    "    def rouge_mean(self):\n",
    "        return self.rouges_values[3]\n",
    "\n",
    "    def forward(self, input_ids, input_mask, input_segment, input_features=None):\n",
    "        \"\"\" Train for 1st stage of model\n",
    "\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param input_mask: torch.Size([batch_size, article_len])\n",
    "        :param input_segment: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            logprobs | torch.Size([batch_size, summary_len, vocab_size])\n",
    "        \"\"\"\n",
    "        \n",
    "        cls_mask = (input_ids == self.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch\\\n",
    "            .arange(0, self.article_len, dtype=torch.long, device=input_ids.device)\\\n",
    "            .unsqueeze(0)\\\n",
    "            .repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, input_mask, input_segment, pos_ids)\n",
    "        \n",
    "        if self.features:\n",
    "            temp_features = self.features(input_features)\n",
    "            draft_logprobs = self.decoder(torch.cat([enc_output[cls_mask], temp_features], dim=-1))\n",
    "        else:\n",
    "            draft_logprobs = self.decoder(enc_output[cls_mask])\n",
    "\n",
    "        return draft_logprobs\n",
    "\n",
    "    def evaluate(self, input_ids, input_mask, input_segment, input_features=None):\n",
    "        \"\"\" Eval for 1st stage of model\n",
    "\n",
    "        :param input_ids: torch.Size([batch_size, article_len])\n",
    "        :param input_mask: torch.Size([batch_size, article_len])\n",
    "        :param input_segment: torch.Size([batch_size, article_len])\n",
    "        :return:\n",
    "            draft_ids | torch.Size([batch_size, summary_len])\n",
    "        \"\"\"\n",
    "\n",
    "        cls_mask = (input_ids == self.artBOS.idx)\n",
    "\n",
    "        # position ids | torch.Size([batch_size, article_len])\n",
    "        pos_ids = torch\\\n",
    "            .arange(0, self.article_len, dtype=torch.long, device=input_ids.device)\\\n",
    "            .unsqueeze(0)\\\n",
    "            .repeat(len(input_ids), 1)\n",
    "        # extract bert embeddings | torch.Size([batch_size, article_len, d_bert])\n",
    "        enc_output = self.encoder(input_ids, input_mask, input_segment, pos_ids)\n",
    "\n",
    "        ans = []\n",
    "        for eo, cm in zip(enc_output, cls_mask):\n",
    "            if self.features:\n",
    "                scores = self.decoder.evaluate(torch.cat([eo[cm], self.features(input_features)], dim=-1))\n",
    "            else:\n",
    "                scores = self.decoder.evaluate(eo[cm])\n",
    "            ans.append(scores)\n",
    "        return ans\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        x = self.linear1(x).squeeze(-1)\n",
    "        scores = self.sigmoid(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `train_fun` -- training function for model without features. \\\n",
    "`train_fun_ft` -- training function for model with features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from pysrc.review.utils import get_enc_lr, get_dec_lr\n",
    "\n",
    "def backward_step(loss: torch.Tensor, optimizer: Optimizer, model: nn.Module, clip: float, amp_enabled: int):\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    return total_norm\n",
    "\n",
    "def train_fun(model, dataloader, optimizer, scheduler, criter, device, rank, writer, distributed):\n",
    "\n",
    "    # draft, refine\n",
    "    model.train()\n",
    "    model_ref = model.module if distributed else model\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False, disable=rank != 0)\n",
    "    for idx_batch, batch in pbar:\n",
    "        \n",
    "        input_ids, input_mask, input_segment, target_scores = \\\n",
    "            [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "\n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "        # loss\n",
    "            loss = criter(\n",
    "                draft_probs,\n",
    "                target_scores,\n",
    "            )\n",
    "        except Exception:\n",
    "            print(idx_batch, draft_probs.shape, target_scores.shape, input_segment)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value, amp_enabled=cfg.amp_enabled)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        # loss_val += loss.item() * len(input_ids)\n",
    "        pbar.set_description(f\"loss:{loss.item():.2f}\")\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    # overall loss per epoch\n",
    "    # if distributed:\n",
    "    #     loss_val = distribute(loss_val, device)\n",
    "    # logging.info(f\"mean loss: {loss_val / len(dataloader.dataset):.4f}\", is_print=rank == 0)\n",
    "\n",
    "    # save model, just in case\n",
    "    if rank == 0:\n",
    "        model_ref.save('temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer\n",
    "\n",
    "def train_fun_ft(model, dataloader, optimizer, scheduler, criter, device, rank, writer, distributed):\n",
    "\n",
    "    # draft, refine\n",
    "    model.train()\n",
    "    model_ref = model.module if distributed else model\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False, disable=rank != 0)\n",
    "    for idx_batch, batch in pbar:\n",
    "        \n",
    "        input_ids, input_mask, input_segment, target_scores, input_features = \\\n",
    "            [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        input_features = torch.cat(input_features).to(device)\n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment, input_features,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "        # loss\n",
    "            loss = criter(\n",
    "                draft_probs,\n",
    "                target_scores,\n",
    "            )\n",
    "        except Exception:\n",
    "            print(idx_batch, draft_probs.shape, target_scores.shape, input_segment)\n",
    "            return\n",
    "\n",
    "        # backward\n",
    "        grad_norm = backward_step(loss, optimizer, model, optimizer.clip_value, amp_enabled=cfg.amp_enabled)\n",
    "        grad_norm = 0 if (math.isinf(grad_norm) or math.isnan(grad_norm)) else grad_norm\n",
    "\n",
    "        # record a loss value\n",
    "        # loss_val += loss.item() * len(input_ids)\n",
    "        pbar.set_description(f\"loss:{loss.item():.2f}\")\n",
    "        writer.add_scalar(f\"Train/loss\", loss.item(), writer.train_step)\n",
    "        writer.add_scalar(\"Train/grad_norm\", grad_norm, writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_enc\", get_enc_lr(optimizer), writer.train_step)\n",
    "        writer.add_scalar(\"Train/lr_dec\", get_dec_lr(optimizer), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "        # make a gradient step\n",
    "        if (idx_batch + 1) % optimizer.accumulation_interval == 0 or (idx_batch + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    # overall loss per epoch\n",
    "    # if distributed:\n",
    "    #     loss_val = distribute(loss_val, device)\n",
    "    # logging.info(f\"mean loss: {loss_val / len(dataloader.dataset):.4f}\", is_print=rank == 0)\n",
    "\n",
    "    # save model, just in case\n",
    "    if rank == 0:\n",
    "        model_ref.save('temp')\n",
    "\n",
    "    return model, optimizer, scheduler, writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `load_data` loads all the needed datafiles for building train dataset.\\\n",
    "The several next steps are only should be done if no train/test/val datasets are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ft):\n",
    "    root2data = Path(os.path.expanduser(cfg.dataset_path))\n",
    "    logging.info(f'Loading references dataset from {root2data}')\n",
    "    \n",
    "    logging.info('Loading citations_df')\n",
    "    citations_df = pd.read_csv(root2data / \"citations.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(citations_df)))\n",
    "\n",
    "    logging.info('Loading sentences_df')\n",
    "    sentences_df = pd.read_csv(root2data / \"sentences.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(sentences_df)))\n",
    "\n",
    "    logging.info('Loading review_files_df')\n",
    "    review_files_df = pd.read_csv(root2data / \"review_files.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(review_files_df)))\n",
    "\n",
    "    logging.info('Loading reverse_ref_df')\n",
    "    reverse_ref_df = pd.read_csv(root2data / \"reverse_ref.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(reverse_ref_df)))\n",
    "    if not ft:\n",
    "        return citations_df, sentences_df, review_files_df, reverse_ref_df\n",
    "\n",
    "    logging.info('Loading abstracts_df')\n",
    "    abstracts_df = pd.read_csv(root2data / \"abstracts.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(abstracts_df)))\n",
    "    \n",
    "    logging.info('Loading filelist_df')\n",
    "    filelist_df = pd.read_csv(root2data / \"filelist.csv\", sep=',')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(filelist_df)))\n",
    "        \n",
    "    logging.info('Loading figures_df')\n",
    "    figures_df = pd.read_csv(root2data / \"figures.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(figures_df)))\n",
    "        \n",
    "    logging.info('Loading tables_df')\n",
    "    tables_df = pd.read_csv(root2data / \"tables.csv\", sep='\\t')\n",
    "    logging.info(sizeof_fmt(sys.getsizeof(tables_df)))\n",
    "    return citations_df, sentences_df, review_files_df, reverse_ref_df, \\\n",
    "        abstracts_df, filelist_df, figures_df, tables_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `get_rouge` function will be needed anyway. You can uncomment `rouge-l`, if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ROUGE_METER = Rouge()\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def get_rouge(sent1, sent2):\n",
    "    sent_1 = TOKENIZER.tokenize(sent1)\n",
    "    #print(sent_1)\n",
    "    sent_1 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_1)))\n",
    "    #print(sent_1)\n",
    "    sent_2 = TOKENIZER.tokenize(sent2)\n",
    "    sent_2 = \" \".join(list(filter(lambda x: x.isalpha() or x in '.!,?', sent_2)))\n",
    "    rouges = ROUGE_METER.get_scores(sent_1, sent_2)[0]\n",
    "    rouges = [rouges[f'rouge-{x}'][\"f\"] for x in ('1', '2')] # , 'l')]\n",
    "    return np.mean(rouges) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rouge(\"I am scout. True!\", \"No you are not a scout.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Several functions needed to build the datasets. If the datasets alredy exist, do not have to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "REPLACE_SYMBOLS = {\n",
    "    '—': '-',\n",
    "    '–': '-',\n",
    "    '―': '-',\n",
    "    '…': '...',\n",
    "    '´´': \"´\",\n",
    "    '´´´': \"´´\",\n",
    "    \"''\": \"'\",\n",
    "    \"'''\": \"'\",\n",
    "    \"``\": \"`\",\n",
    "    \"```\": \"`\",\n",
    "    \":\": \" : \",\n",
    "}\n",
    "\n",
    "def parse_sents(data):\n",
    "    sents = sum([sent_tokenize(text) for text in data], [])\n",
    "    sents = [x for x in text if len(x) > 3]\n",
    "    return sents\n",
    "\n",
    "def sent_standardize(sent):\n",
    "    sent = unidecode(sent)\n",
    "    sent = re.sub(r\"\\[(xref_\\w*_\\w\\d*]*)(, xref_\\w*_\\w\\d*)*\\]\", \" \", sent)  # delete [xref,...]\n",
    "    sent = re.sub(r\"\\( (xref_\\w*_\\w\\d*)(; xref_\\w*_\\w\\d*)* \\)\", \" \", sent)  # delete (xref; ...)\n",
    "    sent = re.sub(r\"\\[xref_\\w*_\\w\\d*\\]\", \" \", sent)  # delete [xref]\n",
    "    sent = re.sub(r\"xref_\\w*_\\w\\d*\", \" \", sent)  # delete [[xref]]\n",
    "    for k, v in REPLACE_SYMBOLS.items():\n",
    "        sent = sent.replace(k, v)\n",
    "    return sent.strip()\n",
    "\n",
    "\n",
    "def standardize(text):\n",
    "    return [x for x in (sent_standardize(sent) for sent in text) if len(x) > 3]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_paper(paper_id, sentences_df, ref_sents_df):\n",
    "    paper = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    paper = standardize(paper)\n",
    "    \n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "    \n",
    "    if len(paper) < 50:\n",
    "        return None\n",
    "            \n",
    "    if len(paper) > 100:\n",
    "        paper = list(paper[:50]) + list(paper[-50:])\n",
    "    \n",
    "    preprocessed_score = [sum(get_rouge(sent, ref_sent) for ref_sent in ref_sents) / len(ref_sents)\n",
    "                          for sent in paper]\n",
    "    return paper, preprocessed_score\n",
    "\n",
    "def preprocess_paper_with_features(paper_id, sentences_df, ref_sents_df, abstracts_df, \\\n",
    "                                   figures_df, reverse_ref_df, tables_df):\n",
    "    preprocessed_score = []\n",
    "    features = []\n",
    "    \n",
    "    paper = sentences_df[sentences_df['pmid'] == paper_id]['sentence']\n",
    "    paper = standardize(paper)\n",
    "    \n",
    "    sent_ids = sentences_df[sentences_df['pmid'] == paper_id]['sent_id']\n",
    "    \n",
    "    sent_types = sentences_df[sentences_df['pmid'] == paper_id]['type']\n",
    "    \n",
    "    ref_sents = ref_sents_df[ref_sents_df['ref_pmid'] == paper_id]['sentence']\n",
    "    ref_sents = standardize(ref_sents)\n",
    "\n",
    "    \n",
    "    fig_captions = figures_df[figures_df['pmid'] == paper_id]['caption']\n",
    "    fig_captions = standardize(fig_captions)\n",
    "    \n",
    "    tab_captions = tables_df[tables_df['pmid'] == paper_id]['caption']\n",
    "    tab_captions = standardize(tab_captions)\n",
    "    \n",
    "    abstract = abstracts_df[abstracts_df['pmid'] == paper_id]['abstract']\n",
    "    if len(abstract) != 0:\n",
    "        abstract = standardize(abstract)\n",
    "    \n",
    "    tmp_df = reverse_ref_df[reverse_ref_df['pmid'] == paper_id]\n",
    "    \n",
    "    if len(paper) < 50:\n",
    "        return None\n",
    "            \n",
    "    if len(paper) > 100:\n",
    "        paper = list(paper[:50]) + list(paper[-50:])\n",
    "        sent_ids = list(sent_ids[:50]) + list(sent_ids[-50:])\n",
    "        sent_types = list(sent_types[:50]) + list(sent_types[-50:])\n",
    "        \n",
    "        \n",
    "    def mean_rouge(sent, text):\n",
    "        try:\n",
    "            return sum(get_rouge(sent, ref_sent) for ref_sent in text) / len(text)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception at mean_rouge {e}')\n",
    "            return None\n",
    "    \n",
    "    def min_rouge(sent, text):\n",
    "        try:\n",
    "            score = 100000000\n",
    "            for ref_sent in text:\n",
    "                score = min(get_rouge(sent, ref_sent), score)\n",
    "            if score == 100000000:\n",
    "                return None\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception at min_rouge {e}')\n",
    "            return None\n",
    "    \n",
    "    def max_rouge(sent, text):\n",
    "        try:\n",
    "            score = -100000\n",
    "            for ref_sent in text:\n",
    "                score = max(get_rouge(sent, ref_sent), score)\n",
    "            if score == -100000:\n",
    "                return None\n",
    "            return score\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception at max_rouge {e}')\n",
    "            return None\n",
    "    \n",
    "    for i, sent in enumerate(paper):\n",
    "        score = mean_rouge(sent, ref_sents)\n",
    "        if score is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            abst_diff = get_rouge(sent, abstract[0])\n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception at preprocess_paper_with_features {e}')\n",
    "            abst_diff = None\n",
    "        num_refs = len(tmp_df[(tmp_df['sent_type'] == sent_types[i]) & (tmp_df['sent_id'] == sent_ids[i])])\n",
    "        preprocessed_score.append(score)\n",
    "        features.append((sent_ids[i], int(sent_types[i] == \"general\"), abst_diff, num_refs,\\\n",
    "                        mean_rouge(sent, fig_captions), mean_rouge(sent, tab_captions),\\\n",
    "                        min_rouge(sent, fig_captions), min_rouge(sent, tab_captions), \\\n",
    "                        max_rouge(sent, fig_captions), max_rouge(sent, tab_captions)))\n",
    "    return paper, preprocessed_score, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Download data to make datasets from it. If datasets with features are needed, the commented code here should be uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_df, sentences_df, review_files_df, reverse_ref_df = load_data(ft=False)\n",
    "\n",
    "# Uncomment to load additional features\n",
    "# citations_df, sentences_df, review_files_df, reverse_ref_df, \\\n",
    "#     abstracts_df, filelist_df, figures_df, tables_df = load_data()\n",
    "\n",
    "logging.info('Done loading references dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(sentences_df['pmid'].values))\n",
    "plt.hist(res.values(), bins=range(-1, 400))\n",
    "plt.title('Length of papers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. In case datasets are not yet created and `ref_sents_df` is also not yet created, let's create `ref_sents_df`.\\\n",
    "For each paper pmid there is a list of sentences from review papers in which the paper with this `pmid` is cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_SENTS_DF_PATH = f\"{os.path.expanduser(cfg.base_path)}/ref_sents.csv\"\n",
    "if os.path.exists(REF_SENTS_DF_PATH):\n",
    "    ref_sents_df = pd.read_csv(REF_SENTS_DF_PATH, sep='\\t')\n",
    "else:\n",
    "    logging.info('Creating reference sentences dataset')\n",
    "    ref_sents_df = pd.merge(citations_df, reverse_ref_df, left_on = ['pmid', 'ref_id'], right_on = ['pmid', 'ref_id'])\n",
    "    ref_sents_df = pd.merge(ref_sents_df, sentences_df, left_on = ['pmid', 'sent_type', 'sent_id'], right_on = ['pmid', 'type', 'sent_id'])\n",
    "    ref_sents_df = ref_sents_df[ref_sents_df['pmid'].isin(review_files_df['pmid'].values)]    \n",
    "    ref_sents_df = ref_sents_df.drop_duplicates()\n",
    "    logging.info(f'Len of unique ref_sents {len(set(ref_sents_df[\"ref_pmid\"]))}')\n",
    "    ref_sents_df = ref_sents_df[['pmid', 'ref_id', 'pub_type', 'ref_pmid', 'sent_type', 'sent_id', 'sentence']]\n",
    "    ref_sents_df.to_csv(REF_SENTS_DF_PATH, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ref_sents_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. To create a dataset with features use `preprocess_paper_with_features`. Otherwise, use `preprocess_paper`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_reference_sentences_dataset(sentences_df, ref_sents_df):\n",
    "    res = {}\n",
    "    inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "    linter = len(inter)\n",
    "    for i, pmid in enumerate(inter):\n",
    "        try:\n",
    "            # For dataset with additional features uncomment this\n",
    "    #         temp = preprocess_paper_with_features(pmid, sentences_df, ref_sents_df, abstracts_df,\n",
    "    #                                               figures_df, reverse_ref_df, tables_df)\n",
    "            temp = preprocess_paper(pmid, sentences_df, ref_sents_df)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Error during processing {pmid} {e}')\n",
    "            continue\n",
    "        if temp is None:\n",
    "            logging.warning(f'temp is None for {pmid}')\n",
    "            continue\n",
    "        res[pmid] = temp\n",
    "        print(f\"\\r{i}/{linter} {pmid} {np.mean(res[pmid][1])}\", end=\"\")\n",
    "\n",
    "    logging.info(f'Successfully preprocessed {len(res)} of {len(inter)} papers')\n",
    "\n",
    "    logging.info(f'Creating train dataset')\n",
    "    feature_names = ['sent_id', 'sent_type', 'r_abs', 'num_refs', \\\n",
    "                 'mean_r_fig', 'mean_r_tab',\\\n",
    "                 'min_r_fig', 'min_r_tab',\\\n",
    "                 'max_r_fig', 'max_r_tab']\n",
    "    train_dic = {'pmid':[], 'sentence':[], 'score':[],\\\n",
    "                'sent_id':[], 'sent_type':[], 'r_abs':[], 'num_refs':[], \\\n",
    "                     'mean_r_fig':[], 'mean_r_tab':[],\\\n",
    "                     'min_r_fig':[], 'min_r_tab':[],\\\n",
    "                     'max_r_fig':[], 'max_r_tab':[]}\n",
    "\n",
    "    for pmid, stat in tqdm(res.items()):\n",
    "        if len(stat) == 2:\n",
    "            for sent, score in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)    \n",
    "        else:\n",
    "            for sent, score, features in zip(*stat):\n",
    "                train_dic['pmid'].append(pmid)\n",
    "                train_dic['sentence'].append(sent)\n",
    "                train_dic['score'].append(score)\n",
    "                for name, val in zip(feature_names, features):\n",
    "                    train_dic[name].append(val)\n",
    "\n",
    "    train_df = pd.DataFrame({k:v for k,v in train_dic.items() if v})\n",
    "    logging.info(f'Full train dataset {len(train_df)}')\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = f'{os.path.expanduser(cfg.base_path)}/dataset.csv'\n",
    "if os.path.exists(TRAIN_DATASET_PATH):\n",
    "    train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
    "else:\n",
    "    train_df = process_reference_sentences_dataset(sentences_df, ref_sents_df)\n",
    "    train_df.to_csv(TRAIN_DATASET_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. A function to preprocess input text for `BERT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_paper_bert(text, max_len, tokenizer):\n",
    "    sents = [[tokenizer.artBOS.tkn] + tokenizer.tokenize(sent) + [tokenizer.artEOS.tkn]\n",
    "             for sent in text]\n",
    "    ids, segments, segment_signature = [], [], 0\n",
    "    n_setns = 0\n",
    "    for s in sents:\n",
    "        if len(ids) + len(s) <= max_len:\n",
    "            n_setns += 1\n",
    "            ids.extend(tokenizer.convert_tokens_to_ids(s))\n",
    "            segments.extend([segment_signature] * len(s))\n",
    "            segment_signature = (segment_signature + 1) % 2\n",
    "        else:\n",
    "            break\n",
    "    mask = [1] * len(ids)\n",
    "\n",
    "    pad_len = max(0, max_len - len(ids))\n",
    "    ids += [tokenizer.PAD.idx] * pad_len\n",
    "    mask += [0] * pad_len\n",
    "    segments += [segment_signature] * pad_len\n",
    "\n",
    "    return ids, mask, segments, n_setns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Splitting data into train/test/val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(set(train_df['pmid'].values)), test_size=0.2)\n",
    "test_ids, val_ids = train_test_split(test_ids, test_size = 0.4)\n",
    "train, test, val = train_df[train_df['pmid'].isin(train_ids)], \\\n",
    "    train_df[train_df['pmid'].isin(test_ids)], train_df[train_df['pmid'].isin(val_ids)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Dataset classes.\\\n",
    "`Other*` classes first preprocess all the data and then give out the batches.\\\n",
    "`Ordinary` data classes preprocess data before each batch to give out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Helpers for creating the model, training and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "\n",
    "\n",
    "class OtherTrainDatasetFeatures(Dataset):\n",
    "    \"\"\" Custom Train Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len):\n",
    "        self.df = dataframe\n",
    "        self.data = []\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        \n",
    "        for name in tqdm(self.names):\n",
    "            ex = self.df[self.df['pmid'] == name]\n",
    "            paper = ex['sentence'].values\n",
    "# Uncomment me in case of features are used            \n",
    "#             features = np.nan_to_num(ex[['sent_id', 'sent_type', 'r_abs',\n",
    "#                            'num_refs', 'mean_r_fig', 'mean_r_tab', \n",
    "#                            'min_r_fig', 'min_r_tab',\n",
    "#                            'max_r_fig', 'max_r_tab']].values.astype(float))\n",
    "#             abstract = standardize(ex.abstract)\n",
    "            total_sents = 0\n",
    "            while total_sents < len(paper):\n",
    "                magic = max(0, total_sents - 5)\n",
    "                article_ids, article_mask, article_segment, n_setns = \\\n",
    "                    preprocess_paper_bert(paper[magic:], self.article_len, self.tokenizer)\n",
    "                if n_setns <= 5:\n",
    "                    total_sents += 1\n",
    "                    continue\n",
    "                target_scores = ex['score'].values[magic:magic + n_setns] / 100\n",
    "                self.data.append((article_ids, article_mask, article_segment, target_scores)) #, features[magic:magic + n_setns]))\n",
    "                total_sents = magic + n_setns\n",
    "        \n",
    "        self.n_examples = len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "class OtherTrainDataset(Dataset):\n",
    "    \"\"\" Custom Train Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len):\n",
    "        self.df = dataframe\n",
    "        self.data = []\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "        \n",
    "        for name in tqdm(self.names):\n",
    "            ex = self.df[self.df['pmid'] == name]\n",
    "            paper = ex['sentence'].values\n",
    "#             abstract = standardize(ex.abstract)\n",
    "            total_sents = 0\n",
    "            while total_sents < len(paper):\n",
    "                magic = max(0, total_sents - 5)\n",
    "                article_ids, article_mask, article_segment, n_setns = \\\n",
    "                    preprocess_paper_bert(paper[magic:], self.article_len, self.tokenizer)\n",
    "                if n_setns <= 5:\n",
    "                    total_sents += 1\n",
    "                    continue\n",
    "                target_scores = ex['score'].values[magic:magic + n_setns] / 100\n",
    "                self.data.append((article_ids, article_mask, article_segment, target_scores))\n",
    "                total_sents = magic + n_setns\n",
    "        \n",
    "        self.n_examples = len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    \"\"\" Custom Train Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len):\n",
    "        self.df = dataframe\n",
    "        self.n_examples = len(set(dataframe['pmid'].values))\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.names[idx]\n",
    "        ex = self.df[self.df['pmid'] == idx]\n",
    "        paper = ex['sentence'].values\n",
    "        \n",
    "        # abstract = standardize(ex.abstract)\n",
    "        article_ids, article_mask, article_segment, n_setns = \\\n",
    "            preprocess_paper_bert(paper, self.article_len, self.tokenizer)\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_setns] / 100\n",
    "\n",
    "        return article_ids, article_mask, article_segment, target_scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "\n",
    "class EvalDatasetFeatures(Dataset):\n",
    "    \"\"\" Custom Valid/Test Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len):\n",
    "        self.df = dataframe\n",
    "        self.n_examples = len(set(dataframe['pmid'].values))\n",
    "        print(self.n_examples)\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.names[idx]\n",
    "        ex = self.df[self.df['pmid'] == idx]\n",
    "        paper = ex['sentence'].values\n",
    "        features = np.nan_to_num(ex[['sent_id', 'sent_type', 'r_abs',\n",
    "                           'num_refs', 'mean_r_fig', 'mean_r_tab', \n",
    "                           'min_r_fig', 'min_r_tab',\n",
    "                           'max_r_fig', 'max_r_tab']].values.astype(float))\n",
    "        \n",
    "        article_ids, article_mask, article_segment, n_setns = \\\n",
    "            preprocess_paper_bert(paper, self.article_len, self.tokenizer)\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_setns] / 100\n",
    "\n",
    "        return article_ids, article_mask, article_segment, target_scores, features[:n_setns]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_gold_sents(paper, gold_ids):\n",
    "        paper = sent_tokenize(paper)\n",
    "        gold_sents = [sent for i, sent in enumerate(paper) if i in gold_ids]\n",
    "        return gold_sents\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "    \n",
    "class EvalDataset(Dataset):\n",
    "    \"\"\" Custom Valid/Test Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, article_len):\n",
    "        self.df = dataframe\n",
    "        self.n_examples = len(set(dataframe['pmid'].values))\n",
    "        print(self.n_examples)\n",
    "        self.names = list(set(dataframe['pmid'].values))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.article_len = article_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.names[idx]\n",
    "        ex = self.df[self.df['pmid'] == idx]\n",
    "        paper = ex['sentence'].values\n",
    "        \n",
    "        article_ids, article_mask, article_segment, n_setns = \\\n",
    "            preprocess_paper_bert(paper, self.article_len, self.tokenizer)\n",
    "\n",
    "        # form target\n",
    "        target_scores = ex['score'].values[:n_setns] / 100\n",
    "\n",
    "        return article_ids, article_mask, article_segment, target_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_gold_sents(paper, gold_ids):\n",
    "        paper = sent_tokenize(paper)\n",
    "        gold_sents = [sent for i, sent in enumerate(paper) if i in gold_ids]\n",
    "        return gold_sents\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.review.utils import count_parameters\n",
    "from transformers import AdamW\n",
    "from torch.nn import BCELoss, BCEWithLogitsLoss, MSELoss\n",
    "\n",
    "def load_model(model_type, froze_strategy, rank, article_len, features=False):\n",
    "    model = Summarizer(model_type, article_len, features)\n",
    "    model.expand_posembs_ifneed()\n",
    "#     model.load('temp')\n",
    "    model.froze_backbone(froze_strategy)\n",
    "    model.unfroze_head()\n",
    "    if rank == 0:\n",
    "        print(f\"Model trainable parameters: {count_parameters(model)}\")\n",
    "    return model\n",
    "\n",
    "def train_collate_fn_ft(batch_data):\n",
    "    print(str(batch_data))\n",
    "    data0, data1, data2, data3, data4 = list(zip(*batch_data))\n",
    "    return torch.tensor(data0, dtype=torch.long), \\\n",
    "        torch.tensor(data1, dtype=torch.long), \\\n",
    "        torch.tensor(data2, dtype=torch.long), \\\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3], \\\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data4]\n",
    "\n",
    "def eval_collate_fn_ft(batch_data):\n",
    "    print(str(batch_data))\n",
    "    data0, data1, data2, data3, data4 = list(zip(*batch_data))\n",
    "\n",
    "    return torch.tensor(data0, dtype=torch.long), \\\n",
    "        torch.tensor(data1, dtype=torch.long), \\\n",
    "        torch.tensor(data2, dtype=torch.long), \\\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data3], \\\n",
    "        [torch.tensor(e, dtype=torch.float) for e in data4]\n",
    "\n",
    "def get_dataloaders(train, val, batch_size,\n",
    "                    article_len, tokenizer, ddp):\n",
    "    dl_func = create_ddp_loader if ddp else create_loader\n",
    "\n",
    "    logging.info('Creating train dataset...')\n",
    "    train_ds = OtherTrainDataset(train, tokenizer, article_len) \n",
    "#     train_ds = OtherTrainDatasetFeatures(train, tokenizer, article_len)     \n",
    "    logging.info('Applying loader functions to train...')\n",
    "    train_dl = dl_func(train_ds, batch_size, train_collate_fn)\n",
    "#     train_dl = dl_func(train_df, batch_size, train_collate_fn_ft)    \n",
    "    \n",
    "    logging.info('Creating val dataset...')    \n",
    "    val_ds = EvalDataset(val, tokenizer, article_len)\n",
    "#     val_ds = EvalDatasetFeatures(val, tokenizer, article_len)     \n",
    "    logging.info('Applying loader functions to val...')\n",
    "    val_dl = dl_func(val_ds, batch_size, eval_collate_fn)\n",
    "#     val_dl = dl_func(val_ds, batch_size, eval_collate_fn_ft)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def get_tools(model, enc_lr, dec_lr, warmup,\n",
    "              weight_decay, clip_value,\n",
    "              accumulation_interval):\n",
    "\n",
    "    enc_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and name.startswith('bert.')\n",
    "    ]\n",
    "    dec_parameters = [\n",
    "        param for name, param in model.named_parameters()\n",
    "        if param.requires_grad and not name.startswith('bert.')\n",
    "    ]\n",
    "    optimizer = AdamW([\n",
    "        {'params': enc_parameters, 'lr': enc_lr},\n",
    "        {'params': dec_parameters, 'lr': dec_lr},\n",
    "    ], weight_decay=weight_decay)\n",
    "    optimizer.clip_value = clip_value\n",
    "    optimizer.accumulation_interval = accumulation_interval\n",
    "\n",
    "    scheduler = NoamScheduler(optimizer, warmup=warmup)\n",
    "    criter = MSELoss()\n",
    "\n",
    "    return optimizer, scheduler, criter\n",
    "\n",
    "\n",
    "# def setup_multi_gpu(model, optimizer, rank, size):\n",
    "#     logging.info('Setup distributed settings...')\n",
    "#     distrib_config = DistributedConfig(local_rank=rank, size=size, amp_enabled=cfg.amp_enabled)\n",
    "#     setup_distributed(distrib_config)\n",
    "#     device = choose_device(local_rank=rank)\n",
    "#     model = model.to(device)\n",
    "#     model, optimizer = setup_apex_if_enabled(model, optimizer, config=distrib_config)\n",
    "#     model = setup_distrib_if_enabled(model, config=distrib_config)\n",
    "#     return model, device, optimizer\n",
    "\n",
    "\n",
    "def setup_single_gpu(model):\n",
    "    logging.info('Setup single-device settings...')\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criter, device, rank, writer, distributed):\n",
    "    model.eval()\n",
    "    model_ref = model.module if distributed else model\n",
    "    loss_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "    \n",
    "    #pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False, disable=rank != 0)\n",
    "    for idx_batch, batch in enumerate(dataloader):\n",
    "        input_ids, input_mask, input_segment, target_scores = \\\n",
    "            [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment,\n",
    "        )\n",
    "        #print(draft_probs.shape, target_scores.shape)\n",
    "        \n",
    "        # loss\n",
    "        loss = criter(\n",
    "            draft_probs,\n",
    "            target_scores,\n",
    "        )\n",
    "\n",
    "        # record a loss value\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    # overall loss per epoch\n",
    "    # if distributed:\n",
    "    #     loss_val = distribute(loss_val, device)\n",
    "    # logging.info(f\"mean loss: {loss_val / len(dataloader.dataset):.4f}\", is_print=rank == 0)\n",
    "    print(\"Val loss:\", loss_val/len(dataloader))\n",
    "    print(\"Mean sent len:\", mean_sents/szs)\n",
    "    # save model, just in case\n",
    "    model_ref.save('validated_weights.pth')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_ft(model, dataloader, criter, device, rank, writer, distributed):\n",
    "    model.eval()\n",
    "    model_ref = model.module if distributed else model\n",
    "    loss_val = 0\n",
    "    mean_sents = 0\n",
    "    szs = 0\n",
    "    \n",
    "    #pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False, disable=rank != 0)\n",
    "    for idx_batch, batch in enumerate(dataloader):\n",
    "        input_ids, input_mask, input_segment, target_scores, input_features = \\\n",
    "            [(x.to(device) if isinstance(x, torch.Tensor) else x) for x in batch]\n",
    "        sizes = [dc.shape[0] for dc in target_scores]\n",
    "        mean_sents += sum(sizes)\n",
    "        szs += len(sizes)\n",
    "        input_features = torch.cat(input_features).to(device)\n",
    "        target_scores = torch.cat(target_scores).to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        draft_probs = model(\n",
    "            input_ids, input_mask, input_segment, input_features,\n",
    "        )\n",
    "        #print(draft_probs.shape, target_scores.shape)\n",
    "        \n",
    "        # loss\n",
    "        loss = criter(\n",
    "            draft_probs,\n",
    "            target_scores,\n",
    "        )\n",
    "\n",
    "        # record a loss value\n",
    "        loss_val += loss.item()\n",
    "        writer.add_scalar(f\"Eval/loss\", loss.item(), writer.train_step)\n",
    "        writer.train_step += 1\n",
    "\n",
    "    # overall loss per epoch\n",
    "    # if distributed:\n",
    "    #     loss_val = distribute(loss_val, device)\n",
    "    # logging.info(f\"mean loss: {loss_val / len(dataloader.dataset):.4f}\", is_print=rank == 0)\n",
    "    print(\"Val loss:\", loss_val/len(dataloader))\n",
    "    print(\"Mean sent len:\", mean_sents/szs)\n",
    "    # save model, just in case\n",
    "    model_ref.save('validated_weights.pth')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = 512\n",
    "\n",
    "model = load_model(\"bert\", \"froze_all\", 0, MODEL_SIZE, features=False)    \n",
    "model, device = setup_single_gpu(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Create dataloaders and start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_fun_ft` for training model with additional features and `train_fun` without.\\\n",
    "Use `evaluate_ft` for training model with additional features and `evaluate` without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "MODEL_PATH = f'{os.path.expanduser(cfg.weights_path)}/learn_simple_berta.pth'\n",
    "\n",
    "def load_or_train_model(model):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logging.info(f'Loading model {MODEL_PATH}')\n",
    "        model.load(\"learn_simple_berta\")\n",
    "        model, device = setup_single_gpu(model)\n",
    "    else:\n",
    "        logging.info('Create dataloaders...')\n",
    "        model_ref = model\n",
    "        train_loader, valid_loader = get_dataloaders(train, val, 4, MODEL_SIZE, model_ref.tokenizer, ddp=False)\n",
    "\n",
    "        writer = SummaryWriter(log_dir=f'{os.path.expanduser(cfg.base_path)}/logs')\n",
    "        writer.train_step, writer.eval_step = 0, 0\n",
    "\n",
    "        optimizer, scheduler, criter = get_tools(model, 0.00001, 0.001, 5, 0.005, 1.0, 1)    \n",
    "\n",
    "        for epoch in range(1, 20 + 1):\n",
    "            logging.info(f\"{epoch} epoch training...\")\n",
    "\n",
    "            model, optimizer, scheduler, writer = train_fun(\n",
    "                model, train_loader, optimizer, scheduler,\n",
    "                criter, device, 0, writer, False\n",
    "            )\n",
    "        #     model, optimizer, scheduler, writer = train_fun_ft(\n",
    "        #         model, train_loader, optimizer, scheduler,\n",
    "        #         criter, device, 0, writer, False\n",
    "        #     )\n",
    "\n",
    "\n",
    "            logging.info(f\"{epoch} epoch validation...\")\n",
    "            model = evaluate(model, valid_loader, criter, device, 0, writer, False, )\n",
    "        #         model = evaluate_ft(model, valid_loader, criter, device, 0, writer, False, )\n",
    "        # Save trained model\n",
    "        model.save('learn_simple_berta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. See the example of how model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_or_train_model(model)\n",
    "model_ref = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first value\n",
    "ex = val[val['pmid'] == list(val['pmid'])[0]]\n",
    "paper = ex['sentence'].values\n",
    "article_ids, article_mask, article_segment, n_sents = \\\n",
    "            preprocess_paper_bert(paper, MODEL_SIZE, model_ref.tokenizer)\n",
    "res_sents = paper[:n_sents]\n",
    "scores = ex['score'].values[:n_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([article_ids]).to(device)\n",
    "input_mask = torch.tensor([article_mask]).to(device)\n",
    "input_segment = torch.tensor([article_segment]).to(device)\n",
    "draft_probs = model(input_ids, input_mask, input_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show_df = pd.DataFrame({\n",
    "    'sentence': res_sents, \n",
    "    'ideal_score': scores / 100, \n",
    "    'res_score': draft_probs.cpu().detach().numpy()\n",
    "})\n",
    "display(to_show_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(to_show_df[to_show_df['res_score'] > 0.07]['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show_df.to_csv(f'{cfg.base_path}/show_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Evaluation.\\\n",
    "First, let's see if the model trained well.\\\n",
    "Then will count an `MSE` score on some real example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' not in globals():\n",
    "    model = load_model(\"bert\", \"froze_all\", 0, MODEL_SIZE, False)\n",
    "    model.load(\"learn_simple_berta\")\n",
    "    model, device = setup_single_gpu(model)\n",
    "    model_ref = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "REF_SCORES_PATH = os.path.expanduser(f\"{cfg.base_path}/refs_and_scores.csv\")\n",
    "\n",
    "if os.path.exists(REF_SCORES_PATH):\n",
    "    final_ref_show_df = pd.read_csv(f\"{cfg.base_path}/refs_and_scores.csv\")\n",
    "else:\n",
    "    if 'train_loader' not in globals() or 'valid_loader' not in globals():\n",
    "        train_loader, valid_loader = get_dataloaders(train, val, 4, MODEL_SIZE, model_ref.tokenizer, ddp=False)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=os.path.expanduser(cfg.log_dir))\n",
    "    writer.train_step, writer.eval_step = 0, 0\n",
    "    optimizer, scheduler, criter = get_tools(model, 0.00001, 0.001, 5, 0.005, 1.0, 1)\n",
    "    model = evaluate(model, train_loader, criter, device, 0, writer, False,)\n",
    "\n",
    "    # model = evaluate_ft(model, valid_loader, criter, device, 0, writer, False,)    \n",
    "    to_show_ref = pd.merge(train_df, ref_sents_df[['ref_pmid', 'sentence']], \n",
    "                           left_on = ['pmid'], right_on = ['ref_pmid'])\n",
    "    to_show_ref = to_show_ref.rename(columns={'sentence_x': 'sentence', 'sentence_y': 'ref_sentence'})\n",
    "    to_show_ref = to_show_ref[['pmid', 'sentence', 'ref_sentence', 'score']]    \n",
    "    final_ref_show_dic = {'pmid': [], 'sentence': [], 'ref_sentence': [], 'score':[]}\n",
    "    ite = [(pmid, sent) for pmid, sent in to_show_ref[['pmid', 'sentence']].values]\n",
    "\n",
    "    for pmid, sent in tqdm(set(ite)):\n",
    "        refs_df = to_show_ref[(to_show_ref['pmid'] == pmid) & (to_show_ref['sentence'] == sent)]\n",
    "        final_ref_show_dic['pmid'].append(pmid)\n",
    "        final_ref_show_dic['sentence'].append(sent)\n",
    "        final_ref_show_dic['ref_sentence'].append(\" \".join(refs_df['ref_sentence'].values))\n",
    "        final_ref_show_dic['score'].append(refs_df['score'].values[0])\n",
    "    final_ref_show_df = pd.DataFrame(final_ref_show_dic)\n",
    "    final_ref_show_df.to_csv(REF_SCORES_PATH, index=False)    \n",
    "\n",
    "display(final_ref_show_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_test = final_ref_show_df[final_ref_show_df['pmid'].isin(set(val['pmid'].values))]\n",
    "display(to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'pmid':[], 'sentence':[], 'ref_sentences':[], 'score': [], 'res_score':[]}\n",
    "\n",
    "for id in tqdm(set(to_test['pmid'].values)):\n",
    "    ex = to_test[to_test['pmid'] == id]\n",
    "    paper = ex['sentence'].values\n",
    "    article_ids, article_mask, article_segment, n_sents = \\\n",
    "            preprocess_paper_bert(paper, MODEL_SIZE, model_ref.tokenizer)\n",
    "    res_sents = paper[:n_sents]\n",
    "    scores = ex['score'].values[:n_sents] / 100\n",
    "    input_ids = torch.tensor([article_ids]).to(device)\n",
    "    input_mask = torch.tensor([article_mask]).to(device)\n",
    "    input_segment = torch.tensor([article_segment]).to(device)\n",
    "    draft_probs = model(input_ids, input_mask, input_segment,)\n",
    "    for sent, sc, res_sc in zip(res_sents, scores, draft_probs.cpu().detach().numpy()):\n",
    "        res['pmid'].append(id)\n",
    "        res['sentence'].append(sent)\n",
    "        res['ref_sentences'].append(ex['ref_sentence'].values[0])\n",
    "        res['score'].append(sc)\n",
    "        res['res_score'].append(res_sc)\n",
    "res_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(f\"{cfg.base_path}/saved_example_refs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = ((res_df['score'].values - res_df['res_score'].values)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff**0.5 #The MSE score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "Cleanup the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Let's see what is the original score distribution to understand the quality of `MSE`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "res = Counter(list(train_df['score'].values))\n",
    "\n",
    "plt.hist(res.values(), bins=range(2, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ref = sentences_df[sentences_df['pmid']==26194312]['sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid']==26194312]['ref_pmid'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(papers_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. A test of several paper summarization into review one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .model import Summarizer as BertSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer('bert', MODEL_SIZE)\n",
    "# model.load('bert_sum')\n",
    "model.froze_backbone(\"froze_all\")\n",
    "model.unfroze_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model, device = setup_single_gpu(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features-related evaluation is commented right now.\n",
    "test_stat = {'rev_pmid':[], 'sent_num':[], 'true_rouge':[], 'diff_papers': []} #'rouge': [], \n",
    "inter = set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values)\n",
    "review_papers = list(set(ref_sents_df[ref_sents_df['ref_pmid'].isin(inter)]['pmid'].values))\n",
    "print('Review papers', len(review_papers))\n",
    "cnt = 0\n",
    "\n",
    "for rev_id in tqdm(review_papers):\n",
    "#     print(f\"\\r{rev_id} {cnt} / {len(review_papers)}\", end=\"\")\n",
    "    cnt += 1\n",
    "    paper_ref = sentences_df[sentences_df['pmid']==rev_id]['sentence'].values\n",
    "    papers_to_check = list(set(ref_sents_df[ref_sents_df['pmid']==rev_id]['ref_pmid'].values))\n",
    "    result = {'pmid':[], 'sentence':[], 'score':[]}\n",
    "    for paper_id in papers_to_check:\n",
    "        ex = test[test['pmid'] == paper_id]\n",
    "        paper = ex['sentence'].values\n",
    "        #features = np.nan_to_num(ex[['sent_id', 'sent_type', 'r_abs',\n",
    "        #                   'num_refs', 'mean_r_fig', 'mean_r_tab', \n",
    "        #                   'min_r_fig', 'min_r_tab',\n",
    "        #                   'max_r_fig', 'max_r_tab']].values.astype(float))\n",
    "        total_sents = 0\n",
    "        while total_sents < len(paper):\n",
    "            magic = max(0, total_sents - 5)\n",
    "            article_ids, article_mask, article_segment, n_setns = \\\n",
    "            preprocess_paper_bert(paper[magic:], MODEL_SIZE, model_ref.tokenizer)\n",
    "            if n_setns <= 5:\n",
    "                total_sents += 1\n",
    "                continue\n",
    "            old_total = total_sents\n",
    "            total_sents = magic + n_setns\n",
    "            input_ids = torch.tensor([article_ids]).to(device)\n",
    "            input_mask = torch.tensor([article_mask]).to(device)\n",
    "            input_segment = torch.tensor([article_segment]).to(device)\n",
    "            #input_features = [torch.tensor(e, dtype=torch.float) for e in features[magic:total_sents]]\n",
    "            #input_features = torch.stack(input_features).to(device)\n",
    "            #print(input_features)\n",
    "            draft_probs = model(\n",
    "                        input_ids, input_mask, input_segment, #input_features,\n",
    "                    )\n",
    "            result['pmid'].extend([paper_id] * (total_sents - old_total))\n",
    "            result['sentence'].extend(list(paper[old_total:total_sents]))\n",
    "            result['score'].extend(list(draft_probs.cpu().detach().numpy())[old_total - magic:])\n",
    "    res_df = pd.DataFrame(result)\n",
    "    sorted_arr = sorted(list(res_df['score'].values))\n",
    "    for i in range(5, 103, 5):\n",
    "        if len(sorted_arr) < i:\n",
    "            break\n",
    "        treshold = sorted_arr[-i]\n",
    "        final_text = res_df[res_df['score'] >= treshold][['pmid', 'sentence']]\n",
    "        #mean_score = 0\n",
    "        #num = 0\n",
    "        #for sent in final_text['sentence'].values:\n",
    "        #    for ref_sent in paper_ref:\n",
    "        #        try:\n",
    "        #            mean_score += get_rouge(sent, ref_sent)\n",
    "        #            num += 1\n",
    "        #        except Exception:\n",
    "        #            continue\n",
    "        #mean_score /= num\n",
    "        real_score = get_rouge(\" \".join(final_text['sentence'].values), \" \".join(paper_ref))\n",
    "        test_stat['rev_pmid'].append(rev_id)\n",
    "        test_stat['sent_num'].append(i)\n",
    "        #print(len(\" \".join(final_text['sentence'].values)), len(\" \".join(paper_ref)))\n",
    "        \n",
    "        \n",
    "        #test_stat['rouge'].append(mean_score)\n",
    "        test_stat['true_rouge'].append(real_score)\n",
    "        test_stat['diff_papers'].append(len(set(final_text['pmid'])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[len(arr) for key, arr in test_stat.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_df = pd.DataFrame(test_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_df = test_stat_df[test_stat_df['rev_pmid'] != 29574033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_df.to_csv(f\"{cfg.base_path}/simple_right_test_on_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = sorted(list(res_df['score'].values))[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = res_df[res_df['score'] >= treshold][['pmid', 'sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(final_text['pmid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(final_text['sentence'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = 0\n",
    "num = 0\n",
    "for sent in final_text['sentence'].values:\n",
    "    for ref_sent in paper_ref:\n",
    "        mean_score += get_rouge(sent, ref_sent)\n",
    "        num += 1\n",
    "mean_score /= num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = list(set(sentences_df['pmid'].values) & set(ref_sents_df['ref_pmid'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26194312 in inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sents_df[ref_sents_df['ref_pmid']==25559091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_stat_df = pd.read_csv(f\"{cfg.base_path}/simple_right_test_on_review.csv\")\n",
    "ft_test_stat_df = pd.read_csv(f\"{cfg.base_path}/bertsum_test_on_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test_stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = test_stat_df.assign(model = ['Основная модель']*len(test_stat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = ft_test_stat_df.assign(model = ['BERTSUM']*len(ft_test_stat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_df = pd.concat([df_1, df_2])\n",
    "draw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_means = []\n",
    "rouge_err = []\n",
    "papers_means = []\n",
    "papers_err = []\n",
    "\n",
    "for i in range(5, 103, 5):\n",
    "    tmp = test_stat_df.groupby(['sent_num']).get_group(i)\n",
    "    rouge_means.append(tmp['rouge'].mean())\n",
    "    rouge_err.append(tmp['rouge'].std())\n",
    "    papers_means.append(tmp['diff_papers'].mean())\n",
    "    papers_err.append(tmp['diff_papers'].std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), rouge_means, yerr=rouge_err, fmt='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(list(range(5, 103, 5)), papers_means, yerr=papers_err, fmt='-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"rouge\", kind=\"box\", hue='model', aspect=1.7, color='lightblue', data=draw_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ROUGE, %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"diff_papers\", kind=\"box\", aspect=1.5, color = 'lightblue', data=test_stat_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ЧИСЛО СТАТЕЙ В РЕЗЮМЕ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"sent_num\", y=\"true_rouge\", kind=\"box\", aspect=1.7, hue='model', color='lightblue', data=draw_df).set_axis_labels(\"ЧИСЛО ПРЕДЛОЖЕНИЙ\", \"ROUGE, %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
