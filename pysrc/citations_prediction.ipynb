{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to predict citations count\n",
    "* Using arXiv data\n",
    "* Using pubmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from pysrc.prediction.ss_arxiv_loader import SSArxivLoader\n",
    "from pysrc.prediction.ss_pubmed_loader import SSPubmedLoader\n",
    "from pysrc.prediction.predict_analyzer import PredictAnalyzer\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "# Use SSPubmedLoader for Pubmed data and SSArxivLoader for Arxiv data\n",
    "loader = SSPubmedLoader(config)\n",
    "# Analyzer configures progress in loader,\n",
    "analyzer = PredictAnalyzer(loader, config)\n",
    "\n",
    "ids = loader.search_pubmed(limit=1000)\n",
    "log = analyzer.analyze(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create balanced dataset with respect to cytations count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cytations histogram\n",
    "analyzer.df['total'].hist(bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance dataset of papers without any citations and with citations.\n",
    "papers_df_not_null = analyzer.df[analyzer.df.total > 0]\n",
    "papers_df_null = analyzer.df[analyzer.df.total == 0].head(n=10000)\n",
    "papers_df = pd.concat([papers_df_not_null, papers_df_null]).drop(columns=['crc32id', 'aux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced cytations histogram\n",
    "papers_df['total'].hist(bins=[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram by year\n",
    "papers_df['year'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only 1975-2015 years\n",
    "papers_df_years = papers_df[np.logical_and(1975 <= papers_df.year, papers_df.year <= 2015)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topics using LDA algorithm to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "topic_names = [f'topic{i}' for i in range(n_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 1995, 2016 # end year exclusive\n",
    "topics_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topic(row):\n",
    "    index = np.argmax(np.array(row))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from papers.analysis.text import tokenize\n",
    "\n",
    "# Note: this method was restored in papers.utils to restore notebook functionality.\n",
    "# TODO: Refactor with newer code.\n",
    "def lda_topics_df(df, n_words, n_topics):\n",
    "    logging.info(f'Building corpus from {len(df)} articles')\n",
    "    corpus = [f'{title} {abstract}'\n",
    "              for title, abstract in zip(df['title'], df['abstract'])]\n",
    "\n",
    "    logging.info(f'Counting word usage in the corpus, using only {n_words} most frequent words')\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda t: tokenize(t), \n",
    "                                 max_features=n_words, min_df=0.01, max_df=0.8)\n",
    "    vectorized = vectorizer.fit_transform(corpus)\n",
    "    logging.info(f'Output shape: {vectorized.shape}')\n",
    "\n",
    "    logging.info(f'Performing LDA topic analysis')\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(vectorized)\n",
    "\n",
    "    topics = lda.transform(vectorized)\n",
    "    logging.info('Done')\n",
    "    return topics, lda, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_lda_topics(lda, vectorizer, n_top_words=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    explanations = {}\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        explanations[i] = [(topic[i], feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Find topics for year {year}\")\n",
    "    topics, lda, vectorizer = lda_topics_df(papers_df_years[papers_df_years.year <= year], \n",
    "                                            n_words=1000, n_topics=n_topics)\n",
    "    \n",
    "    topics_df = pd.DataFrame(data=topics, columns=topic_names)\n",
    "    topics_df.index = papers_df_years[papers_df_years.year <= year].index\n",
    "    topics_df['main_topic'] = topics_df[topic_names].apply(lambda row: find_topic(row), axis=1)\n",
    "    \n",
    "    explanations = explain_lda_topics(lda, vectorizer, n_top_words=20)\n",
    "    \n",
    "    topics_info[year] = {'topics': topics_df, 'lda': lda, 'vectorizer': vectorizer, 'explanations': explanations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_info[2000]['explanations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count citations before each year (including this year)\n",
    "def before_year_citations(df):\n",
    "    df[f'before_{start_year}'] = df[start_year]\n",
    "    for year in range(start_year + 1, end_year):\n",
    "        df[f'before_{year}'] = df[f'before_{year - 1}'] + df[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_year_citations(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_citations = {}\n",
    "topic_ranks = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Counting topics citations for year {year}\")\n",
    "    # count citations of each topic\n",
    "    topic_citations[year] = []\n",
    "    for i in range(n_topics):\n",
    "        p_topic_i = topics_info[year]['topics'][f'topic{i}']\n",
    "        \n",
    "        cit_documents = papers_df_years[papers_df_years.year <= year][f'before_{year}']\n",
    "        assert p_topic_i.shape[0] == cit_documents.shape[0]\n",
    "        \n",
    "        topic_citations[year].append(np.dot(p_topic_i, cit_documents))\n",
    "    topic_ranks[year] = pd.Series(topic_citations[year]).rank(ascending=False, method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_rank(row):\n",
    "    df = topics_info[row.year]['topics']\n",
    "    main_topic = df.loc[row.name,:]['main_topic']\n",
    "    topics_rank = topic_ranks[row.year][main_topic]\n",
    "    return topics_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df_years['topic_rank'] = papers_df_years[papers_df_years.year >= 1995].apply(lambda row: get_topic_rank(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diversity(row):\n",
    "    topic_probs = topics_info[row.year]['topics'].loc[row.name,:][topic_names]\n",
    "    return np.dot(list(topic_probs), np.log(list(topic_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df_years['diversity'] = papers_df_years[papers_df_years.year >= 1995].apply(lambda row: get_diversity(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (aurhors and journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scholarmetrics\n",
    "from statistics import mean\n",
    "from scholarmetrics import hindex, gindex\n",
    "# An h-index of x means that the author has at least x publications that have been cited at least x times.\n",
    "# An g-index of x means that the authorâ€™s top x publications together accumulated at least x2 citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_after_n_years(row, n):\n",
    "    paper_year = row['year']\n",
    "    cit = 0\n",
    "    for cur_year in range(paper_year, paper_year + n):\n",
    "        if cur_year in row:\n",
    "            cit += row[cur_year]\n",
    "    return cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal_rank_and_mean(df):\n",
    "    journals_citations_years = pd.DataFrame()\n",
    "    for year in range(start_year, end_year):\n",
    "        journals_citations_cur = df[df.year <= year][['journal', 'total']]\\\n",
    "                                                .groupby(['journal'])\\\n",
    "                                                .agg({'total': 'mean'}).reset_index()\\\n",
    "                                                .rename(columns={'total':'journal_citations'})\n",
    "        \n",
    "        journals_citations_cur['journal'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "        journals_citations_cur.dropna(subset=['journal'], inplace=True)\n",
    "        \n",
    "        journals_citations_cur['rank'] = journals_citations_cur['journal_citations'].rank(ascending=False, \n",
    "                                                                                          method='min')\n",
    "        journals_citations_cur['year'] = year\n",
    "        journals_citations_years = pd.concat([journals_citations_years, journals_citations_cur], axis=0)\n",
    "        \n",
    "    return journals_citations_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_journals_graph(df, cit_df):\n",
    "    with_journal_out = pd.merge(cit_df, df[['id', 'journal']], left_on='id_out', right_on='id')\\\n",
    "                                .rename(columns={'journal': 'journal_out'})\\\n",
    "                                .drop(columns=['id'])\n",
    "    \n",
    "    journal_edges = pd.merge(with_journal_out, df[['id', 'journal']], left_on='id_in', right_on='id')\\\n",
    "                                .rename(columns={'journal': 'journal_in'})\\\n",
    "                                .drop(columns=['id'])[['journal_out', 'journal_in']]\n",
    "    # clear empty journals\n",
    "    journal_edges.replace({'': np.nan}, inplace=True)\n",
    "    journal_edges.dropna(inplace=True)\n",
    "    \n",
    "    journal_edges = journal_edges.groupby(['journal_out', 'journal_in']).size().reset_index(name='weight')\n",
    "    \n",
    "    # build graph\n",
    "    journal_graph = nx.from_pandas_edgelist(journal_edges, 'journal_out', 'journal_in', 'weight')\n",
    "    \n",
    "    return journal_graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_journals = {}\n",
    "pagerank_journals_df = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Count pagerank of each journal for year {year}\")\n",
    "    journals_graph = build_journals_graph(papers_df_years[papers_df_years.year <= year], analyzer.cit_df)\n",
    "    pagerank_journals[year] = nx.pagerank(journals_graph, alpha=0.85)\n",
    "    pagerank_journals_df[year] = pd.DataFrame([pagerank_journals[year]]).transpose().reset_index()\n",
    "    pagerank_journals_df[year].columns = ['journal', 'pagerank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years = journal_rank_and_mean(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.metadata import split_df_list\n",
    "\n",
    "\n",
    "def build_authors_graph(df, cit_df):\n",
    "    authors_df = df[['authors', 'id']]\n",
    "    authors_df['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    authors_df.dropna(subset=['authors'], inplace=True)\n",
    "    \n",
    "    authors_df = split_df_list(authors_df, target_column='authors', separator=', ')\\\n",
    "                        .rename(columns={'authors':'author'})\n",
    "    \n",
    "    with_author_out = pd.merge(cit_df, authors_df, left_on='id_out', right_on='id')\\\n",
    "                                .rename(columns={'author': 'author_out'})\\\n",
    "                                .drop(columns=['id'])\n",
    "    \n",
    "    author_edges = pd.merge(with_author_out, authors_df, left_on='id_in', right_on='id')\\\n",
    "                                .rename(columns={'author': 'author_in'})\\\n",
    "                                .drop(columns=['id'])[['author_out', 'author_in']]\n",
    "    \n",
    "    # clear empty authors\n",
    "    author_edges.replace({'': np.nan}, inplace=True)\n",
    "    author_edges.dropna(inplace=True)\n",
    "    \n",
    "    author_edges = author_edges.groupby(['author_out', 'author_in']).size().reset_index(name='weight')\n",
    "    \n",
    "    # build graph\n",
    "    author_graph = nx.from_pandas_edgelist(author_edges, 'author_out', 'author_in', 'weight')\n",
    "    \n",
    "    return author_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count pagerank for graph of authors citation and productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_graph_features = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Started counting graph of authors citations for year {year}\")\n",
    "    authors_graph = build_authors_graph(papers_df_years[papers_df_years.year <= year], analyzer.cit_df)\n",
    "    author_graph_features[year] = nx.pagerank(authors_graph, alpha=0.85)\n",
    "    \n",
    "    author_graph_features[year] = {k: {'pagerank': v, 'productivity': 0} \n",
    "                                   for k, v in author_graph_features[year].items()}\n",
    "    for author, _, weight in nx.selfloop_edges(authors_graph, data='weight'):\n",
    "        author_graph_features[year][author]['productivity'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_features(df):\n",
    "    author_total = df[['authors', 'total', 'year']]\n",
    "    author_total['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_total.dropna(subset=['authors'], inplace=True)\n",
    "    author_total['co_authors'] = author_total['authors'].apply(lambda authors: len(authors.split(', ')) - 1)\n",
    "    \n",
    "    author_total = split_df_list(author_total, target_column='authors', separator=', ')\n",
    "    \n",
    "    authors_dict_years = {}\n",
    "    for year in range(start_year, end_year):\n",
    "        logging.info(f\"Started counting authors ranks and mean number citations for year {year}\")\n",
    "        authors_citations_groupped = author_total[author_total.year <= year].groupby(['authors'])\n",
    "        authors_citations = authors_citations_groupped.agg({'total': ['mean', hindex, gindex], 'co_authors': 'mean'}).reset_index()\n",
    "        authors_citations.columns = authors_citations.columns.droplevel(level=1)\n",
    "        authors_citations.columns = ['author', 'total', 'hindex', 'gindex', 'co_authors']\n",
    "\n",
    "        authors_citations = authors_citations.loc[authors_citations['author'] != '']\n",
    "        authors_citations['rank'] = authors_citations['total'].rank(ascending=False, method='min')\n",
    "        cur_authors_dict = authors_citations.set_index('author')[['total', 'rank', 'hindex', 'gindex', 'co_authors']]\\\n",
    "                                                            .to_dict(orient='index')\n",
    "\n",
    "        authors_dict_years[year] = cur_authors_dict\n",
    "\n",
    "    return authors_dict_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict_years = author_features(papers_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None, None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    default_features = {'rank': None, 'hindex': None, 'gindex': None, 'co_authors': None}\n",
    "    features_of_given_authors = list(map(lambda author: authors_dict_years[year][author] \n",
    "                                    if author in authors_dict_years[year] else default_features,\n",
    "                                    authors_list))\n",
    "    \n",
    "    ranks = filter(None.__ne__, map(lambda features: features['rank'], features_of_given_authors))\n",
    "    hindexes = filter(None.__ne__, map(lambda features: features['hindex'], features_of_given_authors))\n",
    "    gindexes = filter(None.__ne__, map(lambda features: features['gindex'], features_of_given_authors))\n",
    "    socialities = filter(None.__ne__, map(lambda features: features['co_authors'], features_of_given_authors))\n",
    "\n",
    "    return pd.Series([mean(ranks), mean(hindexes), mean(gindexes), mean(socialities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_graph_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "        \n",
    "    default_features = {'pagerank': np.nan, 'productivity': np.nan}\n",
    "    features_of_given_authors = list(map(lambda author: author_graph_features[year][author] \n",
    "                                    if author in author_graph_features[year] else default_features,\n",
    "                                    authors_list))\n",
    "    \n",
    "    pageranks = filter(None.__ne__, map(lambda features: features['pagerank'], features_of_given_authors))\n",
    "    productivities = filter(None.__ne__, map(lambda features: features['productivity'], features_of_given_authors))\n",
    "    return pd.Series([mean(pageranks), mean(productivities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_papers(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    papers_of_given_authors = list(map(lambda author: authors_papers_dict[author] \n",
    "                                    if author in authors_papers_dict else 1,\n",
    "                                   authors_list))\n",
    "\n",
    "    return pd.Series([mean(papers_of_given_authors), max(papers_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_citations(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    cit_of_given_authors = list(map(lambda author: authors_dict[author] if author in authors_dict else 0,\n",
    "                                    authors_list))\n",
    "\n",
    "    return pd.Series([mean(cit_of_given_authors), max(cit_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_as_in_paper(df2, step=5, current_year=2020):\n",
    "    df = df2.copy()\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['recency'] = current_year - df['year']\n",
    "    final_features = ['recency', 'topic_rank', 'diversity']\n",
    "    \n",
    "#   early citations (cumulative)\n",
    "    for i in range(1, step + 2):\n",
    "        feature_name = f'c{i}'\n",
    "        df[feature_name] = df.apply(lambda row: citations_after_n_years(row, n=i), axis=1)\n",
    "        \n",
    "    logging.info(\"Done counting early citations\")\n",
    "    \n",
    "    \n",
    "    features_to_add = ['authors_mean_rank', 'authors_mean_hindex', 'authors_mean_gindex', 'authors_mean_sociality']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_features(row), axis=1)\n",
    "    \n",
    "    logging.info(\"Done counting author rank, h-indexes, g-indexes and sociality\")\n",
    "\n",
    "    features_to_add = ['authors_mean_pagerank', 'authors_mean_productivity']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_graph_features(row), axis=1)\n",
    "    \n",
    "    logging.info(\"Done counting author pagerank and productivity\")\n",
    "    \n",
    "    df = pd.merge(df, pagerank_journals_df[year], on='journal', how='left')\\\n",
    "                                        .rename(columns={'pagerank': 'journal_pagerank'})\n",
    "    df = pd.merge(df, journals_citations_years[['journal', 'rank', 'year']], on=['journal', 'year'], how='left')\\\n",
    "                                            .rename(columns={'rank': 'journal_rank'})\n",
    "    final_features += ['journal_pagerank', 'journal_rank']\n",
    "    logging.info(\"Done counting rank and pagerank of each journal\")\n",
    "\n",
    "#   extra features\n",
    "    df['title_len'] = df['title'].apply(lambda title: 0 if pd.isnull(title) else len(title))\n",
    "    df['abstract_len'] = df['abstract'].apply(lambda abstract: 0 if pd.isnull(abstract) else len(abstract))\n",
    "    df['n_authors'] = df['authors'].apply(lambda authors: len(authors.split(', ')))\n",
    "    final_features += ['title_len', 'abstract_len', 'n_authors']\n",
    "\n",
    "    final_targets = ['c1', 'c5']\n",
    "    \n",
    "    return df[final_features + final_targets], final_features, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, features, targets = preprocess_as_in_paper(papers_df_years[papers_df_years.year >= 1995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_features = ['recency', 'topic_rank', 'diversity', 'authors_mean_rank', 'authors_mean_hindex',\n",
    "                'authors_mean_sociality', 'authors_mean_pagerank', 'authors_mean_productivity',\n",
    "                'journal_pagerank', 'journal_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist(figsize=(40, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_log = ['authors_mean_pagerank', 'authors_mean_sociality']\n",
    "logged_features = []\n",
    "for f in features_to_log:\n",
    "    if test_df[f].min(skipna=True) >= 0:\n",
    "        test_df['log_' + f] = test_df[f].apply(lambda x: np.log1p(x))\n",
    "        logged_features.append('log_' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df.astype(np.float32)\n",
    "print(\"Shape before imputing nans\", df.shape)\n",
    "# Imputer silently removes columns, with empty values    \n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(df)\n",
    "df = pd.DataFrame(imp.transform(df))\n",
    "print(\"Shape after\", df.shape)\n",
    "imputed_features = [c for c in test_df.columns if not test_df[c].isnull().values.all()]\n",
    "df.columns = imputed_features\n",
    "print(\"tranform done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with different features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, features, target, model=LinearRegression(), show_plt=False, log_target=False, n=1):\n",
    "    features = [f for f in features + [target] if f in imputed_features]\n",
    "    train_validate = df[df.recency > step + 1][features]\n",
    "    train = train_validate[train_validate.recency > 11] \n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "    \n",
    "    train.recency = train.recency\n",
    "    validate.recency = validate.recency\n",
    "        \n",
    "    coefs = []\n",
    "    r_squared = []\n",
    "    rmse = []\n",
    "    for i in range(n):\n",
    "        frac = 0.8\n",
    "        train_sample = train.sample(frac=frac)\n",
    "        X = train_sample.iloc[:,:-1]\n",
    "        y = train_sample.iloc[:,-1]\n",
    "        val_sample = validate.sample(frac=frac)\n",
    "        X_validate = val_sample.iloc[:,:-1]\n",
    "        y_validate = val_sample.iloc[:,-1]\n",
    "\n",
    "        if log_target:\n",
    "            y = np.log(y + 1)\n",
    "            y_validate = np.log(y_validate + 1)\n",
    "\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        X_validate = scaler.transform(X_validate)\n",
    "\n",
    "        reg = model.fit(X, y)\n",
    "        coefs.append(reg.coef_)\n",
    "        \n",
    "        if show_plt:\n",
    "            x = []\n",
    "            for a, b in zip(list(y_validate), list(reg.predict(X_validate))):\n",
    "                if a != 0:\n",
    "                    x.append(b - a)\n",
    "            plt.hist(x, bins=50)\n",
    "            plt.show()\n",
    "\n",
    "        r_squared.append((reg.score(X, y), reg.score(X_validate, y_validate)))\n",
    "        rmse.append((sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate))))\n",
    "        print(f\"R^2 train: {reg.score(X, y)} validate: {reg.score(X_validate, y_validate)}\")\n",
    "        print(f\"RMSE train: {sqrt(mse(reg.predict(X), y))} validate: {sqrt(mse(reg.predict(X_validate), y_validate))}\")\n",
    "    \n",
    "    return reg, (X, y), (X_validate, y_validate), (coefs, r_squared, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_influencers(df, features, reg, coefs=None, n=10):\n",
    "    if coefs:\n",
    "        maxcoef = np.argsort(-np.abs(coefs))\n",
    "        coef = np.array(coefs)[maxcoef]\n",
    "    else:\n",
    "        maxcoef = np.argsort(-np.abs(reg.coef_))\n",
    "        coef = reg.coef_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict c5 given c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'c5'\n",
    "all_features = features + ['c1'] # + logged_features\n",
    "print('All features', len(all_features))\n",
    "print('Test DF shape', test_df.shape)\n",
    "all_features.remove('authors_mean_gindex')\n",
    "reg, (X, y), (X_val, y_val), extras = predict(df, all_features, target, model=LassoCV(cv=5), show_plt=True, log_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_influencers(test_df, all_features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reg.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict c_5 without any early citations info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.remove('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, (X, y), (X_val, y_val), extras = predict(df, all_features, 'c5', show_plt=True, log_target=True)\n",
    "print_top_influencers(test_df, all_features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(['c1'], axis=1).to_csv('/mnt/stripe/shpynov/predict.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or with regularisation (L1 or L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lasso regularisation; target {target}\")\n",
    "Ls, (X, y), (X_val, y_val), extras = predict(test_df, all_features, target, model=LassoCV(cv=5), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ls.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ridge regularisation; target {target}\")\n",
    "Rr, (X, y), (X_val, y_val), extras = predict(test_df, all_features, target, model=RidgeCV(), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train: 0.7480838199160941 validate: 0.7585117032531079\n",
      "RMSE train: 0.5133033271582135 validate: 0.5609957662089448\n",
      "R^2 train: 0.6964768672196057 validate: 0.614092577523093\n",
      "RMSE train: 0.5546585397816535 validate: 0.6625451198544108\n",
      "R^2 train: 0.7349569825002056 validate: 0.7030860588351935\n",
      "RMSE train: 0.5204738298097811 validate: 0.6487477031659755\n",
      "R^2 train: 0.7452345178564034 validate: 0.6468999297724086\n",
      "RMSE train: 0.5117542458373483 validate: 0.6967121139179533\n",
      "R^2 train: 0.7483363961106011 validate: 0.7884901872230092\n",
      "RMSE train: 0.5241395029126203 validate: 0.5399583476833143\n",
      "R^2 train: 0.7320316134526179 validate: 0.724459750941043\n",
      "RMSE train: 0.5482196625205776 validate: 0.6047741258657239\n",
      "R^2 train: 0.730187919391621 validate: 0.7252166956388093\n",
      "RMSE train: 0.5578045638772733 validate: 0.6015310403103215\n",
      "R^2 train: 0.7202996870055671 validate: 0.6851475950660646\n",
      "RMSE train: 0.5403831723857299 validate: 0.662854992926598\n",
      "R^2 train: 0.7085222769747251 validate: 0.6176083204798556\n",
      "RMSE train: 0.5372507082782264 validate: 0.6777617039515943\n",
      "R^2 train: 0.7450180247653244 validate: 0.6601367191035176\n",
      "RMSE train: 0.5183213853174512 validate: 0.6578647864576512\n",
      "R^2 train: 0.7119648977410813 validate: 0.6874307383521678\n",
      "RMSE train: 0.5616630049038086 validate: 0.6235771194951149\n",
      "R^2 train: 0.7355422332785353 validate: 0.6139504026440303\n",
      "RMSE train: 0.5327169696265441 validate: 0.7057335256369193\n",
      "R^2 train: 0.7270262848056667 validate: 0.6301211957516335\n",
      "RMSE train: 0.563119573577579 validate: 0.6946595158262303\n",
      "R^2 train: 0.7414052097527724 validate: 0.7205676329862145\n",
      "RMSE train: 0.5222468427705995 validate: 0.6168094636141014\n",
      "R^2 train: 0.7263816285087905 validate: 0.7409554038961584\n",
      "RMSE train: 0.5520265448051478 validate: 0.5841967091428822\n",
      "R^2 train: 0.7088867338914115 validate: 0.7339172423049034\n",
      "RMSE train: 0.5663948321013763 validate: 0.5832226228105577\n",
      "R^2 train: 0.720031408465458 validate: 0.7040610319991352\n",
      "RMSE train: 0.550496291446927 validate: 0.619755100058452\n",
      "R^2 train: 0.7207120153731377 validate: 0.7370419849865277\n",
      "RMSE train: 0.5416085474916816 validate: 0.5971099750496318\n",
      "R^2 train: 0.7397208306716219 validate: 0.69618451174287\n",
      "RMSE train: 0.5198210001474451 validate: 0.622060179815006\n",
      "R^2 train: 0.7142947609793174 validate: 0.7018283395372706\n",
      "RMSE train: 0.5250805077813028 validate: 0.6309205489127572\n",
      "R^2 train: 0.7461689933125025 validate: 0.782371887780922\n",
      "RMSE train: 0.5332273820103373 validate: 0.5495483180159432\n",
      "R^2 train: 0.7024657212564978 validate: 0.7210274528421614\n",
      "RMSE train: 0.5763506854340736 validate: 0.6485295287860166\n",
      "R^2 train: 0.7178927403061397 validate: 0.710924425719591\n",
      "RMSE train: 0.5430747553034906 validate: 0.6098323475159987\n",
      "R^2 train: 0.7286863176924108 validate: 0.7172743158271063\n",
      "RMSE train: 0.5558157179222523 validate: 0.620703424606107\n",
      "R^2 train: 0.7131764388430717 validate: 0.7163865351088299\n",
      "RMSE train: 0.5651565972776139 validate: 0.6213262110963303\n",
      "R^2 train: 0.7162236244595319 validate: 0.6759226263012557\n",
      "RMSE train: 0.5452088451516715 validate: 0.6645488080311447\n",
      "R^2 train: 0.7078076515701657 validate: 0.711166293041466\n",
      "RMSE train: 0.5674915308047295 validate: 0.6197273530903945\n",
      "R^2 train: 0.7291591505506458 validate: 0.7150600582479677\n",
      "RMSE train: 0.5544721427108061 validate: 0.6063393271401456\n",
      "R^2 train: 0.7522051110868813 validate: 0.7639349086103493\n",
      "RMSE train: 0.5042369260251487 validate: 0.5629506425277386\n",
      "R^2 train: 0.7289662965282595 validate: 0.7338390985082469\n",
      "RMSE train: 0.5392296020689202 validate: 0.597675323181984\n",
      "R^2 train: 0.7372767115634234 validate: 0.7014321964640726\n",
      "RMSE train: 0.548254344385471 validate: 0.6465196398195447\n",
      "R^2 train: 0.7303867314104598 validate: 0.6401406038122444\n",
      "RMSE train: 0.511163483086867 validate: 0.6841348305038955\n",
      "R^2 train: 0.7101755793220408 validate: 0.7498310452822938\n",
      "RMSE train: 0.5492569412258176 validate: 0.5828633600701131\n",
      "R^2 train: 0.7300641507906662 validate: 0.7390596914345751\n",
      "RMSE train: 0.5268969302970009 validate: 0.5994203996618541\n",
      "R^2 train: 0.7398159080681841 validate: 0.7428090368260747\n",
      "RMSE train: 0.5334807012279708 validate: 0.5865521576690595\n",
      "R^2 train: 0.7258599539812152 validate: 0.7620122739879287\n",
      "RMSE train: 0.5577700216751236 validate: 0.5600622122131365\n",
      "R^2 train: 0.7184444765367353 validate: 0.7028259164525101\n",
      "RMSE train: 0.5614121036452634 validate: 0.6133717512670898\n",
      "R^2 train: 0.7278439018723891 validate: 0.7006849556297061\n",
      "RMSE train: 0.547204681205176 validate: 0.6180194366154678\n",
      "R^2 train: 0.7148277812847736 validate: 0.7121257955021946\n",
      "RMSE train: 0.5563005445854488 validate: 0.6327766125710804\n",
      "R^2 train: 0.7457566095327147 validate: 0.7242748128088852\n",
      "RMSE train: 0.5253324795565146 validate: 0.6054706450401921\n",
      "R^2 train: 0.7400206351894587 validate: 0.7080315136073482\n",
      "RMSE train: 0.5474478854454984 validate: 0.6316386980677308\n",
      "R^2 train: 0.7267210706356391 validate: 0.7116638623892069\n",
      "RMSE train: 0.5508713073693267 validate: 0.618320125989737\n",
      "R^2 train: 0.7357116829483166 validate: 0.7041769062474696\n",
      "RMSE train: 0.5464541587083553 validate: 0.6439593234055098\n",
      "R^2 train: 0.7261169200842327 validate: 0.6768151745508599\n",
      "RMSE train: 0.5344192832310884 validate: 0.6749192622253735\n",
      "R^2 train: 0.7642022992711457 validate: 0.7301082647779469\n",
      "RMSE train: 0.5062849692052712 validate: 0.6014485189282157\n",
      "R^2 train: 0.7212197939947211 validate: 0.7446768317508686\n",
      "RMSE train: 0.5531761372090648 validate: 0.5809341177346861\n",
      "R^2 train: 0.741069248343027 validate: 0.7260383260437304\n",
      "RMSE train: 0.5336318475798175 validate: 0.5997577943299381\n",
      "R^2 train: 0.7221125528633925 validate: 0.70659597608099\n",
      "RMSE train: 0.5411385627187174 validate: 0.6411199750935773\n",
      "R^2 train: 0.7185445465191478 validate: 0.7751773420306413\n",
      "RMSE train: 0.5519429394213456 validate: 0.5552170603091913\n",
      "R^2 train: 0.7149463119629769 validate: 0.7144101280429372\n",
      "RMSE train: 0.5625899295974878 validate: 0.621703586244847\n",
      "R^2 train: 0.7148337161256038 validate: 0.7511917993886926\n",
      "RMSE train: 0.5404538429412312 validate: 0.600115459975741\n",
      "R^2 train: 0.7085690361568939 validate: 0.7540298846873843\n",
      "RMSE train: 0.5543989854976047 validate: 0.5938724341396303\n",
      "R^2 train: 0.7539543047027043 validate: 0.6710100625836137\n",
      "RMSE train: 0.5222204207356586 validate: 0.6607169755122396\n",
      "R^2 train: 0.7123738971670137 validate: 0.7278635031407927\n",
      "RMSE train: 0.552173748050206 validate: 0.6079986140225951\n",
      "R^2 train: 0.7211399986798843 validate: 0.7441440974776481\n",
      "RMSE train: 0.5543035064580164 validate: 0.5892781204502237\n",
      "R^2 train: 0.7158800542441972 validate: 0.745498731755801\n",
      "RMSE train: 0.5617932011655663 validate: 0.5967036116029004\n",
      "R^2 train: 0.7415414593354193 validate: 0.7666075856371235\n",
      "RMSE train: 0.5406311817587524 validate: 0.5706689321730705\n",
      "R^2 train: 0.7508833386917825 validate: 0.7377565655480066\n",
      "RMSE train: 0.5299470856181259 validate: 0.5899741679129267\n",
      "R^2 train: 0.7364928807718365 validate: 0.6887943596998864\n",
      "RMSE train: 0.529458784838952 validate: 0.6421154640801103\n",
      "R^2 train: 0.7109569959527274 validate: 0.7410456338294179\n",
      "RMSE train: 0.5544268573302428 validate: 0.5813647300536532\n",
      "R^2 train: 0.7051146278936674 validate: 0.698840953801166\n",
      "RMSE train: 0.5569821029746446 validate: 0.6140367750064594\n",
      "R^2 train: 0.7327300578120879 validate: 0.7155980566668522\n",
      "RMSE train: 0.5438063767683261 validate: 0.6208360965291772\n",
      "R^2 train: 0.7148802168330389 validate: 0.7113566771409372\n",
      "RMSE train: 0.5419258145768582 validate: 0.6194937387970539\n",
      "R^2 train: 0.7229449957958721 validate: 0.7032050137592132\n",
      "RMSE train: 0.5465924759371731 validate: 0.6258854554241844\n",
      "R^2 train: 0.7307563432835176 validate: 0.6855126412835968\n",
      "RMSE train: 0.5422508378644549 validate: 0.648314475832916\n",
      "R^2 train: 0.7087392796260451 validate: 0.6663508480545204\n",
      "RMSE train: 0.5418088310699208 validate: 0.6831192985917987\n",
      "R^2 train: 0.7201969619786517 validate: 0.6696533006297367\n",
      "RMSE train: 0.5488622254336096 validate: 0.656033752779706\n",
      "R^2 train: 0.7049563467990447 validate: 0.6573141486137721\n",
      "RMSE train: 0.5657066233631747 validate: 0.6568994260578432\n",
      "R^2 train: 0.7096937602860316 validate: 0.719519356642059\n",
      "RMSE train: 0.5561840929140707 validate: 0.6225627106175154\n",
      "R^2 train: 0.7275077604715082 validate: 0.7632152942695221\n",
      "RMSE train: 0.5504926642432396 validate: 0.564252481141591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train: 0.7236694009981117 validate: 0.7081886271242064\n",
      "RMSE train: 0.5641663822875815 validate: 0.6093141574318616\n",
      "R^2 train: 0.7522279884900539 validate: 0.7396492718532008\n",
      "RMSE train: 0.5206198509117382 validate: 0.607255305667909\n",
      "R^2 train: 0.7363788847627869 validate: 0.7286073614735387\n",
      "RMSE train: 0.5478032272694352 validate: 0.5899962676463635\n",
      "R^2 train: 0.7403364215631025 validate: 0.7248143041367712\n",
      "RMSE train: 0.5437211237288345 validate: 0.603255725193166\n",
      "R^2 train: 0.7267581984911776 validate: 0.6213592184893497\n",
      "RMSE train: 0.5095300824509632 validate: 0.6958006527549223\n",
      "R^2 train: 0.7131128816209799 validate: 0.7333919249577205\n",
      "RMSE train: 0.5569331688689502 validate: 0.622317206764639\n",
      "R^2 train: 0.7234662695094303 validate: 0.6889780838197159\n",
      "RMSE train: 0.5387320957173792 validate: 0.6473475338224243\n",
      "R^2 train: 0.7277249787618415 validate: 0.6945812620000943\n",
      "RMSE train: 0.5396187165587931 validate: 0.6298938838961589\n",
      "R^2 train: 0.7321000779318849 validate: 0.6925245415064523\n",
      "RMSE train: 0.5321162678237542 validate: 0.622367081450229\n",
      "R^2 train: 0.722172535239876 validate: 0.7429663466306347\n",
      "RMSE train: 0.5575090293409669 validate: 0.5911277790645549\n",
      "R^2 train: 0.6963937060975167 validate: 0.6869257170506948\n",
      "RMSE train: 0.5584918109204638 validate: 0.6459499120604001\n",
      "R^2 train: 0.7348904806182397 validate: 0.7369935492717953\n",
      "RMSE train: 0.5373804968681226 validate: 0.5922544114450555\n",
      "R^2 train: 0.7285047615915767 validate: 0.6732392382475202\n",
      "RMSE train: 0.5330152075657439 validate: 0.6747761346321648\n",
      "R^2 train: 0.7346282973914051 validate: 0.6973437147707882\n",
      "RMSE train: 0.5440248605853748 validate: 0.6402881248306489\n",
      "R^2 train: 0.7229049037945892 validate: 0.6258903623580081\n",
      "RMSE train: 0.5561270502918233 validate: 0.6865661738023493\n",
      "R^2 train: 0.7168988870165038 validate: 0.7291498095030851\n",
      "RMSE train: 0.5582504284451758 validate: 0.6041578560767711\n",
      "R^2 train: 0.7287503845521384 validate: 0.7021007426129067\n",
      "RMSE train: 0.5508220468116024 validate: 0.6430510661758282\n",
      "R^2 train: 0.7319600204648959 validate: 0.7868203515417137\n",
      "RMSE train: 0.5200374976264758 validate: 0.5548835998765891\n",
      "R^2 train: 0.7417594231201647 validate: 0.7450830508022794\n",
      "RMSE train: 0.5340849741877143 validate: 0.5735479835700943\n",
      "R^2 train: 0.7344240341221228 validate: 0.7295432006614154\n",
      "RMSE train: 0.5398165363347689 validate: 0.6184842454111074\n"
     ]
    }
   ],
   "source": [
    "reg, (X, y), (X_val, y_val), (coefs, r_squared, rmse) = predict(df, all_features, target, \n",
    "                                                                model=LassoCV(cv=5, tol=0.4), log_target=True, n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(len(all_features)):\n",
    "    print(all_features[j])\n",
    "    plt.hist(np.transpose(coefs)[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_coef = []\n",
    "for j in range(len(all_features)):\n",
    "    average_coef.append(mean(np.transpose(coefs)[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = print_top_influencers(test_df, all_features, reg, coefs=average_coef, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression:\n",
    "    def __init__(self, coef=[]):\n",
    "        self.coef = coef\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = preprocessing.scale(X)\n",
    "        self.y = y\n",
    "        self.coef = []\n",
    "        b = sum(y) / len(y)\n",
    "        a = np.dot(np.dot(np.linalg.pinv(np.dot(self.X.transpose(), self.X)), self.X.transpose()), y)\n",
    "        self.coef = [b] + a.tolist()\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted_y = np.dot(x, self.coef[1:]) + self.coef[0]\n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validate(df2):\n",
    "    df = df2[all_features + [target]].copy()\n",
    "    df = df.astype(np.float32)\n",
    "    print(\"start fill nans\")\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp.fit(df)\n",
    "    df = pd.DataFrame(imp.transform(df))\n",
    "    df.columns = all_features + [target]\n",
    "\n",
    "    print(\"tranform done\")\n",
    "    \n",
    "    train_validate = df[df.recency > step + 1][all_features + [target]]\n",
    "    train = train_validate[train_validate.recency > 11] \n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "    return train, validate\n",
    "    \n",
    "train, validate = get_train_validate(test_df)\n",
    "X = train.iloc[:,:-1]\n",
    "y = np.log1p(train.iloc[:,-1])\n",
    "X_validate = validate.iloc[:,:-1]\n",
    "y_validate = np.log1p(validate.iloc[:,-1])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_validate = scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_features)):\n",
    "    if abs(average_coef[i]) < 0.002:\n",
    "        average_coef[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_average_coef = [mean(y)] + average_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_test = linear_regression(coef=all_average_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = reg_test.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y, reg_test.predict(X)), r2_score(y_validate, reg_test.predict(X_validate)))\n",
    "print(r2_score(np.exp(y), np.exp(reg_test.predict(X))), r2_score(np.exp(y_validate), np.exp(reg_test.predict(X_validate))))\n",
    "print(sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reg_test.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use desicion tree instead of linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df[all_features + ['c5']].astype(np.float32)\n",
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start fill nans\")\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(df)\n",
    "df = pd.DataFrame(imp.transform(df))\n",
    "df.columns = columns\n",
    "\n",
    "print(\"tranform done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate = df[df.recency > step + 1][all_features + [target]] \n",
    "train = train_validate[train_validate.recency > 11] \n",
    "validate = train_validate[train_validate.recency <= 11]\n",
    "\n",
    "train_sample = train.sample(frac=0.7)\n",
    "X = train_sample.iloc[:,:-1]\n",
    "y = np.log(train_sample.iloc[:,-1] + 1)\n",
    "validate_sample = validate.sample(frac=0.7)\n",
    "X_validate = validate_sample.iloc[:,:-1]\n",
    "y_validate = np.log(validate_sample.iloc[:,-1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=6, min_samples_split=40, n_estimators=1000, n_jobs=-1, verbose=4)\n",
    "regr.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"RMSE train: {sqrt(mse(regr.predict(X), y))} validate: {sqrt(mse(regr.predict(X_validate), y_validate))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2: train {regr.score(X, y)} validate: {regr.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(np.exp(y_validate), np.exp(regr.predict(X_validate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regr.predict(X_validate), y_validate, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"R^2: train {r2_score(y, regr.predict(X))} validate: {r2_score(y_validate, regr.predict(X_validate))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_influencers_tree(df, features, reg, n=10):\n",
    "    maxcoef = np.argsort(-np.abs(reg.feature_importances_))\n",
    "    coef = reg.feature_importances_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = print_top_influencers_tree(test_df, all_features, regr, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor, CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Pool(data=X, label=y)\n",
    "eval_dataset = Pool(data=X_validate, label=y_validate)\n",
    "model = CatBoostRegressor(iterations=1400, use_best_model=True, learning_rate=0.02, max_depth=6, loss_function='RMSE')\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          use_best_model=True,\n",
    "          eval_set=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2: {model.score(X, y)} validate: {model.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.predict(X_validate), y_validate, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.get_feature_importance(data=train_dataset,\n",
    "                       prettified=True,\n",
    "                       thread_count=-1,\n",
    "                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(train, validate):\n",
    "    X = train[all_features + ['is_top']].iloc[:,:-1]\n",
    "    y = train[all_features + ['is_top']].iloc[:,-1]\n",
    "    X_validate = validate[all_features + ['is_top']].iloc[:,:-1]\n",
    "    y_validate = validate[all_features + ['is_top']].iloc[:,-1]\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_validate = scaler.transform(X_validate)\n",
    "    return X, y, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best = train.nlargest(columns=['c5'], n = int(part * train.shape[0]))\n",
    "train_min = train_best['c5'].min()\n",
    "validate_best = validate.nlargest(columns=['c5'], n = int(part * validate.shape[0]))\n",
    "val_min = validate_best['c5'].min()\n",
    "train['is_top'] = train['c5'].apply(lambda x: 1 if x > train_min else 0)\n",
    "validate['is_top'] = validate['c5'].apply(lambda x: 1 if x > val_min else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best.groupby(by=['recency'])['c5'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = train[train['is_top'] == 1]\n",
    "not_top = train[train['is_top'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"top size: {top.shape[0]} not top size: {not_top.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_top_downsampled = resample(not_top, replace = False, n_samples = len(top), random_state = 27)\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_top_downsampled, top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_upsampled = resample(top, replace=True, n_samples=len(not_top), random_state=27) \n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_top, top_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in [downsampled, upsampled]:\n",
    "    X, y, X_validate, y_validate = train_validate(dataset, validate)\n",
    "    target = 'is_top'\n",
    "    for weight in ([1, 3, 5]):\n",
    "        print(\"weight =\", weight)\n",
    "        clf = RandomForestClassifier(max_depth=7, n_estimators=1000, class_weight={0: 1, 1: weight})\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print(\"score train: \", clf.score(X, y), \"validate :\", clf.score(X_validate, y_validate))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_validate, clf.predict(X_validate)).ravel()\n",
    "        print(tn, fp, fn, tp)\n",
    "        print(\"recall :\", recall_score(y_validate,  clf.predict(X_validate)))\n",
    "        print(\"precision :\", precision_score(y_validate,  clf.predict(X_validate)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
