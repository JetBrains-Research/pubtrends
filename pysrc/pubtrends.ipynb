{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubtrends\n",
    "\n",
    "Experimental notebook for hypothesis testing and development purposes.\n",
    "\n",
    "**IMPORTANT** \n",
    "Turn on experimental features in config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Define the `SEARCH_QUERY` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Run all cells & see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_QUERY = 'human aging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.db.ss_postgres_loader import SemanticScholarPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED, SORT_MOST_RECENT, cut_authors_list\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "# Avoid info message about compilation flags\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "config.feature_evolution_enabled = True\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)\n",
    "try:\n",
    "    ids = analyzer.search_terms(SEARCH_QUERY, limit=SEARCH_PAPERS, sort=SEARCH_SORT)\n",
    "    analyzer.analyze_papers(ids, SEARCH_QUERY)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.papers_by_year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens, get_topic_word_cloud_data\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)\n",
    "wc, _ = plotter.papers_word_cloud_and_callback(freq_kwds)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show(plotter.top_cited_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.most_cited_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.fastest_growth_per_year_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent keywords timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original keywords frequencies')\n",
    "show(plotter.plot_keywords_frequencies(freq_kwds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single paper citations dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.paper_citations_per_year(analyzer.df, analyzer.df['id'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cluster papers\n",
    "# show(plotter.topics_info_and_word_cloud_and_callback()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.topic_years_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.heatmap_topics_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.plot.plot_preprocessor import PlotPreprocessor\n",
    "\n",
    "similarity_df, topics = PlotPreprocessor.topics_similarity_data(\n",
    "    analyzer.similarity_graph, analyzer.partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.topics_hierarchy_with_keywords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities analysis\n",
    "\n",
    "We hope that the distribution of similarities edge weights illustrates that majority of linked nodes are insignificantly similar in terms of their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bibcoupling_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "cocitations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "citations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "text_similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    bibcoupling_array[i] = np.log1p(data.get('bibcoupling', 0))\n",
    "    cocitations_array[i] = np.log1p(data.get('cocitation', 0))\n",
    "    citations_array[i] = data.get('citation', 0)\n",
    "    text_similarities_array[i] = data.get('text', 0)\n",
    "    similarities_array[i] = PapersAnalyzer.similarity(data)\n",
    "    \n",
    "fig = plt.figure(figsize=(5 * 4, 5))\n",
    "ax = plt.subplot(1, 4, 1)\n",
    "print(f'Bibcoupling, non-zero {np.count_nonzero(bibcoupling_array)} of {len(bibcoupling_array)}')\n",
    "bibcoupling_array = bibcoupling_array[np.nonzero(bibcoupling_array)]\n",
    "print(stats.describe(bibcoupling_array))\n",
    "sns.kdeplot(bibcoupling_array)\n",
    "plt.title('Bibcoupling')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "print(f'Co-citations, non-zero {np.count_nonzero(cocitations_array)} of {len(cocitations_array)}')\n",
    "cocitations_array = cocitations_array[np.nonzero(cocitations_array)]\n",
    "print(stats.describe(cocitations_array))\n",
    "sns.kdeplot(cocitations_array)\n",
    "plt.title('Co-citations')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "print(f'Text similarities, non-zero {np.count_nonzero(text_similarities_array)} of {len(text_similarities_array)}')\n",
    "text_similarities_array = text_similarities_array[np.nonzero(text_similarities_array)]\n",
    "print(stats.describe(text_similarities_array))\n",
    "sns.kdeplot(text_similarities_array)\n",
    "plt.title('Text')\n",
    "# plt.show\n",
    "\n",
    "ax = plt.subplot(1, 4, 4)\n",
    "print(f'Similarities, non-zero {np.count_nonzero(similarities_array)} of {len(similarities_array)}')\n",
    "print(stats.describe(similarities_array))\n",
    "sns.kdeplot(similarities_array)\n",
    "plt.title('Similarity')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Citations, non-zero {np.count_nonzero(citations_array)} of {len(citations_array)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional text similarities exploration\n",
    "We use cutoff = 0.1 as min text similarity, and limit those to 20 max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Analyze similarities between all papers')\n",
    "cos_similarities = cosine_similarity(analyzer.corpus_counts)\n",
    "cos_similarities_array = cos_similarities.reshape(-1)\n",
    "print(stats.describe(cos_similarities_array))\n",
    "print('Q1', np.percentile(cos_similarities_array, 25), \n",
    "      'Q2', np.percentile(cos_similarities_array, 50), \n",
    "      'Q3', np.percentile(cos_similarities_array, 75))\n",
    "\n",
    "fig = plt.figure(figsize=(5 * 2, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(cos_similarities_array)\n",
    "plt.title('Cosine similarities among all papers')\n",
    "# plt.show()\n",
    "\n",
    "print('Analyze similarities between papers with direct citations')\n",
    "pid_indx = {pid: i for i, pid in enumerate(analyzer.df['id'])}\n",
    "cited_cos_similarities = []\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    if data.get('citation', 0) != 0:\n",
    "        cited_cos_similarities.append(cos_similarities[pid_indx[u], pid_indx[v]])\n",
    "\n",
    "print(stats.describe(cited_cos_similarities))\n",
    "print('Q1', np.percentile(cited_cos_similarities, 25), \n",
    "      'Q2', np.percentile(cited_cos_similarities, 50), \n",
    "      'Q3', np.percentile(cited_cos_similarities, 75))\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(cited_cos_similarities)\n",
    "plt.title('Cosine similarity between cited papers')\n",
    "          \n",
    "plt.show()                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = analyzer.similarity_graph\n",
    "degrees = [d for (n, d) in G.degree()]\n",
    "plt.title('Similarity graph degrees')\n",
    "sns.kdeplot(data=degrees)          \n",
    "plt.show()  \n",
    "print('Average degree', sum(degrees) / float(G.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.structure_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarities graph embeddings with various Node2Vec params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.node2vec import node2vec\n",
    "from pysrc.papers.analysis.graph import to_weighted_graph\n",
    "\n",
    "g = to_weighted_graph(analyzer.similarity_graph, weight_func=PapersAnalyzer.similarity)\n",
    "node_ids, weighted_node_embeddings = node2vec(g, walk_length=100, walks_per_node=10, vector_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "weighted_node_embeddings_2d = tsne.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from umap import UMAP \n",
    "\n",
    "# logger.debug('Apply UMAP transformation on node embeddings')\n",
    "# umap = UMAP(n_components=2, random_state=42)\n",
    "# weighted_node_embeddings_2d = umap.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about papers and projected coordinates\n",
    "df = analyzer.df[['id', 'title', 'year', 'type', 'total', 'authors', 'journal', 'comp']].copy()\n",
    "indx = [pid_indx[pid] for pid in node_ids]\n",
    "df['x'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 0])\n",
    "df['y'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "\n",
    "from pysrc.papers.utils import cut_authors_list\n",
    "\n",
    "\n",
    "def plot_embeddings(df, clusters):\n",
    "    cmap = Plotter.factors_colormap(len(set(clusters)))\n",
    "    palette = dict(zip(sorted(set(clusters)), [Plotter.color_to_rgb(cmap(i)).to_hex() \n",
    "                                               for i in range(len(set(clusters)))]))\n",
    "\n",
    "    df['size'] = 5 + df['total'] / df['total'].max() * 20\n",
    "\n",
    "    # Split authors\n",
    "    df['authors'] = df['authors'].apply(lambda authors: cut_authors_list(authors))\n",
    "\n",
    "    ds = ColumnDataSource(df)\n",
    "    # Add clusters coloring\n",
    "    ds.add([palette[c] for c in clusters], 'color')\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'x'\n",
    "    p.yaxis.axis_label = 'y'\n",
    "\n",
    "    p.hover.tooltips = plotter._html_tooltips([\n",
    "        (\"Author(s)\", '@authors'),\n",
    "        (\"Journal\", '@journal'),\n",
    "        (\"Year\", '@year'),\n",
    "        (\"Type\", '@type'),\n",
    "        (\"Cited by\", '@total paper(s) total')])\n",
    "    p.circle(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='comp')\n",
    "    p.legend.visible = False\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot default clusters in embeddings coordinates')\n",
    "plot_embeddings(df, analyzer.df['comp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.topics import cluster_and_sort\n",
    "\n",
    "clusters, children = cluster_and_sort(weighted_node_embeddings, 10, 20)\n",
    "\n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'cluster': clusters, \n",
    "                  'size': np.ones(len(clusters))}).groupby(['cluster']).sum().astype(int).reset_index()    \n",
    "sns.barplot(data=t, x='cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "\n",
    "print('Computing clusters keywords')\n",
    "clusters_pids = pd.DataFrame(dict(id=node_ids, comp=clusters)).groupby('comp')['id'].apply(list).to_dict()\n",
    "\n",
    "clusters_description = get_topics_description(\n",
    "    analyzer.df, clusters_pids,\n",
    "    analyzer.corpus_terms, analyzer.corpus_counts,\n",
    "    query=analyzer.query,\n",
    "    n_words=analyzer.TOPIC_DESCRIPTION_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = [(comp, ','.join([f'{t}:{v:.3f}' for t, v in vs[:20]]))\n",
    "        for comp, vs in clusters_description.items()]\n",
    "kwd_df = pd.DataFrame(kwds, columns=['comp', 'kwd'])\n",
    "display(kwd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comp'] = pd.Series(index=indx, data=clusters, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot clusters in embeddings coordinates')\n",
    "plot_embeddings(df, df['comp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_partition = dict(zip(df['id'], df['comp']))\n",
    "\n",
    "similarity_df, topics = PlotPreprocessor.topics_similarity_data(\n",
    "    analyzer.similarity_graph, clusters_partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(plotter.heatmap_topics_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "\n",
    "def structure_graph(g, df):\n",
    "    nodes = df['id']\n",
    "    graph = GraphRenderer()\n",
    "    comps = df['comp']\n",
    "    cmap = Plotter.factors_colormap(len(set(comps)))\n",
    "    palette = dict(zip(sorted(set(comps)), [Plotter.color_to_rgb(cmap(i)).to_hex()\n",
    "                                            for i in range(len(set(comps)))]))\n",
    "\n",
    "    graph.node_renderer.data_source.add(df['id'], 'index')\n",
    "    graph.node_renderer.data_source.data['id'] = df['id']\n",
    "    graph.node_renderer.data_source.data['title'] = df['title']\n",
    "    graph.node_renderer.data_source.data['authors'] = df['authors']\n",
    "    graph.node_renderer.data_source.data['journal'] = df['journal']\n",
    "    graph.node_renderer.data_source.data['year'] = df['year']\n",
    "    graph.node_renderer.data_source.data['cited'] = df['total']\n",
    "    # Limit size\n",
    "    graph.node_renderer.data_source.data['size'] = df['total'] * 20 / df['total'].max() + 5\n",
    "    graph.node_renderer.data_source.data['topic'] = [c + 1 for c in comps]\n",
    "    graph.node_renderer.data_source.data['color'] = [palette[c] for c in comps]\n",
    "\n",
    "    graph.edge_renderer.data_source.data = dict(start=[u for u, _ in g.edges],\n",
    "                                                end=[v for _, v in g.edges])\n",
    "\n",
    "    # start of layout code\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(plot_width=600,\n",
    "               plot_height=600,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange), \n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),\n",
    "               tools=\"pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    tooltip = \"\"\"\n",
    "    <div style=\"max-width: 500px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Author(s)</span>\n",
    "            <span style=\"font-size: 10px;\">@authors</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Journal</span>\n",
    "            <span style=\"font-size: 10px;\">@journal</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Cited</span>\n",
    "            <span style=\"font-size: 10px;\">@cited</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Topic</span>\n",
    "            <span style=\"font-size: 10px;\">@topic</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    p.add_tools(HoverTool(tooltips=tooltip))\n",
    "\n",
    "    graph_layout = dict(zip(nodes, zip(x, y)))\n",
    "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "    graph.node_renderer.glyph = Circle(size='size', fill_color='color')\n",
    "    graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "    graph.edge_renderer.glyph = MultiLine(line_color='grey', line_alpha=0.1, line_width=1)\n",
    "    graph.edge_renderer.hover_glyph = MultiLine(line_color='blue', line_alpha=1.0, line_width=2)\n",
    "\n",
    "    graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "    p.renderers.append(graph)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.graph import local_sparse\n",
    "\n",
    "print('Visualize structure graph using projected coordinates')\n",
    "show(structure_graph(local_sparse(analyzer.similarity_graph,  0.5), df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_authors_citations_and_papers(df):\n",
    "    logger.debug('Compute author citations')\n",
    "    author_citations = {}\n",
    "    for i, row in df[['authors', 'total']].iterrows():\n",
    "        authors = row['authors'].split(', ')\n",
    "        #     authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "        for a in authors:\n",
    "            author_citations[a] = author_citations.get(a, 0) + row['total']\n",
    "\n",
    "    logger.debug('Compute number of papers per author')\n",
    "    author_papers = {}\n",
    "    for i, row in df[['title', 'authors']].iterrows():\n",
    "        authors = row['authors'].split(', ')\n",
    "        #     authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "        for a in authors:\n",
    "            author_papers[a] = author_papers.get(a, 0) + 1\n",
    "\n",
    "    return author_citations, author_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pysrc.papers.analysis.metadata import popular_authors, popular_journals\n",
    "\n",
    "logging.info(\"Analyzing groups of similar authors\")\n",
    "authors_citations, authors_papers = compute_authors_citations_and_papers(analyzer.df)\n",
    "authors_productivity = {a: np.log1p(authors_citations.get(a, 1)) * p for a, p in authors_papers.items()}\n",
    "min_threshold = np.percentile(list(authors_productivity.values()), 95)\n",
    "min_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_authors_similarity_graph(df,\n",
    "                                   cocit_grouped_df, bibcoupling_df, citations_graph, texts_similarity,\n",
    "                                   first_last_only=True, check_author_func=lambda a: True):\n",
    "    logger.debug('Processing papers')\n",
    "    result = nx.Graph()\n",
    "    for _, row in df[['authors']].iterrows():\n",
    "        authors = row[0].split(', ')\n",
    "        if first_last_only:\n",
    "            authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                a1 = authors[i]\n",
    "                a2 = authors[j]\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'authorship', 1)\n",
    "\n",
    "    logger.debug('Processing co-citations')\n",
    "    for el in cocit_grouped_df[['cited_1', 'cited_2', 'total']].values:\n",
    "        start, end, cocitation = str(el[0]), str(el[1]), float(el[2])\n",
    "        authors1 = df.loc[df['id'] == start]['authors'].values[0].split(', ')\n",
    "        authors2 = df.loc[df['id'] == end]['authors'].values[0].split(', ')\n",
    "        if first_last_only:\n",
    "            authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "            authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'cocitation', cocitation)\n",
    "\n",
    "    logger.debug('Bibliographic coupling')\n",
    "    if len(bibcoupling_df) > 0:\n",
    "        for el in bibcoupling_df[['citing_1', 'citing_2', 'total']].values:\n",
    "            start, end, bibcoupling = str(el[0]), str(el[1]), float(el[2])\n",
    "            authors1 = df.loc[df['id'] == start]['authors'].values[0].split(', ')\n",
    "            authors2 = df.loc[df['id'] == end]['authors'].values[0].split(', ')\n",
    "            if first_last_only:\n",
    "                authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "                authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]\n",
    "            for a1, a2 in itertools.product(authors1, authors2):\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'bibcoupling', bibcoupling)\n",
    "\n",
    "    logger.debug('Text similarity')\n",
    "    pids = list(df['id'])\n",
    "    if len(df) >= 2:\n",
    "        for i, pid1 in enumerate(df['id']):\n",
    "            similarity_queue = texts_similarity[i]\n",
    "            while not similarity_queue.empty():\n",
    "                similarity, j = similarity_queue.get()\n",
    "                pid2 = pids[j]\n",
    "                authors1 = df.loc[df['id'] == pid1]['authors'].values[0].split(', ')\n",
    "                authors2 = df.loc[df['id'] == pid2]['authors'].values[0].split(', ')\n",
    "                if first_last_only:\n",
    "                    authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "                    authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]\n",
    "                for a1, a2 in itertools.product(authors1, authors2):\n",
    "                    if check_author_func(a1) and check_author_func(a2):\n",
    "                        update_edge(result, a1, a2, 'text', similarity)\n",
    "\n",
    "    logger.debug('Citations')\n",
    "    for u, v in citations_graph.edges:\n",
    "        authors1 = df.loc[df['id'] == u]['authors'].values[0].split(', ')\n",
    "        authors2 = df.loc[df['id'] == v]['authors'].values[0].split(', ')\n",
    "        if first_last_only:\n",
    "            authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "            authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'citation', 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_edge(graph, a1, a2, name, value):\n",
    "    if a1 == a2:\n",
    "        return\n",
    "    if a1 > a2:\n",
    "        a1, a2 = a2, a1\n",
    "    if not graph.has_edge(a1, a2):\n",
    "        graph.add_edge(a1, a2)\n",
    "    edge = graph[a1][a2]\n",
    "    edge[name] = edge.get(name, 0) + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "import itertools\n",
    "\n",
    "logger = logging.getLogger('Test')\n",
    "\n",
    "authors_similarity_graph = build_authors_similarity_graph(\n",
    "    analyzer.df, analyzer.cocit_grouped_df,\n",
    "    analyzer.bibliographic_coupling_df,\n",
    "    analyzer.citations_graph,\n",
    "    analyzer.texts_similarity,\n",
    "    check_author_func=lambda a: authors_productivity[a] >= min_threshold\n",
    ")\n",
    "\n",
    "# authors_similarity_graph = analyzer.authors_similarity_graph\n",
    "logging.info(f'Built authors graph - '\n",
    "             f'{len(authors_similarity_graph.nodes())} nodes and {len(authors_similarity_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Compute aggregated similarity using co-authorship')\n",
    "for _, _, d in authors_similarity_graph.edges(data=True):\n",
    "    d['similarity'] = 100 * d.get('authorship', 0) + PapersAnalyzer.similarity(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec embeddings for authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = to_weighted_graph(authors_similarity_graph, weight_func=lambda d: d['similarity'])\n",
    "authors_node_ids, authors_weighted_node_embeddings = node2vec(ga, walk_length=50, walks_per_node=10, vector_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "authors_tsne = TSNE(n_components=2, random_state=42)\n",
    "authors_weighted_node_embeddings_2d = tsne.fit_transform(authors_weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about authors and projected coordinates\n",
    "authors_df = pd.DataFrame(dict(author=authors_node_ids, \n",
    "                               d1=authors_weighted_node_embeddings_2d[:, 0],\n",
    "                               d2=authors_weighted_node_embeddings_2d[:, 1]))\n",
    "authors_df['cited'] = [authors_citations[a] for a in authors_df['author']]\n",
    "authors_df['papers'] = [authors_papers[a] for a in authors_df['author']]\n",
    "authors_df['size'] = [1 + authors_productivity[a] for a in authors_df['author']]\n",
    "# Limit max size\n",
    "authors_df['size'] = authors_df['size'] * 20 / authors_df['size'].max() + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ColumnDataSource(authors_df)\n",
    "p = figure(plot_width=600, plot_height=600,\n",
    "           tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\", toolbar_location=\"right\",\n",
    "           tooltips=[(\"Author\", '@author'),(\"Papers\", '@papers'), (\"Cited\", '@cited')])\n",
    "p.sizing_mode = 'stretch_width'\n",
    "p.xaxis.axis_label = 'd1'\n",
    "p.yaxis.axis_label = 'd2'\n",
    "\n",
    "p.circle(x='d1', y='d2', fill_alpha=0.8, source=ds, size='size',\n",
    "         line_color='black', fill_color='blue', legend_field='author')\n",
    "p.legend.location = None\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_clusters, _ = cluster_and_sort(authors_weighted_node_embeddings_2d, 10, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_authors_clusters(authors_df):\n",
    "    cmap = Plotter.factors_colormap(len(set(authors_df['cluster'])))\n",
    "    palette = dict(zip(sorted(set(authors_df['cluster'])), \n",
    "                       [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(set(authors_df['cluster'])))]))\n",
    "    authors_df['color'] = [palette[c] for c in authors_df['cluster']]\n",
    "\n",
    "    ds = ColumnDataSource(authors_df)\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\", toolbar_location=\"right\",\n",
    "               tooltips=[(\"Author\", '@author'),(\"Papers\", '@papers'), (\"Cited\", '@cited'), ('Cluster', '@cluster')])\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'd1'\n",
    "    p.yaxis.axis_label = 'd2'\n",
    "\n",
    "    p.circle(x='d1', y='d2', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='author')\n",
    "    p.legend.location = None\n",
    "    show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_indx = {a: i for i, a in enumerate(authors_df['author'])}\n",
    "authors_df['cluster'] = pd.Series(index=[authors_indx[a] for a in authors_node_ids], data=authors_clusters)\n",
    "plot_authors_clusters(authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ego-splitting to compute possible overlapping groups of authors\n",
    "Taken from https://github.com/benedekrozemberczki/EgoSplitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EgoNetSplitter(object):\n",
    "    \"\"\"An implementation of `\"Ego-Splitting\" see:\n",
    "    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n",
    "    From the KDD '17 paper \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\".\n",
    "    The tool first creates the egonets of nodes.\n",
    "    A persona-graph is created which is clustered by the Louvain method.\n",
    "    The resulting overlapping cluster memberships are stored as a dictionary.\n",
    "    Args:\n",
    "        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution=1.0):\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def _create_egonet(self, node):\n",
    "        \"\"\"\n",
    "        Creating an ego net, extracting personas and partitioning it.\n",
    "\n",
    "        Args:\n",
    "            node: Node ID for egonet (ego node).\n",
    "        \"\"\"\n",
    "        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n",
    "        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n",
    "        new_mapping = {}\n",
    "        personalities = []\n",
    "        for k, v in components.items():\n",
    "            personalities.append(self.index)\n",
    "            for other_node in v:\n",
    "                new_mapping[other_node] = self.index\n",
    "            self.index = self.index+1\n",
    "        self.components[node] = new_mapping\n",
    "        self.personalities[node] = personalities\n",
    "\n",
    "    def _create_egonets(self):\n",
    "        \"\"\"\n",
    "        Creating an egonet for each node.\n",
    "        \"\"\"\n",
    "        self.components = {}\n",
    "        self.personalities = {}\n",
    "        self.index = 0\n",
    "        print(\"Creating egonets.\")\n",
    "        for node in tqdm(self.graph.nodes()):\n",
    "            self._create_egonet(node)\n",
    "\n",
    "    def _map_personalities(self):\n",
    "        \"\"\"\n",
    "        Mapping the personas to new nodes.\n",
    "        \"\"\"\n",
    "        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n",
    "\n",
    "    def _get_new_edge_ids(self, edge):\n",
    "        \"\"\"\n",
    "        Getting the new edge identifiers.\n",
    "        Args:\n",
    "            edge: Edge being mapped to the new identifiers.\n",
    "        \"\"\"\n",
    "        return (self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]])\n",
    "\n",
    "    def _create_persona_graph(self):\n",
    "        \"\"\"\n",
    "        Create a persona graph using the egonet components.\n",
    "        \"\"\"\n",
    "        print(\"Creating the persona graph.\")\n",
    "        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n",
    "        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n",
    "\n",
    "    def _create_partitions(self):\n",
    "        \"\"\"\n",
    "        Creating a non-overlapping clustering of nodes in the persona graph.\n",
    "        \"\"\"\n",
    "        print(\"Clustering the persona graph.\")\n",
    "        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n",
    "        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n",
    "        for node, membership in self.partitions.items():\n",
    "            self.overlapping_partitions[self.personality_map[node]].append(membership)\n",
    "\n",
    "    def fit(self, graph):\n",
    "        \"\"\"\n",
    "        Fitting an Ego-Splitter clustering model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be clustered.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self._create_egonets()\n",
    "        self._map_personalities()\n",
    "        self._create_persona_graph()\n",
    "        self._create_partitions()\n",
    "\n",
    "    def get_memberships(self):\n",
    "        r\"\"\"Getting the cluster membership of nodes.\n",
    "        Return types:\n",
    "            * **memberships** *(dictionary of lists)* - Cluster memberships.\n",
    "        \"\"\"\n",
    "        return self.overlapping_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = EgoNetSplitter(5)\n",
    "splitter.fit(authors_similarity_graph)\n",
    "splitter.overlapping_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters in papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORS_PER_COMP = 20\n",
    "group_authors = {}\n",
    "for group in sorted(set(authors_clusters)):\n",
    "    authors = list(authors_df.loc[authors_df['cluster'] == group]['author'])\n",
    "    authors.sort(key=lambda a: authors_productivity[a], reverse=True)\n",
    "    top = authors[:AUTHORS_PER_COMP]\n",
    "    group_authors[group] = \", \".join(top)\n",
    "    print(f'#{group} ({len(authors)}) {\", \".join(top)}' + (', ...' if len(authors) > AUTHORS_PER_COMP else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_sizes = Counter(authors_clusters)\n",
    "authors_clusters_map=dict(zip(authors_df['author'], authors_df['cluster']))\n",
    "paper_groups = np.zeros(shape=(len(analyzer.df), len(set(authors_clusters))))\n",
    "for i, row in analyzer.df[['authors']].iterrows():\n",
    "    for a in row[0].split(', '):\n",
    "        if a in authors_clusters_map:\n",
    "            group = authors_clusters_map[a]\n",
    "            paper_groups[i, group] += 1 / part_sizes[group]\n",
    "groups = np.argmax(paper_groups, axis=1)\n",
    "papers_assigned = paper_groups.sum(axis=1) > 0\n",
    "groups_partition = {pid: groups[i] for i, pid in enumerate(analyzer.df['id']) if papers_assigned[i]}\n",
    "\n",
    "groups_part_sizes = {c: sum([groups_partition[node] == c for node in groups_partition.keys()]) \n",
    "                     for c in set(groups_partition.values())}\n",
    "logging.info(f'Components: {groups_part_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "\n",
    "groups_pids = pd.DataFrame(groups_partition.items(), columns=['id', 'comp']). \\\n",
    "                groupby('comp')['id'].apply(list).to_dict()\n",
    "groups_description = get_topics_description(\n",
    "    analyzer.df.iloc[np.flatnonzero(papers_assigned), :], groups_pids,\n",
    "    analyzer.corpus_terms, analyzer.corpus_counts[np.flatnonzero(papers_assigned), :],\n",
    "    query=analyzer.query,\n",
    "    n_words=analyzer.TOPIC_DESCRIPTION_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.DataFrame(columns=['group', 'authors', 'papers', 'keywords'], dtype=object)\n",
    "for g, pids in groups_pids.items():\n",
    "    if g in group_authors and g in groups_description:\n",
    "        groups_df.loc[len(groups_df)] = (g, group_authors[g], len(pids), \n",
    "                                         ', '.join(v[0] for v in groups_description[g][:10]))\n",
    "\n",
    "display(groups_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evolution_data, keywords_data = plotter.topic_evolution()\n",
    "show(evolution_data)\n",
    "print(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(analyzer.citations_graph, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(analyzer.citations_graph, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in analyzer.citations_graph:\n",
    "    for u in analyzer.citations_graph[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['id'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "titles = []\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    sel = analyzer.df[analyzer.df['id'] == node]\n",
    "    year = sel['year'].values[0]\n",
    "    \n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    \n",
    "    titles.append(sel['title'].values[0])\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PMID</span>\n",
    "            <span style=\"font-size: 10px;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PageRank</span>\n",
    "            <span style=\"font-size: 10px;\">@pagerank</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['title'] = titles\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "# graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['id'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['id'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "# graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "# graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.min_border_left = 75\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Papers by PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pmid, pagerank in sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)[:10]:\n",
    "    print(f\"{(100*pagerank):.2f} {analyzer.df[analyzer.df['id'] == pmid]['title'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank and citation ranking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "analyzer.df['citation_rank'] = analyzer.df['total'].rank(method='first', ascending=False)\n",
    "pagerank_rank = sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)\n",
    "\n",
    "r = np.zeros((len(pagerank_rank), 2))\n",
    "for i, (pmid, pr) in enumerate(pagerank_rank):\n",
    "    sel = analyzer.df[analyzer.df['id'] == pmid]\n",
    "    if len(sel) > 0:\n",
    "        r[i, 0] = i\n",
    "        r[i, 1] = int(sel['citation_rank'].values[0])\n",
    "        \n",
    "TOP_X = [5, 10, 30, 50, 100]\n",
    "for x in TOP_X:\n",
    "    rho, _ = spearmanr(r[:x, 0], r[:x, 1])\n",
    "    print(f'Spearman correlation coefficient for top {x}: {rho}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
