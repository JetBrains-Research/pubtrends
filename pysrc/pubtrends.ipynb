{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubtrends\n",
    "\n",
    "This Jupyter Notebook can be used to perform basic publication analysis.\n",
    "\n",
    "**IMPORTANT** \n",
    "Turn on experimental features in config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Define the `SEARCH_QUERY` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Run all cells & see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T11:13:20.757962Z",
     "start_time": "2019-05-20T11:13:19.650470Z"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_QUERY = 'human aging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T11:13:45.558630Z",
     "start_time": "2019-05-20T11:13:20.941831Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.db.ss_postgres_loader import SemanticScholarPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED, SORT_MOST_RECENT, cut_authors_list\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:33.794284Z",
     "start_time": "2019-05-20T11:13:45.591588Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "config.feature_evolution_enabled = True\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)\n",
    "try:\n",
    "    ids = analyzer.search_terms(SEARCH_QUERY, limit=SEARCH_PAPERS, sort=SEARCH_SORT)\n",
    "    analyzer.analyze_papers(ids, SEARCH_QUERY)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:35.056256Z",
     "start_time": "2019-05-20T12:00:34.356360Z"
    }
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.papers_by_year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc, _ = plotter.papers_word_cloud_and_callback()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.top_cited_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.most_cited_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.fastest_growth_per_year_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single paper citations dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.paper_citations_per_year(analyzer.df, analyzer.df['id'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics a.k.a. Clusters in the Co-citation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.topic_years_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:45.632292Z",
     "start_time": "2019-05-20T12:00:35.064248Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(plotter.heatmap_topics_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:48.074797Z",
     "start_time": "2019-05-20T12:00:45.760212Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(plotter.topics_hierarchy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:26.334327Z",
     "start_time": "2019-05-20T12:00:48.085780Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First cluster papers\n",
    "show(plotter.topics_info_and_word_cloud_and_callback()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent keywords timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import vectorize_corpus, get_frequent_tokens\n",
    "\n",
    "corpus_terms, corpus_counts = analyzer.corpus_terms, analyzer.corpus_counts\n",
    "# corpus_terms, corpus_counts = vectorize_corpus(\n",
    "#     analyzer.pub_df,\n",
    "#     max_features=PapersAnalyzer.VECTOR_WORDS,\n",
    "#     min_df=0.4,\n",
    "#     max_df=0.9\n",
    "# )\n",
    "\n",
    "TERMS = 10\n",
    "\n",
    "logging.info('Computing frequent terms')\n",
    "ftkwds = get_frequent_tokens(analyzer.df, query=SEARCH_QUERY)\n",
    "freq_terms = [t for t, _ in list(ftkwds.items())[:TERMS]]\n",
    "freq_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "logging.info('Grouping papers by year')\n",
    "t = analyzer.df[['year']].copy()\n",
    "t['i'] = range(len(t))\n",
    "papers_by_year = t[['year', 'i']].groupby('year')['i'].apply(list).to_dict()\n",
    "\n",
    "\n",
    "logging.info('Collecting numbers of papers with term per year')\n",
    "binary_counts = corpus_counts.copy()\n",
    "binary_counts[binary_counts.nonzero()] = 1\n",
    "numbers_per_year = np.zeros(shape=(len(papers_by_year), len(corpus_terms)))\n",
    "for i, (year, iss) in enumerate(papers_by_year.items()):\n",
    "    numbers_per_year[i, :] = binary_counts[iss].sum(axis=0)[0, :] # * 100 / len(iss)\n",
    "    \n",
    "\n",
    "logging.info('Collect top terms with maximum sum of numbers over years')\n",
    "# top_term_idxs = set([])\n",
    "# summary_numbers = numbers_per_year.mean(axis=0)\n",
    "# idxs = np.argsort(summary_numbers)\n",
    "# for i in range(len(idxs) - TERMS, len(idxs)):\n",
    "#     top_term_idxs.add(idxs[i])    \n",
    "top_term_idxs = [corpus_terms.index(t) for t in freq_terms]  \n",
    "\n",
    "logging.info('Collecting dataframe with numbers for terms')\n",
    "years = [year for year, _ in papers_by_year.items()]\n",
    "term_dfs = []\n",
    "for idx in top_term_idxs:\n",
    "    term = corpus_terms[idx]\n",
    "    term_df = pd.DataFrame(data=numbers_per_year[:, idx].astype(int), columns=['number'])\n",
    "    term_df['term'] = term\n",
    "    term_df['year'] = years\n",
    "    term_dfs.append(term_df)\n",
    "terms_df = pd.concat(term_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "# display(terms_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "from pysrc.papers.plot.plotter import Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the value dimensions\n",
    "max_numbers = terms_df['number'].max()\n",
    "vdim = hv.Dimension('number', range=(-10, max_numbers + 10))\n",
    "    \n",
    "# Define the dataset\n",
    "ds = hv.Dataset(terms_df, vdims=vdim)\n",
    "curves = ds.to(hv.Curve, 'year', groupby='term').overlay().redim(year=dict(range=(min(years)-1, max(years) + 10)))\n",
    "\n",
    "\n",
    "# Define a function to get the text annotations\n",
    "max_year = ds['year'].max()\n",
    "label_df = terms_df[terms_df.year==max_year].copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Update percentages for better labels representation\n",
    "pgroups = label_df.groupby('number')['term'].apply(list).to_dict()\n",
    "delta = max_numbers / 10 # 3 for 0-100 looks fine, using extrapolation here\n",
    "deltas = {}\n",
    "for i, row in label_df.iterrows():\n",
    "    papers, term, year = row\n",
    "    if papers in deltas:\n",
    "        deltas[papers] += delta\n",
    "    else:\n",
    "        deltas[papers] = -delta * (len(pgroups[papers]) - 1) / 2\n",
    "    label_df.loc[i, 'number'] = papers + deltas[papers]\n",
    "label_df.sort_values(by='term', inplace=True)\n",
    "labels = hv.Labels(label_df, ['year', 'number'], 'term')\n",
    "\n",
    "\n",
    "overlay = (curves * labels).relabel('Number of papers, containing most frequent keywords')\n",
    "\n",
    "cmap = Plotter.factors_colormap(len(label_df))\n",
    "palette = [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(label_df))]\n",
    "overlay.opts(\n",
    "    opts.Curve(show_frame=False, labelled=[], tools=['hover'],\n",
    "               height=600, width=900, show_legend=False, xticks=list(reversed(range(max(years), min(years), -5))),\n",
    "               color=hv.Cycle(values=palette), alpha=0.3, line_width=2, show_grid=True),\n",
    "    opts.Labels(text_color='term', cmap=palette, text_align='left'),\n",
    "    opts.NdOverlay(batched=False, \n",
    "                   gridstyle={'grid_line_dash': [6, 4], 'grid_line_width': 1, 'grid_bounds': (0, 100)})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "\n",
    "def update_edge(graph, a1, a2, name, value):\n",
    "    if a1 == a2:\n",
    "        return\n",
    "    if a1 > a2:\n",
    "        a1, a2 = a2, a1\n",
    "    if not graph.has_edge(a1, a2):\n",
    "        graph.add_edge(a1, a2)\n",
    "    edge = graph[a1][a2]\n",
    "    edge[name] = edge.get(name, 0) + value\n",
    "\n",
    "\n",
    "def build_authors_graph(df, texts_similarity, citations_graph, cocit_grouped_df, bibliographic_coupling_df):\n",
    "    result = nx.Graph()\n",
    "    # NOTE: we use nodes id as String to avoid problems str keys in jsonify\n",
    "    # during graph visualization\n",
    "\n",
    "    logging.info('Processing papers')\n",
    "    for i, row in df[['authors']].iterrows():\n",
    "        authors = row[0].split(', ')\n",
    "#         authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                update_edge(result, authors[i], authors[j], 'authorship', 1)\n",
    "\n",
    "    logging.info('Processing co-citations')\n",
    "    for el in cocit_grouped_df[['cited_1', 'cited_2', 'total']].values:\n",
    "        start, end, cocitation = str(el[0]), str(el[1]), float(el[2])\n",
    "        authors1 = df.loc[df['id'] == start]['authors'].values[0].split(', ')\n",
    "        authors2 = df.loc[df['id'] == end]['authors'].values[0].split(', ')\n",
    "#         authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "#         authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]        \n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            update_edge(result, a1, a2, 'cocitation', cocitation)\n",
    "\n",
    "    logging.info('Bibliographic coupling')\n",
    "    if len(bibliographic_coupling_df) > 0:\n",
    "        for el in bibliographic_coupling_df[['citing_1', 'citing_2', 'total']].values:\n",
    "            start, end, bibcoupling = str(el[0]), str(el[1]), float(el[2])\n",
    "            authors1 = df.loc[df['id'] == start]['authors'].values[0].split(', ')\n",
    "            authors2 = df.loc[df['id'] == end]['authors'].values[0].split(', ')\n",
    "#             authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "#             authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]        \n",
    "            for a1, a2 in itertools.product(authors1, authors2):\n",
    "                update_edge(result, a1, a2, 'bibcoupling', bibcoupling)\n",
    "\n",
    "    logging.info('Text similarity')\n",
    "    pids = list(df['id'])\n",
    "    if len(df) >= 2:\n",
    "        for i, pid1 in enumerate(df['id']):\n",
    "            similarity_queue = texts_similarity[i]\n",
    "            while not similarity_queue.empty():\n",
    "                similarity, j = similarity_queue.get()\n",
    "                pid2 = pids[j]\n",
    "                authors1 = df.loc[df['id'] == pid1]['authors'].values[0].split(', ')\n",
    "                authors2 = df.loc[df['id'] == pid2]['authors'].values[0].split(', ')\n",
    "#                 authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "#                 authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]        \n",
    "                for a1, a2 in itertools.product(authors1, authors2):\n",
    "                    update_edge(result, a1, a2, 'text', similarity)\n",
    "\n",
    "    logging.info('Citations')\n",
    "    for u, v in citations_graph.edges:\n",
    "        authors1 = df.loc[df['id'] == u]['authors'].values[0].split(', ')\n",
    "        authors2 = df.loc[df['id'] == v]['authors'].values[0].split(', ')\n",
    "#         authors1 = authors1 if len(authors1) <= 2 else [authors1[0], authors1[-1]]\n",
    "#         authors2 = authors2 if len(authors2) <= 2 else [authors2[0], authors2[-1]]        \n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            update_edge(result, a1, a2, 'citation', 1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_graph = build_authors_graph(\n",
    "    analyzer.df, analyzer.texts_similarity, analyzer.citations_graph, \n",
    "    analyzer.cocit_grouped_df, analyzer.bibliographic_coupling_df\n",
    ")\n",
    "logging.info(f'Built authors graph - {len(authors_graph.nodes())} nodes and {len(authors_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Compute author citations')\n",
    "author_citations = {}\n",
    "for i, row in analyzer.df[['authors', 'total']].iterrows():\n",
    "    authors = row['authors'].split(', ')\n",
    "#     authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "    for a in authors:\n",
    "        author_citations[a] = author_citations.get(a, 0) + row['total']\n",
    "        \n",
    "logging.info('Compute number of papers per author')\n",
    "author_papers = {}\n",
    "for i, row in analyzer.df[['title', 'authors']].iterrows():\n",
    "    authors = row['authors'].split(', ')\n",
    "#     authors = authors if len(authors) <= 2 else [authors[0], authors[-1]]\n",
    "    for a in authors:\n",
    "        author_papers[a] = author_papers.get(a, []) + [row['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "connected_components = nx.number_connected_components(authors_graph)\n",
    "logging.info(f'Authors graph has {connected_components} connected components')\n",
    "\n",
    "logging.info('Compute aggregated similarity')\n",
    "for _, _, d in authors_graph.edges(data=True):\n",
    "    d['similarity'] = \\\n",
    "        100 * d.get('authorship', 0) + \\\n",
    "        PapersAnalyzer.SIMILARITY_COCITATION * d.get('cocitation', 0) + \\\n",
    "        PapersAnalyzer.SIMILARITY_BIBLIOGRAPHIC_COUPLING * d.get('bibcoupling', 0) + \\\n",
    "        PapersAnalyzer.SIMILARITY_CITATION * d.get('citation', 0) + \\\n",
    "        PapersAnalyzer.SIMILARITY_TEXT_CITATION * d.get('text', 0)\n",
    "\n",
    "logging.info('Graph clustering via Louvain community algorithm')\n",
    "partition_louvain = community.best_partition(\n",
    "    authors_graph, weight='similarity', random_state=42\n",
    ")\n",
    "logging.info(f'Best partition {len(set(partition_louvain.values()))} components')\n",
    "components = set(partition_louvain.values())\n",
    "comp_sizes = {c: sum([partition_louvain[node] == c for node in partition_louvain.keys()]) for c in components}\n",
    "logging.info(f'Components: {comp_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_CITED_AUTHORS_PER_COMP = 10\n",
    "top_cited_authors = set([])\n",
    "for group in sorted(set(partition_louvain.values())):\n",
    "    authors = [a for a in partition_louvain.keys() if partition_louvain[a] == group]\n",
    "    authors.sort(key=lambda a: author_citations[a], reverse=True)\n",
    "    top = authors[:TOP_CITED_AUTHORS_PER_COMP]\n",
    "    top_cited_authors.update(top)\n",
    "    print(f'#{group} {\", \".join(top)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top cited graph\n",
    "filtered_authors_graph = nx.Graph()\n",
    "for (a1, a2, d) in authors_graph.edges(data=True):\n",
    "    # len(author_papers[a1]) > 1 and len(author_papers[a2]) > 1 and \\\n",
    "    if a1 in top_cited_authors and a2 in top_cited_authors:\n",
    "        filtered_authors_graph.add_edge(a1, a2, **d)\n",
    "logging.info(f'Built top authors graph - '\n",
    "             f'{len(filtered_authors_graph.nodes())} nodes and {len(filtered_authors_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "from pysrc.papers.analysis.graph import local_sparse\n",
    "\n",
    "# g = authors_similarity_graph\n",
    "g = local_sparse(filtered_authors_graph, e=0.8)\n",
    "# Layout by nx\n",
    "\n",
    "\n",
    "pos = nx.spring_layout(g)\n",
    "# nx.draw_networkx(g, pos)\n",
    "# labels = nx.get_edge_attributes(g, 'similarity')\n",
    "# nx.draw_networkx_edge_labels(g, pos, edge_labels=labels)\n",
    "\n",
    "nodes = [a for a, _ in pos.items()]\n",
    "graph = GraphRenderer()\n",
    "\n",
    "clusters = [partition_louvain[n] for n in nodes]\n",
    "cmap = Plotter.factors_colormap(len(set(clusters)))\n",
    "palette = dict(zip(set(clusters), [Plotter.color_to_rgb(cmap(i)).to_hex() \n",
    "                                   for i in range(len(set(partition_louvain.values())))]))\n",
    "\n",
    "graph.node_renderer.data_source.add(nodes, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = nodes\n",
    "graph.node_renderer.data_source.data['cited'] = [author_citations[n] for n in nodes]\n",
    "graph.node_renderer.data_source.data['papers'] = [len(author_papers[n]) for n in nodes]\n",
    "graph.node_renderer.data_source.data['titles'] = ['\\n'.join(author_papers[n]) for n in nodes]\n",
    "graph.node_renderer.data_source.data['size'] = [np.log1p(len(author_papers[n])) * np.log1p(author_citations[n]) \n",
    "                                                for n in nodes]\n",
    "graph.node_renderer.data_source.data['cluster'] = clusters\n",
    "graph.node_renderer.data_source.data['color'] = [palette[partition_louvain[n]] for n in nodes]\n",
    "graph.edge_renderer.data_source.data = dict(start=[a for a, _ in g.edges], \n",
    "                                            end=[a for _, a in g.edges])\n",
    "\n",
    "### start of layout code   \n",
    "x = [v[0] for _, v in pos.items()]\n",
    "y = [v[1] for _, v in pos.items()]\n",
    "plot = figure(title=\"Authors plot\",\n",
    "              width=900,\n",
    "              height=800,\n",
    "              x_range=(min(x), max(x)), y_range=(min(y), max(y)),\n",
    "              tools=\"pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Papers</span>\n",
    "            <span style=\"font-size: 10px;\">@papers</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Cited</span>\n",
    "            <span style=\"font-size: 10px;\">@cited</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Cluster</span>\n",
    "            <span style=\"font-size: 10px;\">@cluster</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Titles</span>\n",
    "            <span style=\"font-size: 10px;\">@titles</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "\n",
    "graph_layout = dict(zip(nodes, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='color')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "graph.edge_renderer.glyph = MultiLine(line_color='grey', line_alpha=0.1, line_width=1)\n",
    "graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_alpha=1.0, line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.renderers.append(graph)\n",
    "plot.min_border_left = 75\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:02:52.682050Z",
     "start_time": "2019-05-20T12:02:49.182634Z"
    }
   },
   "outputs": [],
   "source": [
    "evolution_data, keywords_data = plotter.topic_evolution()\n",
    "show(evolution_data)\n",
    "print(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.552985Z",
     "start_time": "2019-05-20T12:15:08.834Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(analyzer.citations_graph, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.078073Z",
     "start_time": "2019-05-20T11:13:20.732Z"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(analyzer.citations_graph, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in analyzer.citations_graph:\n",
    "    for u in analyzer.citations_graph[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.118051Z",
     "start_time": "2019-05-20T11:13:20.740Z"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.145031Z",
     "start_time": "2019-05-20T11:13:20.832Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.179011Z",
     "start_time": "2019-05-20T11:13:25.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['id'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "titles = []\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    sel = analyzer.df[analyzer.df['id'] == node]\n",
    "    year = sel['year'].values[0]\n",
    "    \n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    \n",
    "    titles.append(sel['title'].values[0])\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PMID</span>\n",
    "            <span style=\"font-size: 10px;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PageRank</span>\n",
    "            <span style=\"font-size: 10px;\">@pagerank</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['title'] = titles\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "# graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['id'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['id'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "# graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "# graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.min_border_left = 75\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Papers by PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pmid, pagerank in sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)[:10]:\n",
    "    print(f\"{(100*pagerank):.2f} {analyzer.df[analyzer.df['id'] == pmid]['title'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank and citation ranking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "analyzer.df['citation_rank'] = analyzer.df['total'].rank(method='first', ascending=False)\n",
    "pagerank_rank = sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)\n",
    "\n",
    "r = np.zeros((len(pagerank_rank), 2))\n",
    "for i, (pmid, pr) in enumerate(pagerank_rank):\n",
    "    sel = analyzer.df[analyzer.df['id'] == pmid]\n",
    "    if len(sel) > 0:\n",
    "        r[i, 0] = i\n",
    "        r[i, 1] = int(sel['citation_rank'].values[0])\n",
    "        \n",
    "TOP_X = [5, 10, 30, 50, 100]\n",
    "for x in TOP_X:\n",
    "    rho, _ = spearmanr(r[:x, 0], r[:x, 1])\n",
    "    print(f'Spearman correlation coefficient for top {x}: {rho}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hub nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very slow!\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "adj = np.zeros((analyzer.similarity_graph.number_of_nodes(), analyzer.df['comp'].nunique()))\n",
    "w = np.zeros(adj.shape)\n",
    "\n",
    "for i, v in enumerate(analyzer.similarity_graph.nodes()):\n",
    "    for u in analyzer.similarity_graph[v]:\n",
    "        c = analyzer.df[analyzer.df['id'] == u]['comp'].values[0]\n",
    "        adj[i][c] += 1\n",
    "        w[i][c] += analyzer.similarity_graph[v][u]['similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "size = 10\n",
    "\n",
    "hub_indices = np.argsort(np.sum(adj > 0, axis=1))[-size:]\n",
    "\n",
    "nodes_list = list(analyzer.similarity_graph.nodes)\n",
    "hub_pmids = [nodes_list[idx] for idx in hub_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hub nodes')\n",
    "print(analyzer.df[analyzer.df['id'].isin(hub_pmids)][['id', 'title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
