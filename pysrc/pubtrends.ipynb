{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubtrends\n",
    "\n",
    "Experimental notebook for hypothesis testing and development purposes.\n",
    "\n",
    "**IMPORTANT** \n",
    "Turn on experimental features in config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Define the `SEARCH_QUERY` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Run all cells & see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.utils import SORT_MOST_CITED\n",
    "\n",
    "SEARCH_QUERY = 'Human Aging'\n",
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 100000\n",
    "\n",
    "# File with ids to analyze\n",
    "FILE = '/mnt/stripe/shpynov/pubtrends/pmid-humanaging-set.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.db.ss_postgres_loader import SemanticScholarPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED, SORT_MOST_RECENT, cut_authors_list\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "# Avoid info message about compilation flags\n",
    "import tensorflow as tf\n",
    "# tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "config.feature_evolution_enabled = True\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)\n",
    "try:\n",
    "    if FILE is not None:\n",
    "        with open(FILE) as f:\n",
    "            ids = [l.strip() for l in f.readlines()]\n",
    "    else:\n",
    "        ids = analyzer.search_terms(SEARCH_QUERY, limit=SEARCH_PAPERS, sort=SEARCH_SORT)\n",
    "    analyzer.analyze_papers(ids, SEARCH_QUERY)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.papers_by_year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens, get_topic_word_cloud_data\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)\n",
    "wc, _ = plotter.papers_word_cloud_and_callback(freq_kwds)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show(plotter.top_cited_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.most_cited_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.fastest_growth_per_year_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent keywords timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original keywords frequencies')\n",
    "show(plotter.plot_keywords_frequencies(freq_kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Save frequent tokens to file')\n",
    "output_file(filename=\"frequent.html\", title=\"Frequents topics\")\n",
    "save(plotter.plot_keywords_frequencies(freq_kwds))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single paper citations dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.paper_citations_per_year(analyzer.df, analyzer.df['id'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cluster papers\n",
    "# show(plotter.topics_info_and_word_cloud_and_callback()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(plotter.topic_years_distribution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.topics import compute_similarity_matrix\n",
    "from itertools import product\n",
    "\n",
    "def topics_similarity_data(similarity_graph, partition):\n",
    "    similarity_matrix = compute_similarity_matrix(similarity_graph, PapersAnalyzer.similarity, partition)\n",
    "\n",
    "    # c + 1 is used to start numbering with 1\n",
    "    components = [str(c + 1) for c in sorted(set(partition.values()))]\n",
    "    n_comps = len(components)\n",
    "    similarity_topics_df = pd.DataFrame([\n",
    "        {'comp_x': i, 'comp_y': j, 'similarity': similarity_matrix[i, j]}\n",
    "        for i, j in product(range(n_comps), range(n_comps))\n",
    "    ])\n",
    "    similarity_topics_df['comp_x'] = similarity_topics_df['comp_x'].apply(lambda x: x + 1).astype(str)\n",
    "    similarity_topics_df['comp_y'] = similarity_topics_df['comp_y'].apply(lambda x: x + 1).astype(str)\n",
    "    return similarity_topics_df, components\n",
    "\n",
    "\n",
    "\n",
    "similarity_df, topics = topics_similarity_data(\n",
    "    analyzer.similarity_graph, analyzer.partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.colors import RGB\n",
    "from bokeh.models import LinearColorMapper, PrintfTickFormatter, ColorBar\n",
    "\n",
    "def heatmap_topics_similarity(similarity_df, topics):\n",
    "    logger.debug('Visualizing topics similarity with heatmap')\n",
    "\n",
    "    step = 10\n",
    "    cmap = plt.cm.get_cmap('PuBu', step)\n",
    "    colors = [RGB(*[round(c * 255) for c in cmap(i)[:3]]) for i in range(step)]\n",
    "    mapper = LinearColorMapper(palette=colors,\n",
    "                               low=similarity_df.similarity.min(),\n",
    "                               high=similarity_df.similarity.max())\n",
    "\n",
    "    p = figure(x_range=topics, y_range=topics,\n",
    "               x_axis_location=\"below\", plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\", toolbar_location=\"right\",\n",
    "               tooltips=[('Topic 1', '@comp_x'),\n",
    "                         ('Topic 2', '@comp_y'),\n",
    "                         ('Similarity', '@similarity')])\n",
    "\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.grid.grid_line_color = None\n",
    "    p.axis.axis_line_color = None\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.major_label_text_font_size = \"10pt\"\n",
    "    p.axis.major_label_standoff = 0\n",
    "\n",
    "    p.rect(x=\"comp_x\", y=\"comp_y\", width=1, height=1,\n",
    "           source=similarity_df,\n",
    "           fill_color={'field': 'similarity', 'transform': mapper},\n",
    "           line_color=None)\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper, major_label_text_font_size=\"10pt\",\n",
    "                         formatter=PrintfTickFormatter(format=\"%.2f\"),\n",
    "                         label_standoff=11, border_line_color=None, location=(0, 0))\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    return p\n",
    "\n",
    "\n",
    "show(heatmap_topics_similarity(similarity_df, topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities function analysis\n",
    "\n",
    "We hope that the distribution of similarities edge weights illustrates that majority of linked nodes are insignificantly similar in terms of their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bibcoupling_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "cocitations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "citations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "text_similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    bibcoupling_array[i] = np.log1p(data.get('bibcoupling', 0))\n",
    "    cocitations_array[i] = np.log1p(data.get('cocitation', 0))\n",
    "    citations_array[i] = data.get('citation', 0)\n",
    "    text_similarities_array[i] = data.get('text', 0)\n",
    "    similarities_array[i] = PapersAnalyzer.similarity(data)\n",
    "    \n",
    "fig = plt.figure(figsize=(5 * 4, 5))\n",
    "ax = plt.subplot(1, 4, 1)\n",
    "print(f'Bibcoupling, non-zero {np.count_nonzero(bibcoupling_array)} of {len(bibcoupling_array)}')\n",
    "bibcoupling_array = bibcoupling_array[np.nonzero(bibcoupling_array)]\n",
    "print(stats.describe(bibcoupling_array))\n",
    "sns.kdeplot(bibcoupling_array)\n",
    "plt.title('Bibcoupling')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "print(f'Co-citations, non-zero {np.count_nonzero(cocitations_array)} of {len(cocitations_array)}')\n",
    "cocitations_array = cocitations_array[np.nonzero(cocitations_array)]\n",
    "print(stats.describe(cocitations_array))\n",
    "sns.kdeplot(cocitations_array)\n",
    "plt.title('Co-citations')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "print(f'Text similarities, non-zero {np.count_nonzero(text_similarities_array)} of {len(text_similarities_array)}')\n",
    "text_similarities_array = text_similarities_array[np.nonzero(text_similarities_array)]\n",
    "print(stats.describe(text_similarities_array))\n",
    "sns.kdeplot(text_similarities_array)\n",
    "plt.title('Text')\n",
    "# plt.show\n",
    "\n",
    "ax = plt.subplot(1, 4, 4)\n",
    "print(f'Similarities, non-zero {np.count_nonzero(similarities_array)} of {len(similarities_array)}')\n",
    "print(stats.describe(similarities_array))\n",
    "sns.kdeplot(similarities_array)\n",
    "plt.title('Similarity')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Citations, non-zero {np.count_nonzero(citations_array)} of {len(citations_array)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import build_corpus, tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "logging.info('Analyzing text vectorization')\n",
    "corpus = build_corpus(analyzer.df)\n",
    "vectorizer = CountVectorizer(\n",
    "     max_features=PapersAnalyzer.VECTOR_WORDS,\n",
    "     min_df=0.01,\n",
    "     max_df=0.5,\n",
    "     tokenizer=lambda t: tokenize(t)\n",
    ")\n",
    "counts = vectorizer.fit_transform(corpus)\n",
    "logger.debug(f'Vectorized corpus size {counts.shape}')\n",
    "terms_counts = np.asarray(np.sum(counts, axis=0)).reshape(-1)\n",
    "print(terms_counts.shape)\n",
    "terms_freqs = terms_counts / len(analyzer.df)\n",
    "logger.debug(f'Terms frequencies min={terms_freqs.min()}, max={terms_freqs.max()}, '\n",
    "             f'mean={terms_freqs.mean()}, std={terms_freqs.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print('Max frequent terms:', ', '.join(terms[k] for k in terms_freqs.argsort()[::-1][:20]))\n",
    "print('Min frequent terms:', ', '.join(terms[k] for k in terms_freqs.argsort()[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Analyze similarities between all papers')\n",
    "cos_similarities = cosine_similarity(analyzer.corpus_counts)\n",
    "cos_similarities_array = cos_similarities.reshape(-1)\n",
    "print(stats.describe(cos_similarities_array))\n",
    "print('Q1', np.percentile(cos_similarities_array, 25), \n",
    "      'Q2', np.percentile(cos_similarities_array, 50), \n",
    "      'Q3', np.percentile(cos_similarities_array, 75))\n",
    "\n",
    "fig = plt.figure(figsize=(5 * 2, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(cos_similarities_array)\n",
    "plt.title('Cosine similarities among all papers')\n",
    "# plt.show()\n",
    "\n",
    "print('Analyze similarities between papers with direct citations')\n",
    "pid_indx = {pid: i for i, pid in enumerate(analyzer.df['id'])}\n",
    "cited_cos_similarities = []\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    if data.get('citation', 0) != 0:\n",
    "        cited_cos_similarities.append(cos_similarities[pid_indx[u], pid_indx[v]])\n",
    "\n",
    "print(stats.describe(cited_cos_similarities))\n",
    "print('Q1', np.percentile(cited_cos_similarities, 25), \n",
    "      'Q2', np.percentile(cited_cos_similarities, 50), \n",
    "      'Q3', np.percentile(cited_cos_similarities, 75))\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(cited_cos_similarities)\n",
    "plt.title('Cosine similarity between cited papers')\n",
    "          \n",
    "plt.show()                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = analyzer.similarity_graph\n",
    "degrees = [d for (n, d) in G.degree()]\n",
    "plt.title('Similarity graph degrees')\n",
    "sns.kdeplot(data=degrees)          \n",
    "plt.show()  \n",
    "print('Average degree', sum(degrees) / float(G.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.plot_similarity_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarities graph embeddings with various Node2Vec params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.node2vec import node2vec\n",
    "from pysrc.papers.analysis.graph import local_sparse, to_weighted_graph\n",
    "\n",
    "logger.debug('Preparing node2vec + tsne layout for similarity graph')\n",
    "wsg = to_weighted_graph(analyzer.similarity_graph, weight_func=PapersAnalyzer.similarity)\n",
    "e = 1.0\n",
    "gs = local_sparse(wsg, e)\n",
    "# Limit total number of edges to estimate walk probabilities\n",
    "while e > 0.1 and gs.number_of_edges() / gs.number_of_nodes() > 20:\n",
    "    e -= 0.1\n",
    "    gs = local_sparse(wsg, e)\n",
    "logger.debug(f'Sparse graph for node2vec e={e} nodes={gs.number_of_nodes()} edges={gs.number_of_edges()}')\n",
    "node_ids, weighted_node_embeddings = node2vec(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "weighted_node_embeddings_2d = tsne.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from umap import UMAP \n",
    "\n",
    "# logger.debug('Apply UMAP transformation on node embeddings')\n",
    "# umap = UMAP(n_components=2, random_state=42)\n",
    "# weighted_node_embeddings_2d = umap.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about papers and projected coordinates\n",
    "df = analyzer.df[['id', 'title', 'year', 'type', 'abstract', 'total', 'authors', 'journal', 'comp',\n",
    "                 'keywords', 'mesh']].copy()\n",
    "pid_indx = {pid: i for i, pid in enumerate(df['id'])}\n",
    "indx = [pid_indx[pid] for pid in node_ids]\n",
    "df['x'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 0])\n",
    "df['y'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "\n",
    "from pysrc.papers.utils import cut_authors_list\n",
    "\n",
    "\n",
    "def plot_embeddings(df, clusters):\n",
    "    cmap = Plotter.factors_colormap(len(set(clusters)))\n",
    "    palette = dict(zip(sorted(set(clusters)), [Plotter.color_to_rgb(cmap(i)).to_hex() \n",
    "                                               for i in range(len(set(clusters)))]))\n",
    "\n",
    "    df['size'] = 5 + df['total'] / df['total'].max() * 20\n",
    "\n",
    "    # Split authors\n",
    "    df['authors'] = df['authors'].apply(lambda authors: cut_authors_list(authors))\n",
    "\n",
    "    ds = ColumnDataSource(df)\n",
    "    # Add clusters coloring\n",
    "    ds.add([palette[c] for c in clusters], 'color')\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'x'\n",
    "    p.yaxis.axis_label = 'y'\n",
    "\n",
    "    p.hover.tooltips = plotter._html_tooltips([\n",
    "        (\"Author(s)\", '@authors'),\n",
    "        (\"Journal\", '@journal'),\n",
    "        (\"Year\", '@year'),\n",
    "        (\"Type\", '@type'),\n",
    "        (\"Cited by\", '@total paper(s) total'),\n",
    "        (\"Topic\", '@comp')])\n",
    "    p.circle(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='comp')\n",
    "    p.legend.visible = False\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot default Louvain clusters in embeddings coordinates')\n",
    "plot_embeddings(df, analyzer.df['comp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def cluster_and_sort(x, min_cluster_size, max_clusters):\n",
    "    \"\"\"\n",
    "    :param x: object representations (X x Features)\n",
    "    :param min_cluster_size:\n",
    "    :param max_clusters:\n",
    "    :return: List[cluster], Hierarchical dendrogram of splits.\n",
    "    \"\"\"\n",
    "    logger.debug('Looking for an appropriate number of clusters,'\n",
    "                 f'min_cluster_size={min_cluster_size}, max_clusters={max_clusters}')\n",
    "    r = min(int(x.shape[0] / min_cluster_size), max_clusters) + 1\n",
    "    l = 1\n",
    "\n",
    "    if l >= r - 2:\n",
    "        return [0] * x.shape[0], None\n",
    "\n",
    "    prev_min_size = None\n",
    "    while l < r - 2:\n",
    "        n_clusters = int((l + r) / 2)\n",
    "        logger.debug(f'l = {l}; r = {r}; n_clusters = {n_clusters}')\n",
    "        model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward').fit(x)\n",
    "        clusters_counter = Counter(model.labels_)\n",
    "        assert len(clusters_counter.keys()) == n_clusters, \"Incorrect clusters number\"\n",
    "        min_size = clusters_counter.most_common()[-1][1]\n",
    "        # Track previous_min_size to cope with situation with super distant tiny clusters\n",
    "        if prev_min_size != min_size and min_size < min_cluster_size or n_clusters > max_clusters:\n",
    "            logger.debug(f'prev_min_size({prev_min_size}) != min_size({min_size}) < {min_cluster_size} or '\n",
    "                         f'n_clusters = {n_clusters}  > {max_clusters}')\n",
    "            r = n_clusters + 1\n",
    "        else:\n",
    "            l = n_clusters\n",
    "        prev_min_size = min_size\n",
    "\n",
    "    logger.debug(f'Number of clusters = {n_clusters}')\n",
    "    logger.debug(f'Min cluster size = {prev_min_size}')\n",
    "    logger.debug('Reorder clusters by size descending')\n",
    "    reorder_map = {c: i for i, (c, _) in enumerate(clusters_counter.most_common())}\n",
    "    return [reorder_map[c] for c in model.labels_], model.children_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, dendrogram_children = cluster_and_sort(weighted_node_embeddings, 30, 50)\n",
    "\n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'cluster': clusters, \n",
    "                  'size': np.ones(len(clusters))}).groupby(['cluster']).sum().astype(int).reset_index()    \n",
    "sns.barplot(data=t, x='cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comp'] = pd.Series(index=indx, data=clusters, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_ratio_data(df):\n",
    "    assigned_comps = df[df['comp'] >= 0]\n",
    "    comp_size = dict(assigned_comps.groupby('comp')['id'].count())\n",
    "    total_papers = sum(assigned_comps['comp'] >= 0)\n",
    "    comps = list(comp_size.keys())\n",
    "    ratios = [100 * comp_size[c] / total_papers for c in comps]\n",
    "\n",
    "    # c + 1 is used to start numbering from 1\n",
    "    comps = list(map(str, [c + 1 for c in comps]))\n",
    "    return comps, ratios\n",
    "\n",
    "def plot_components_ratio(df, plot_width=1200, plot_height=800):\n",
    "    comps, ratios = components_ratio_data(df)\n",
    "    n_comps = len(comps)\n",
    "    cmap = Plotter.factors_colormap(n_comps)\n",
    "    colors = [Plotter.color_to_rgb(cmap(i)) for i in range(n_comps)]\n",
    "    source = ColumnDataSource(data=dict(comps=comps, ratios=ratios, colors=colors))\n",
    "\n",
    "    p = figure(plot_width=plot_width, plot_height=plot_height,\n",
    "               toolbar_location=\"right\", tools=\"save\", x_range=comps)\n",
    "    p.vbar(x='comps', top='ratios', width=0.8, fill_alpha=0.5, color='colors', source=source)\n",
    "    p.hover.tooltips = [(\"Topic\", '@comps'), (\"Amount\", '@ratios %')]\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'Topic'\n",
    "    p.yaxis.axis_label = 'Percentage of papers'\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.axis.minor_tick_line_color = None\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    return p\n",
    "\n",
    "show(plot_components_ratio(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Save topics ratios to file')\n",
    "output_file(filename=\"sizes.html\", title=\"Topics sizes\")\n",
    "save(plot_components_ratio(df))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "\n",
    "print('Computing clusters keywords')\n",
    "clusters_pids = pd.DataFrame(dict(id=node_ids, comp=clusters)).groupby('comp')['id'].apply(list).to_dict()\n",
    "\n",
    "clusters_description = get_topics_description(\n",
    "    analyzer.df, clusters_pids,\n",
    "    analyzer.corpus_terms, analyzer.corpus_counts,\n",
    "    query=analyzer.query,\n",
    "    n_words=analyzer.TOPIC_DESCRIPTION_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = [(comp, ','.join([f'{t}:{v:.3f}' for t, v in vs[:20]]))\n",
    "        for comp, vs in clusters_description.items()]\n",
    "kwd_df = pd.DataFrame(kwds, columns=['comp', 'kwd'])\n",
    "display(kwd_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot clusters in embeddings coordinates')\n",
    "plot_embeddings(df, df['comp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_partition = dict(zip(df['id'], df['comp']))\n",
    "\n",
    "similarity_df, topics = topics_similarity_data(\n",
    "    analyzer.similarity_graph, clusters_partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(heatmap_topics_similarity(similarity_df, topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Save similarity heatmap to file')\n",
    "output_file(filename=\"similarity.html\", title=\"Topics mean similarity\")\n",
    "save(heatmap_topics_similarity(similarity_df, topics))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.plot.plot_preprocessor import PlotPreprocessor\n",
    "\n",
    "max_year, min_year = df['year'].max(), df['year'].min()\n",
    "plot_components, data = PlotPreprocessor.component_size_summary_data(\n",
    "    df, sorted(set(df['comp'])), min_year, max_year\n",
    ")\n",
    "\n",
    "show(Plotter._topics_years_distribution(df, kwd_df, plot_components, data, max_year, min_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Save topics years to file')\n",
    "output_file(filename=\"years.html\", title=\"Topics by years\")\n",
    "save(Plotter._topics_years_distribution(df, kwd_df, plot_components, data, max_year, min_year))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi, sin, cos, fabs, pow\n",
    "from bokeh.colors import RGB\n",
    "from queue import PriorityQueue\n",
    "\n",
    "from pysrc.papers.utils import rgb2hex\n",
    "\n",
    "\n",
    "from more_itertools import unique_everseen\n",
    "\n",
    "def compute_clusters_dendrogram_children(clusters, children):\n",
    "    leaves_map = dict(enumerate(clusters))\n",
    "    nodes_map = {}\n",
    "    clusters_children = []\n",
    "    for i, (u, v) in enumerate(children):\n",
    "        u_cluster = leaves_map[u] if u in leaves_map else nodes_map[u]\n",
    "        v_cluster = leaves_map[v] if v in leaves_map else nodes_map[v]\n",
    "        node = len(leaves_map) + i\n",
    "        if u_cluster is not None and v_cluster is not None:\n",
    "            if u_cluster != v_cluster:\n",
    "                nodes_map[node] = None  # Different clusters\n",
    "                clusters_children.append((u, v, node))\n",
    "            else:\n",
    "                nodes_map[node] = u_cluster\n",
    "        else:\n",
    "            nodes_map[node] = None  # Different clusters\n",
    "            clusters_children.append((u, v, node))\n",
    "\n",
    "    def rwc(v):\n",
    "        if v in leaves_map:\n",
    "            return leaves_map[v]\n",
    "        elif v in nodes_map:\n",
    "            res = nodes_map[v]\n",
    "            return res if res is not None else v\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    # Rename nodes to clusters\n",
    "    result = [(rwc(u), rwc(v), rwc(n)) for u, v, n in clusters_children]\n",
    "#     logger.debug(f'Clusters based dendrogram children {result}')\n",
    "    return result\n",
    "\n",
    "\n",
    "def convert_clusters_dendrogram_to_paths(clusters, children):\n",
    "    logger.debug('Converting agglomerate clustering clusters dendrogram format to path for visualization')\n",
    "    paths = [[p] for p in sorted(set(clusters))]\n",
    "    for i, (u, v, n) in enumerate(children):\n",
    "        for p in paths:\n",
    "            if p[i] == u or p[i] == v:\n",
    "                p.append(n)\n",
    "            else:\n",
    "                p.append(p[i])\n",
    "#     logger.debug(f'Paths {paths}')\n",
    "    logger.debug('Radix sort or paths to ensure no overlaps')\n",
    "    for i in range(len(children)):\n",
    "        paths.sort(key=lambda p: p[i])\n",
    "        # Reorder next level to keep order of previous if possible\n",
    "        if i != len(children):\n",
    "            order = dict((v, i) for i, v in enumerate(unique_everseen(p[i + 1] for p in paths)))\n",
    "            for p in paths:\n",
    "                p[i + 1] = order[p[i + 1]]\n",
    "    leaves_order = dict((v, i) for i, v in enumerate(unique_everseen(p[0] for p in paths)))\n",
    "    return paths, leaves_order\n",
    "\n",
    "def contrast_color(rgb):\n",
    "    r, g, b = rgb.r, rgb.g, rgb.b\n",
    "    \"\"\"\n",
    "    Light foreground for dark background and vice verse.\n",
    "    Idea Taken from https://stackoverflow.com/a/1855903/418358\n",
    "    \"\"\"\n",
    "    # Counting the perceptive luminance - human eye favors green color...\n",
    "    if 1 - (0.299 * r + 0.587 * g + 0.114 * b) / 255 < 0.5:\n",
    "        return RGB(0, 0, 0)\n",
    "    else:\n",
    "        return RGB(255, 255, 255)\n",
    "\n",
    "\n",
    "def topics_words(kwd_df, max_words):\n",
    "    words2show = {}\n",
    "    for _, row in kwd_df.iterrows():\n",
    "        comp, kwds = row[0], row[1]        \n",
    "        if kwds != '':  # Correctly process empty freq_kwds encoding\n",
    "            words2show[comp] = [p.split(':')[0] for p in kwds.split(',')[:max_words]]\n",
    "    return words2show\n",
    "\n",
    "\n",
    "def topics_hierarchy_with_keywords(df, kwd_df, clusters, dendrogram_children, \n",
    "                                   max_words=3, plot_width=1200, plot_height=800):\n",
    "    comp_sizes = Counter(df['comp'])\n",
    "    logger.debug('Computing dendrogram for clusters')\n",
    "    if dendrogram_children is None:\n",
    "        return None\n",
    "    clusters_dendrogram = compute_clusters_dendrogram_children(clusters, dendrogram_children)\n",
    "    paths, leaves_order = convert_clusters_dendrogram_to_paths(clusters, clusters_dendrogram)\n",
    "\n",
    "    # Configure dimensions\n",
    "    p = figure(x_range=(-180, 180),\n",
    "               y_range=(-160, 160),\n",
    "               tools=\"save\",\n",
    "               width=plot_width, height=plot_height)\n",
    "    x_coefficient = 1.2  # Ellipse x coefficient\n",
    "    y_delta = 40  # Extra space near pi / 2 and 3 * pi / 2\n",
    "    n_topics = len(leaves_order)\n",
    "    radius = 100  # Radius of circular dendrogram\n",
    "    dendrogram_len = len(paths[0])\n",
    "    d_radius = radius / dendrogram_len\n",
    "    d_degree = 2 * pi / n_topics\n",
    "\n",
    "    # Leaves coordinates\n",
    "    leaves_degrees = dict((v, i * d_degree) for v, i in leaves_order.items())\n",
    "\n",
    "    # Draw dendrogram - from bottom to top\n",
    "    ds = leaves_degrees.copy()\n",
    "    for i in range(1, dendrogram_len):\n",
    "        next_ds = {}\n",
    "        for path in paths:\n",
    "            if path[i] not in next_ds:\n",
    "                next_ds[path[i]] = []\n",
    "            next_ds[path[i]].append(ds[path[i - 1]])\n",
    "        for v, nds in next_ds.items():\n",
    "            next_ds[v] = np.mean(nds)\n",
    "\n",
    "        for path in paths:\n",
    "            current_d = ds[path[i - 1]]\n",
    "            next_d = next_ds[path[i]]\n",
    "            p.line([cos(current_d) * d_radius * (dendrogram_len - i),\n",
    "                    cos(next_d) * d_radius * (dendrogram_len - i - 1)],\n",
    "                   [sin(current_d) * d_radius * (dendrogram_len - i),\n",
    "                    sin(next_d) * d_radius * (dendrogram_len - i - 1)],\n",
    "                   line_color='lightgray')\n",
    "        ds = next_ds\n",
    "\n",
    "    # Draw leaves\n",
    "    n_comps = len(comp_sizes)\n",
    "    cmap = Plotter.factors_colormap(n_comps)\n",
    "    topics_colors = dict((i, Plotter.color_to_rgb(cmap(i))) for i in range(n_comps))\n",
    "    xs = [cos(d) * d_radius * (dendrogram_len - 1) for _, d in leaves_degrees.items()]\n",
    "    ys = [sin(d) * d_radius * (dendrogram_len - 1) for _, d in leaves_degrees.items()]\n",
    "    sizes = [20 + int(min(10, math.log(comp_sizes[v]))) for v, _ in leaves_degrees.items()]\n",
    "    comps = [v + 1 for v, _ in leaves_degrees.items()]\n",
    "    colors = [topics_colors[v] for v, _ in leaves_degrees.items()]\n",
    "    ds = ColumnDataSource(data=dict(x=xs, y=ys, size=sizes, comps=comps, color=colors))\n",
    "    p.circle(x='x', y='y', size='size', fill_color='color', line_color='black', source=ds)\n",
    "\n",
    "    # Topics labels\n",
    "    p.text(x=[cos(d) * d_radius * (dendrogram_len - 1) for _, d in leaves_degrees.items()],\n",
    "           y=[sin(d) * d_radius * (dendrogram_len - 1) for _, d in leaves_degrees.items()],\n",
    "           text=[str(v + 1) for v, _ in leaves_degrees.items()],\n",
    "           text_align='center', text_baseline='middle', text_font_size='10pt',\n",
    "           text_color=[contrast_color(topics_colors[v]) for v, _ in leaves_degrees.items()])\n",
    "\n",
    "    # Show words for components - most popular words per component\n",
    "    topics = leaves_order.keys()\n",
    "    words2show = topics_words(kwd_df, max_words)\n",
    "\n",
    "    # Visualize words\n",
    "    for v, d in leaves_degrees.items():\n",
    "        if v not in words2show:  # No super-specific words for topic\n",
    "            continue\n",
    "        words = words2show[v]\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i, word in enumerate(words):\n",
    "            wd = d + d_degree * (i - len(words) / 2) / len(words)\n",
    "            # Make word degree in range 0 - 2 * pi\n",
    "            if wd < 0:\n",
    "                wd += 2 * pi\n",
    "            elif wd > 2 * pi:\n",
    "                wd -= 2 * pi\n",
    "            xs.append(cos(wd) * radius * x_coefficient)\n",
    "            y = sin(wd) * radius\n",
    "            # Additional vertical space around pi/2 and 3*pi/2\n",
    "            if pi / 4 <= wd < 3 * pi / 4:\n",
    "                y += pow(pi / 4 - fabs(pi / 2 - wd), 1.5) * y_delta\n",
    "            elif 5 * pi / 4 <= wd < 7 * pi / 4:\n",
    "                y -= pow(pi / 4 - fabs(3 * pi / 2 - wd), 1.5) * y_delta\n",
    "            ys.append(y)\n",
    "\n",
    "        # Different text alignment for left | right parts\n",
    "        p.text(x=[x for x in xs if x > 0], y=[y for i, y in enumerate(ys) if xs[i] > 0],\n",
    "               text=[w for i, w in enumerate(words) if xs[i] > 0],\n",
    "               text_align='left', text_baseline='middle', text_font_size='10pt',\n",
    "               text_color=topics_colors[v])\n",
    "        p.text(x=[x for x in xs if x <= 0], y=[y for i, y in enumerate(ys) if xs[i] <= 0],\n",
    "               text=[w for i, w in enumerate(words) if xs[i] <= 0],\n",
    "               text_align='right', text_baseline='middle', text_font_size='10pt',\n",
    "               text_color=topics_colors[v])\n",
    "\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.minor_tick_line_color = None\n",
    "    p.axis.major_label_text_color = None\n",
    "    p.axis.major_label_text_font_size = '0pt'\n",
    "    p.axis.axis_line_color = None\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Plotting topics hierarchy with keywords')\n",
    "show(topics_hierarchy_with_keywords(df, kwd_df, clusters, dendrogram_children, max_words=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Save topics hierarchy with keywords to file')\n",
    "output_file(filename=\"topics.html\", title=\"Topics dendrogram\")\n",
    "save(topics_hierarchy_with_keywords(df, kwd_df, clusters, dendrogram_children, \n",
    "                                    max_words=3, plot_height=1200, plot_width=1200))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and save similarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = [','.join(t for t, _ in clusters_description[c][:5]) for c in df['comp']]\n",
    "\n",
    "logging.info('Saving papers and components dataframes')\n",
    "df.to_csv('papers.csv', index=False)\n",
    "t = kwd_df.copy()\n",
    "t['comp'] += 1\n",
    "t.to_csv('tags.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine, LabelSet\n",
    "\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "\n",
    "def similarity_graph(g, df, plot_width=600, plot_height=600):\n",
    "    nodes = df['id']\n",
    "    graph = GraphRenderer()\n",
    "    comps = df['comp']\n",
    "    cmap = Plotter.factors_colormap(len(set(comps)))\n",
    "    palette = dict(zip(sorted(set(comps)), [Plotter.color_to_rgb(cmap(i)).to_hex()\n",
    "                                            for i in range(len(set(comps)))]))\n",
    "\n",
    "    graph.node_renderer.data_source.add(df['id'], 'index')\n",
    "    graph.node_renderer.data_source.data['id'] = df['id']\n",
    "    graph.node_renderer.data_source.data['title'] = df['title']\n",
    "    graph.node_renderer.data_source.data['authors'] = df['authors']\n",
    "    graph.node_renderer.data_source.data['journal'] = df['journal']\n",
    "    graph.node_renderer.data_source.data['year'] = df['year']\n",
    "    graph.node_renderer.data_source.data['cited'] = df['total']\n",
    "    graph.node_renderer.data_source.data['tags'] = df['tags']\n",
    "    # Limit size\n",
    "    graph.node_renderer.data_source.data['size'] = df['total'] * 20 / df['total'].max() + 5\n",
    "    graph.node_renderer.data_source.data['topic'] = [c + 1 for c in comps]\n",
    "    graph.node_renderer.data_source.data['color'] = [palette[c] for c in comps]\n",
    "\n",
    "    graph.edge_renderer.data_source.data = dict(start=[u for u, _ in g.edges],\n",
    "                                                end=[v for _, v in g.edges])\n",
    "\n",
    "    # start of layout code\n",
    "    x = df['x']\n",
    "    y = df['y']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(plot_width=plot_width,\n",
    "               plot_height=plot_height,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange), \n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),\n",
    "               tools=\"pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    p.add_tools(HoverTool(tooltips=plotter._html_tooltips([\n",
    "            (\"Author(s)\", '@authors'),\n",
    "            (\"Journal\", '@journal'),\n",
    "            (\"Year\", '@year'),\n",
    "            (\"Cited by\", '@total paper(s) total'),\n",
    "            (\"Topic\", '@topic'),\n",
    "            (\"Tags\", '@tags')])))\n",
    "\n",
    "\n",
    "    graph_layout = dict(zip(nodes, zip(x, y)))\n",
    "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "    graph.node_renderer.glyph = Circle(size='size', fill_alpha=0.7, line_alpha=0.7, fill_color='color')\n",
    "    graph.node_renderer.hover_glyph = Circle(size='size', fill_alpha=1.0, line_alpha=1.0, fill_color='color')\n",
    "\n",
    "    graph.edge_renderer.glyph = MultiLine(line_color='lightgrey', line_alpha=0.5, line_width=1)\n",
    "    graph.edge_renderer.hover_glyph = MultiLine(line_color='grey', line_alpha=1.0, line_width=2)\n",
    "\n",
    "    graph.inspection_policy = NodesAndLinkedEdges()\n",
    "    p.renderers.append(graph)\n",
    "    \n",
    "    #Add Labels\n",
    "    lxs = [df.loc[df['comp'] == c]['x'].mean() for c in sorted(set(comps))]\n",
    "    lys = [df.loc[df['comp'] == c]['y'].mean() for c in sorted(set(comps))]\n",
    "    comp_labels = [f\"#{c + 1}\" for c in sorted(set(comps))]\n",
    "    source = ColumnDataSource({'x': lxs, 'y': lys, 'name': comp_labels})\n",
    "    labels = LabelSet(x='x', y='y', text='name', source=source, \n",
    "                      background_fill_color='white', text_font_size='11px', background_fill_alpha=.9)\n",
    "    p.renderers.append(labels)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save similarity plots to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.graph import local_sparse\n",
    "\n",
    "logging.info('Visualize structure graph using projected coordinates')\n",
    "wsg = to_weighted_graph(analyzer.similarity_graph, weight_func=PapersAnalyzer.similarity)\n",
    "\n",
    "# Limit total number of edges in sparse graph\n",
    "gs = wsg\n",
    "e = 1.0\n",
    "while e > 0.1 and gs.number_of_edges() / gs.number_of_nodes() > 3:\n",
    "    e -= 0.1\n",
    "    logging.info(f'Testing e={e}')\n",
    "    gs = local_sparse(wsg, e)\n",
    "logger.info(f'Sparse grap e={e} nodes={gs.number_of_nodes()} edges={gs.number_of_edges()}')\n",
    "\n",
    "show(similarity_graph(gs, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save, reset_output, output_notebook\n",
    "\n",
    "logging.info('Saving papers similarity graph for bokeh')\n",
    "output_file(filename=\"papers.html\", title=\"Papers similarity graph\")\n",
    "save(similarity_graph(gs, df, plot_width=1200, plot_height=1200))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jinja2\n",
    "import networkx as nx\n",
    "\n",
    "from pysrc.papers.utils import cut_authors_list\n",
    "\n",
    "logging.info('Saving papers similarity graph for cytoscape.js')\n",
    "\n",
    "topics_tags = {c: ','.join(t for t, _ in clusters_description[c][:5]) for c in sorted(set(df['comp']))}\n",
    "\n",
    "logger.debug('Creating graph')\n",
    "gss = nx.Graph()\n",
    "for (u, v) in gs.edges():\n",
    "    gss.add_edge(u, v)\n",
    "for n in gs.nodes():\n",
    "    if not gss.has_node(n):\n",
    "        gss.add_node(n)\n",
    "\n",
    "logger.debug('Collect attributes for nodes')\n",
    "attrs = {}\n",
    "for node in df['id']:\n",
    "    sel = df[df['id'] == node]\n",
    "    attrs[node] = dict(\n",
    "        title=sel['title'].values[0],\n",
    "        authors=cut_authors_list(sel['authors'].values[0]),\n",
    "        journal=sel['journal'].values[0],\n",
    "        year=int(sel['year'].values[0]),\n",
    "        cited=int(sel['total'].values[0]),\n",
    "        topic=int(sel['comp'].values[0]),\n",
    "        # These can be heavy\n",
    "        abstract=sel['abstract'].values[0],\n",
    "        mesh=sel['mesh'].values[0],\n",
    "        keywords=sel['keywords'].values[0],\n",
    "    )\n",
    "    \n",
    "nx.set_node_attributes(gss, attrs)\n",
    "graph_cs = nx.cytoscape_data(gss)['elements']\n",
    "\n",
    "logger.debug('Layout')\n",
    "maxy = df['y'].max()\n",
    "for node_cs in graph_cs['nodes']:\n",
    "            nid = node_cs['data']['id']\n",
    "            sel = df.loc[df['id'] == nid]\n",
    "            # Adjust vertical axis with bokeh graph\n",
    "            node_cs['position'] = dict(x=int(sel['x'].values[0] * 8), \n",
    "                                       y=int((maxy - sel['y'].values[0]) * 6))\n",
    "\n",
    "with open('papers_template.html') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "html = jinja2.Environment(loader=jinja2.BaseLoader()).from_string(text).render(\n",
    "    topics_palette_json=json.dumps(Plotter.topics_palette(df)),\n",
    "    topics_description_json=json.dumps(topics_tags),\n",
    "    graph_cytoscape_json=json.dumps(graph_cs)\n",
    ")\n",
    "\n",
    "with open('papers_interactive.html', 'w') as f:\n",
    "    f.write(html)\n",
    "\n",
    "logger.debug('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(authors, first_last_only=True):\n",
    "    return authors if len(authors) <= 2 or not first_last_only else [authors[0], authors[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def compute_authors_citations_and_papers(df):\n",
    "    logger.debug('Compute author citations')\n",
    "    author_citations = {}\n",
    "    for i, row in tqdm(df[['authors', 'total']].iterrows()):\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            author_citations[a] = author_citations.get(a, 0) + row['total']\n",
    "\n",
    "    logger.debug('Compute number of papers per author')\n",
    "    author_papers = {}\n",
    "    for i, row in df[['title', 'authors']].iterrows():\n",
    "        authors = fa(row['authors'].split(', '))\n",
    "        for a in authors:\n",
    "            author_papers[a] = author_papers.get(a, 0) + 1\n",
    "\n",
    "    return author_citations, author_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pysrc.papers.analysis.metadata import popular_authors, popular_journals\n",
    "\n",
    "logging.info(\"Analyzing groups of similar authors\")\n",
    "authors_citations, authors_papers = compute_authors_citations_and_papers(analyzer.df)\n",
    "logging.info(f\"Authors {len(authors_papers)}\")\n",
    "min_threshold = np.percentile(list(authors_papers.values()), 90)\n",
    "logging.info(f'Min papers for author {min_threshold}')\n",
    "logging.info(f'Filtered authors: {sum(v >= min_threshold for v in authors_papers.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_authors_similarity_graph(df,\n",
    "                                   cocit_grouped_df, bibcoupling_df, citations_graph, texts_similarity,\n",
    "                                   check_author_func=lambda a: True):\n",
    "    logger.debug('Processing papers')\n",
    "    result = nx.Graph()\n",
    "    for _, row in tqdm(df[['authors']].iterrows()):\n",
    "        authors = fa(row[0].split(', '))\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                a1 = authors[i]\n",
    "                a2 = authors[j]\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'authorship', 1)\n",
    "\n",
    "    logger.debug('Processing co-citations')\n",
    "    for el in tqdm(cocit_grouped_df[['cited_1', 'cited_2', 'total']].values):\n",
    "        start, end, cocitation = str(el[0]), str(el[1]), float(el[2])\n",
    "        authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'cocitation', cocitation)\n",
    "\n",
    "    logger.debug('Bibliographic coupling')\n",
    "    if len(bibcoupling_df) > 0:\n",
    "        for el in tqdm(bibcoupling_df[['citing_1', 'citing_2', 'total']].values):\n",
    "            start, end, bibcoupling = str(el[0]), str(el[1]), float(el[2])\n",
    "            authors1 = fa(df.loc[df['id'] == start]['authors'].values[0].split(', '))\n",
    "            authors2 = fa(df.loc[df['id'] == end]['authors'].values[0].split(', '))\n",
    "            for a1, a2 in itertools.product(authors1, authors2):\n",
    "                if check_author_func(a1) and check_author_func(a2):\n",
    "                    update_edge(result, a1, a2, 'bibcoupling', bibcoupling)\n",
    "\n",
    "    logger.debug('Text similarity')\n",
    "    pids = list(df['id'])\n",
    "    if len(df) >= 2:\n",
    "        for i, pid1 in enumerate(tqdm(df['id'])):\n",
    "            similarity_queue = texts_similarity[i]\n",
    "            while not similarity_queue.empty():\n",
    "                similarity, j = similarity_queue.get()\n",
    "                pid2 = pids[j]\n",
    "                authors1 = fa(df.loc[df['id'] == pid1]['authors'].values[0].split(', '))\n",
    "                authors2 = fa(df.loc[df['id'] == pid2]['authors'].values[0].split(', '))\n",
    "                for a1, a2 in itertools.product(authors1, authors2):\n",
    "                    if check_author_func(a1) and check_author_func(a2):\n",
    "                        update_edge(result, a1, a2, 'text', similarity)\n",
    "\n",
    "    logger.debug('Citations')\n",
    "    for u, v in tqdm(citations_graph.edges):\n",
    "        authors1 = fa(df.loc[df['id'] == u]['authors'].values[0].split(', '))\n",
    "        authors2 = fa(df.loc[df['id'] == v]['authors'].values[0].split(', '))\n",
    "        for a1, a2 in itertools.product(authors1, authors2):\n",
    "            if check_author_func(a1) and check_author_func(a2):\n",
    "                update_edge(result, a1, a2, 'citation', 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def update_edge(graph, a1, a2, name, value):\n",
    "    if a1 == a2:\n",
    "        return\n",
    "    if a1 > a2:\n",
    "        a1, a2 = a2, a1\n",
    "    if not graph.has_edge(a1, a2):\n",
    "        graph.add_edge(a1, a2)\n",
    "    edge = graph[a1][a2]\n",
    "    edge[name] = edge.get(name, 0) + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "import itertools\n",
    "\n",
    "logger = logging.getLogger('Test')\n",
    "\n",
    "authors_similarity_graph = build_authors_similarity_graph(\n",
    "    analyzer.df, analyzer.cocit_grouped_df,\n",
    "    analyzer.bibliographic_coupling_df,\n",
    "    analyzer.citations_graph,\n",
    "    analyzer.texts_similarity,\n",
    "    check_author_func=lambda a: authors_papers[a] >= min_threshold\n",
    ")\n",
    "\n",
    "logging.info(f'Built authors graph - '\n",
    "             f'{len(authors_similarity_graph.nodes())} nodes and {len(authors_similarity_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec embeddings for authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.debug('Compute aggregated similarity using co-authorship')\n",
    "ga = to_weighted_graph(authors_similarity_graph, \n",
    "                       weight_func=lambda d: 100 * d.get('authorship', 0) + PapersAnalyzer.similarity(d))\n",
    "e = 1.0\n",
    "gs = local_sparse(ga, e)\n",
    "# Limit total number of edges to estimate walk probabilities\n",
    "while e > 0.1 and gs.number_of_edges() / gs.number_of_nodes() > 10:\n",
    "    e -= 0.1\n",
    "    gs = local_sparse(ga, e)\n",
    "logger.debug(f'Sparse graph for node2vec e={e} nodes={gs.number_of_nodes()} edges={gs.number_of_edges()}')\n",
    "authors_node_ids, authors_weighted_node_embeddings = node2vec(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "authors_tsne = TSNE(n_components=2, random_state=42)\n",
    "authors_weighted_node_embeddings_2d = tsne.fit_transform(authors_weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about authors and projected coordinates\n",
    "authors_df = pd.DataFrame(dict(author=authors_node_ids, \n",
    "                               x=authors_weighted_node_embeddings_2d[:, 0],\n",
    "                               y=authors_weighted_node_embeddings_2d[:, 1]))\n",
    "authors_df['cited'] = [authors_citations[a] for a in authors_df['author']]\n",
    "authors_df['papers'] = [authors_papers[a] for a in authors_df['author']]\n",
    "authors_df['size'] = [1 + 10 * np.log1p(authors_citations[a]) for a in authors_df['author']]\n",
    "# Limit max size\n",
    "authors_df['size'] = authors_df['size'] * 10 / authors_df['size'].max() + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = 0\n",
    "authors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_authors(authors_df, plot_width=600, plot_height=600):\n",
    "    clusters = set(authors_df['cluster'])\n",
    "    cmap = Plotter.factors_colormap(len(clusters))\n",
    "    palette = dict(zip(sorted(clusters), \n",
    "                       [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(clusters))]))\n",
    "    authors_df['color'] = [palette[c] for c in authors_df['cluster']]\n",
    "\n",
    "    ds = ColumnDataSource(authors_df)\n",
    "    del authors_df['color']\n",
    "    x = authors_df['x']\n",
    "    y = authors_df['y']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(plot_width=plot_width, plot_height=plot_height,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange), \n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),    \n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\",\n",
    "               tooltips=[(\"Author\", '@author'),\n",
    "                         (\"Papers\", '@papers'),\n",
    "                         (\"Cited by\", '@cited'),\n",
    "                         (\"Cluster\", '@cluster'),\n",
    "                         (\"Tags\", '@tags')])\n",
    "\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    p.circle(x='x', y='y', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color')\n",
    "\n",
    "    lxs = [authors_df.loc[authors_df['cluster'] == c]['x'].mean() for c in sorted(clusters)]\n",
    "    lys = [authors_df.loc[authors_df['cluster'] == c]['y'].mean() for c in sorted(clusters)]\n",
    "    cluster_labels = [f'#{c}' for c in sorted(clusters)]\n",
    "    source = ColumnDataSource({'x': lxs, 'y': lys, 'name': cluster_labels})\n",
    "    labels = LabelSet(x='x', y='y', text='name', source=source, \n",
    "                      background_fill_color='white', text_font_size='11px', background_fill_alpha=.9)\n",
    "    p.renderers.append(labels)\n",
    "\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = 0\n",
    "authors_df['tags'] = 'n/a'\n",
    "show(plot_authors(authors_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierachical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "author_clusters, _ = cluster_and_sort(authors_weighted_node_embeddings, 10, 100)\n",
    "\n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'cluster': author_clusters, \n",
    "                  'size': np.ones(len(author_clusters))}).groupby(['cluster']).sum().astype(int).reset_index()    \n",
    "sns.barplot(data=t, x='cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = author_clusters\n",
    "display(authors_df.head())\n",
    "\n",
    "logging.info('Saving authors and groups dataframes')\n",
    "authors_df.to_csv('authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ego-splitting to compute possible overlapping groups of authors\n",
    "Taken from https://github.com/benedekrozemberczki/EgoSplitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EgoNetSplitter(object):\n",
    "    \"\"\"An implementation of `\"Ego-Splitting\" see:\n",
    "    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n",
    "    From the KDD '17 paper \"Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters\".\n",
    "    The tool first creates the egonets of nodes.\n",
    "    A persona-graph is created which is clustered by the Louvain method.\n",
    "    The resulting overlapping cluster memberships are stored as a dictionary.\n",
    "    Args:\n",
    "        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution=1.0):\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def _create_egonet(self, node):\n",
    "        \"\"\"\n",
    "        Creating an ego net, extracting personas and partitioning it.\n",
    "\n",
    "        Args:\n",
    "            node: Node ID for egonet (ego node).\n",
    "        \"\"\"\n",
    "        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n",
    "        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n",
    "        new_mapping = {}\n",
    "        personalities = []\n",
    "        for k, v in components.items():\n",
    "            personalities.append(self.index)\n",
    "            for other_node in v:\n",
    "                new_mapping[other_node] = self.index\n",
    "            self.index = self.index+1\n",
    "        self.components[node] = new_mapping\n",
    "        self.personalities[node] = personalities\n",
    "\n",
    "    def _create_egonets(self):\n",
    "        \"\"\"\n",
    "        Creating an egonet for each node.\n",
    "        \"\"\"\n",
    "        self.components = {}\n",
    "        self.personalities = {}\n",
    "        self.index = 0\n",
    "        print(\"Creating egonets.\")\n",
    "        for node in tqdm(self.graph.nodes()):\n",
    "            self._create_egonet(node)\n",
    "\n",
    "    def _map_personalities(self):\n",
    "        \"\"\"\n",
    "        Mapping the personas to new nodes.\n",
    "        \"\"\"\n",
    "        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n",
    "\n",
    "    def _get_new_edge_ids(self, edge):\n",
    "        \"\"\"\n",
    "        Getting the new edge identifiers.\n",
    "        Args:\n",
    "            edge: Edge being mapped to the new identifiers.\n",
    "        \"\"\"\n",
    "        return (self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]])\n",
    "\n",
    "    def _create_persona_graph(self):\n",
    "        \"\"\"\n",
    "        Create a persona graph using the egonet components.\n",
    "        \"\"\"\n",
    "        print(\"Creating the persona graph.\")\n",
    "        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n",
    "        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n",
    "\n",
    "    def _create_partitions(self):\n",
    "        \"\"\"\n",
    "        Creating a non-overlapping clustering of nodes in the persona graph.\n",
    "        \"\"\"\n",
    "        print(\"Clustering the persona graph.\")\n",
    "        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n",
    "        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n",
    "        for node, membership in self.partitions.items():\n",
    "            self.overlapping_partitions[self.personality_map[node]].append(membership)\n",
    "\n",
    "    def fit(self, graph):\n",
    "        \"\"\"\n",
    "        Fitting an Ego-Splitter clustering model.\n",
    "\n",
    "        Arg types:\n",
    "            * **graph** *(NetworkX graph)* - The graph to be clustered.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self._create_egonets()\n",
    "        self._map_personalities()\n",
    "        self._create_persona_graph()\n",
    "        self._create_partitions()\n",
    "\n",
    "    def get_memberships(self):\n",
    "        r\"\"\"Getting the cluster membership of nodes.\n",
    "        Return types:\n",
    "            * **memberships** *(dictionary of lists)* - Cluster memberships.\n",
    "        \"\"\"\n",
    "        return self.overlapping_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = EgoNetSplitter(0.8)\n",
    "splitter.fit(authors_similarity_graph)\n",
    "\n",
    "ego_clusters = []\n",
    "for a, cs in splitter.overlapping_partitions.items():\n",
    "    ego_clusters.extend(cs)\n",
    "print('Total clusters', len(set(ego_clusters)))\n",
    "print('Clusters', Counter(ego_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze authors group topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import compute_tfidf\n",
    "\n",
    "def compute_groups_topics(authors_df):\n",
    "    logging.info('Computing groups of authors topics')\n",
    "    groups_counts = \\\n",
    "        np.zeros(shape=(len(set(authors_df['cluster'])), analyzer.corpus_counts.shape[1]), dtype=np.float64)\n",
    "\n",
    "    part_sizes = Counter(authors_df['cluster'])\n",
    "    authors_clusters_map=dict(zip(authors_df['author'], authors_df['cluster']))\n",
    "\n",
    "    for i, row in tqdm(analyzer.df[['authors']].iterrows()):\n",
    "        for a in row[0].split(', '):\n",
    "            if a in authors_clusters_map:\n",
    "                group = authors_clusters_map[a]\n",
    "                groups_counts[group, :] += analyzer.corpus_counts[i, :] / part_sizes[group]\n",
    "\n",
    "    tfidf = compute_tfidf(groups_counts)\n",
    "\n",
    "    logging.info('Take terms with the largest tfidf for topics')\n",
    "    result = {}\n",
    "    for g in range(groups_counts.shape[0]):\n",
    "        counter = Counter()\n",
    "        for i, t in enumerate(analyzer.corpus_terms):\n",
    "            counter[t] += tfidf[g, i]\n",
    "        # Ignore terms with insignificant frequencies\n",
    "        result[g] = [(t, f) for t, f in counter.most_common(10) if f > 0]\n",
    "    return result\n",
    "\n",
    "groups_topics = compute_groups_topics(authors_df)\n",
    "kwds = [(g, ','.join(f'{t}:{v:.3f}' for t, v in vs)) for g, vs in groups_topics.items()]\n",
    "logging.info('Description\\n' + '\\n'.join(f'{g}: {kwd}' for g, kwd in kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.DataFrame(columns=['group', 'authors', 'keywords'], dtype=object)\n",
    "for g in sorted(set(authors_df['cluster'])):\n",
    "    authors = ', '.join(authors_df.loc[authors_df['cluster'] == g]['author'])\n",
    "    groups_df.loc[len(groups_df)] = (g, authors, ','.join(t for t, _ in groups_topics[g]))\n",
    "\n",
    "display(groups_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Saving groups of authors with keywords')\n",
    "groups_df.to_csv('groups.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['tags'] = [', '.join(f'{t}:{v:.3f}' for t, v in groups_topics[c][:5]) for c in authors_df['cluster']]\n",
    "show(plot_authors(authors_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info('Saving author groups graph for bokeh')\n",
    "output_file(filename=\"authors.html\", title=\"Authors similarity graph\")\n",
    "\n",
    "save(plot_authors(authors_df, plot_width=1600, plot_height=1200))\n",
    "reset_output()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evolution_data, keywords_data = plotter.topic_evolution()\n",
    "show(evolution_data)\n",
    "print(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(analyzer.citations_graph, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(analyzer.citations_graph, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in analyzer.citations_graph:\n",
    "    for u in analyzer.citations_graph[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['id'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "titles = []\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    sel = analyzer.df[analyzer.df['id'] == node]\n",
    "    year = sel['year'].values[0]\n",
    "    \n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    \n",
    "    titles.append(sel['title'].values[0])\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PMID</span>\n",
    "            <span style=\"font-size: 10px;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PageRank</span>\n",
    "            <span style=\"font-size: 10px;\">@pagerank</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['title'] = titles\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "# graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['id'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['id'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "# graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "# graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.min_border_left = 75\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Papers by PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pmid, pagerank in sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)[:10]:\n",
    "    print(f\"{(100*pagerank):.2f} {analyzer.df[analyzer.df['id'] == pmid]['title'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank and citation ranking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "analyzer.df['citation_rank'] = analyzer.df['total'].rank(method='first', ascending=False)\n",
    "pagerank_rank = sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)\n",
    "\n",
    "r = np.zeros((len(pagerank_rank), 2))\n",
    "for i, (pmid, pr) in enumerate(pagerank_rank):\n",
    "    sel = analyzer.df[analyzer.df['id'] == pmid]\n",
    "    if len(sel) > 0:\n",
    "        r[i, 0] = i\n",
    "        r[i, 1] = int(sel['citation_rank'].values[0])\n",
    "        \n",
    "TOP_X = [5, 10, 30, 50, 100]\n",
    "for x in TOP_X:\n",
    "    rho, _ = spearmanr(r[:x, 0], r[:x, 1])\n",
    "    print(f'Spearman correlation coefficient for top {x}: {rho}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
