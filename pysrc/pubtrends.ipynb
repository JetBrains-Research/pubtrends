{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubtrends\n",
    "\n",
    "Experimental notebook for hypothesis testing and development purposes.\n",
    "\n",
    "**IMPORTANT** \n",
    "Turn on experimental features in config file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Define the `SEARCH_QUERY` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Run all cells & see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_QUERY = 'human aging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.db.pm_postgres_loader import PubmedPostgresLoader\n",
    "from pysrc.papers.db.ss_postgres_loader import SemanticScholarPostgresLoader\n",
    "from pysrc.papers.analyzer import PapersAnalyzer\n",
    "from pysrc.papers.plot.plotter import Plotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED, SORT_MOST_RECENT, cut_authors_list\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "logger = logging.getLogger('notebook')\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "config.feature_evolution_enabled = True\n",
    "loader = PubmedPostgresLoader(config)\n",
    "analyzer = PapersAnalyzer(loader, config)\n",
    "try:\n",
    "    ids = analyzer.search_terms(SEARCH_QUERY, limit=SEARCH_PAPERS, sort=SEARCH_SORT)\n",
    "    analyzer.analyze_papers(ids, SEARCH_QUERY)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.papers_by_year())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens, get_topic_word_cloud_data\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)\n",
    "wc, _ = plotter.papers_word_cloud_and_callback(freq_kwds)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show(plotter.top_cited_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.most_cited_per_year_papers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.fastest_growth_per_year_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent keywords timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.text import get_frequent_tokens\n",
    "\n",
    "freq_kwds = get_frequent_tokens(analyzer.top_cited_df, query=analyzer.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original keywords frequencies')\n",
    "show(plotter.plot_keywords_frequencies(freq_kwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from holoviews import opts\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "\n",
    "from pysrc.papers.plot.plot_preprocessor import PlotPreprocessor\n",
    "\n",
    "def plot_keywords_frequencies_smooth(freq_kwds, n=20):\n",
    "    keywords_df, years = PlotPreprocessor.frequent_keywords_data(\n",
    "        freq_kwds, analyzer.df, analyzer.corpus_terms, analyzer.corpus_counts, n\n",
    "    )\n",
    "\n",
    "    logging.debug('Local weighted linear regression smoothing')\n",
    "    dfs = []\n",
    "    for kwd in set(keywords_df['keyword']):\n",
    "        t = keywords_df.loc[keywords_df['keyword'] == kwd].copy()\n",
    "        t['number'] = lowess(t['number'], t['year'], frac=0.2, return_sorted=False)\n",
    "        dfs.append(t)\n",
    "        \n",
    "    keywords_df = pd.concat(dfs)\n",
    "\n",
    "    # Define the value dimensions\n",
    "    max_numbers = keywords_df['number'].max()\n",
    "    vdim = hv.Dimension('number', range=(-10, max_numbers + 10))\n",
    "\n",
    "    # Define the dataset\n",
    "    ds = hv.Dataset(keywords_df, vdims=vdim)\n",
    "    curves = ds.to(hv.Curve, 'year', groupby='keyword').overlay().redim(\n",
    "        year=dict(range=(min(years) - 1, max(years) + 5)))\n",
    "\n",
    "    # Define a function to get the text annotations\n",
    "    max_year = ds['year'].max()\n",
    "    label_df = keywords_df[keywords_df.year == max_year].copy().reset_index(drop=True)\n",
    "\n",
    "    # Update layout for better labels representation\n",
    "    label_df.sort_values(by='number', inplace=True)\n",
    "    if len(label_df) > 1:\n",
    "        label_df['number'] = [i * max_numbers / (len(label_df) - 1) for i in range(len(label_df))]\n",
    "    label_df.sort_values(by='keyword', inplace=True)\n",
    "    labels = hv.Labels(label_df, ['year', 'number'], 'keyword')\n",
    "\n",
    "    overlay = curves * labels\n",
    "\n",
    "    cmap = Plotter.factors_colormap(len(label_df))\n",
    "    palette = [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(label_df))]\n",
    "    overlay.opts(\n",
    "        opts.Curve(show_frame=False, labelled=[], tools=['hover'],\n",
    "                   width=600, height=600, show_legend=False,\n",
    "                   xticks=list(reversed(range(max(years), min(years), -5))),\n",
    "                   color=hv.Cycle(values=palette), alpha=0.8, line_width=2, show_grid=True),\n",
    "        opts.Labels(text_color='keyword', cmap=palette, text_align='left'),\n",
    "        opts.NdOverlay(batched=False,\n",
    "                       gridstyle={'grid_line_dash': [6, 4], 'grid_line_width': 1, 'grid_bounds': (0, 100)})\n",
    "    )\n",
    "    p = hv.render(overlay, backend='bokeh')\n",
    "    p.xaxis.axis_label = 'Year'\n",
    "    p.yaxis.axis_label = 'Number of papers'\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Smooth version of keyword frequencies')\n",
    "show(plot_keywords_frequencies_smooth(freq_kwds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single paper citations dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.paper_citations_per_year(analyzer.df, analyzer.df['id'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cluster papers\n",
    "# show(plotter.topics_info_and_word_cloud_and_callback()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.topic_years_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.structure_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.heatmap_topics_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df, topics = PlotPreprocessor.topics_similarity_data(\n",
    "    analyzer.similarity_graph, analyzer.partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Looks not useful at all for louvain clustering')\n",
    "show(plotter.topics_hierarchy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities analysis\n",
    "\n",
    "We hope that the distribution of similarities edge weights illustrates that majority of linked nodes are insignificantly similar in terms of their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bibcoupling_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "cocitations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "citations_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "text_similarities_array = np.zeros(len(analyzer.similarity_graph.edges))\n",
    "\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    bibcoupling_array[i] = np.log1p(data.get('bibcoupling', 0))\n",
    "    cocitations_array[i] = np.log1p(data.get('cocitation', 0))\n",
    "    citations_array[i] = data.get('citation', 0)\n",
    "    text_similarities_array[i] = data.get('text', 0)\n",
    "    similarities_array[i] = PapersAnalyzer.similarity(data)\n",
    "    \n",
    "fig = plt.figure(figsize=(5 * 4, 5))\n",
    "ax = plt.subplot(1, 4, 1)\n",
    "print(f'Bibcoupling, non-zero {np.count_nonzero(bibcoupling_array)} of {len(bibcoupling_array)}')\n",
    "bibcoupling_array = bibcoupling_array[np.nonzero(bibcoupling_array)]\n",
    "print(stats.describe(bibcoupling_array))\n",
    "sns.kdeplot(bibcoupling_array)\n",
    "plt.title('Bibcoupling')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 2)\n",
    "print(f'Co-citations, non-zero {np.count_nonzero(cocitations_array)} of {len(cocitations_array)}')\n",
    "cocitations_array = cocitations_array[np.nonzero(cocitations_array)]\n",
    "print(stats.describe(cocitations_array))\n",
    "sns.kdeplot(cocitations_array)\n",
    "plt.title('Co-citations')\n",
    "# plt.show()\n",
    "\n",
    "ax = plt.subplot(1, 4, 3)\n",
    "print(f'Text similarities, non-zero {np.count_nonzero(text_similarities_array)} of {len(text_similarities_array)}')\n",
    "text_similarities_array = text_similarities_array[np.nonzero(text_similarities_array)]\n",
    "print(stats.describe(text_similarities_array))\n",
    "sns.kdeplot(text_similarities_array)\n",
    "plt.title('Text')\n",
    "# plt.show\n",
    "\n",
    "ax = plt.subplot(1, 4, 4)\n",
    "print(f'Similarities, non-zero {np.count_nonzero(similarities_array)} of {len(similarities_array)}')\n",
    "print(stats.describe(similarities_array))\n",
    "sns.kdeplot(similarities_array)\n",
    "plt.title('Similarity')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Citations, non-zero {np.count_nonzero(citations_array)} of {len(citations_array)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional text similarities exploration\n",
    "We use cutoff = 0.1 as min text similarity, and limit those to 20 max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Analyze similarities between all papers')\n",
    "cos_similarities = cosine_similarity(analyzer.corpus_counts)\n",
    "cos_similarities_array = cos_similarities.reshape(-1)\n",
    "print(stats.describe(cos_similarities_array))\n",
    "print('Q1', np.percentile(cos_similarities_array, 25), \n",
    "      'Q2', np.percentile(cos_similarities_array, 50), \n",
    "      'Q3', np.percentile(cos_similarities_array, 75))\n",
    "\n",
    "fig = plt.figure(figsize=(5 * 2, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(cos_similarities_array)\n",
    "plt.title('Cosine similarities among all papers')\n",
    "# plt.show()\n",
    "\n",
    "print('Analyze similarities between papers with direct citations')\n",
    "pid_indx = {pid: i for i, pid in enumerate(analyzer.df['id'])}\n",
    "cited_cos_similarities = []\n",
    "for i, (u, v, data) in enumerate(analyzer.similarity_graph.edges(data=True)):\n",
    "    if data.get('citation', 0) != 0:\n",
    "        cited_cos_similarities.append(cos_similarities[pid_indx[u], pid_indx[v]])\n",
    "\n",
    "print(stats.describe(cited_cos_similarities))\n",
    "print('Q1', np.percentile(cited_cos_similarities, 25), \n",
    "      'Q2', np.percentile(cited_cos_similarities, 50), \n",
    "      'Q3', np.percentile(cited_cos_similarities, 75))\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "sns.kdeplot(cited_cos_similarities)\n",
    "plt.title('Cosine similarity between cited papers')\n",
    "          \n",
    "plt.show()                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarities graph embeddings with Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "import collections\n",
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def node2vec(graph, weight_func):\n",
    "    logger.debug('Translating nx graph into stellar graph representation')\n",
    "    g_weighted = nx.Graph()\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        g_weighted.add_edge(u, v, weight=weight_func(data))\n",
    "\n",
    "    G = StellarGraph.from_networkx(g_weighted, node_type_default=\"id\", edge_type_default=\"similarity\")\n",
    "    logger.debug(G.info())\n",
    "    \n",
    "    logger.debug('Performing random walks')\n",
    "    walk_length = 100  # maximum length of a random walk\n",
    "    rw = BiasedRandomWalk(G)\n",
    "\n",
    "    weighted_walks = rw.run(\n",
    "        nodes=G.nodes(),  # root nodes\n",
    "        length=walk_length,  # maximum length of a random walk\n",
    "        n=10,  # number of random walks per root node\n",
    "        p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "        q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "        weighted=True,  # for weighted random walks\n",
    "        seed=42,  # random seed fixed for reproducibility\n",
    "    )\n",
    "    logger.debug(f\"Number of random walks: {len(weighted_walks)}\")    \n",
    "    logger.debug('Representation learning using word2vec')\n",
    "    weighted_model = Word2Vec(\n",
    "        weighted_walks, size=128, window=5, min_count=0, sg=1, workers=1, iter=1\n",
    "    )\n",
    "    # Retrieve node embeddings and corresponding subjects\n",
    "    node_ids = weighted_model.wv.index2word  # list of node IDs\n",
    "    weighted_node_embeddings = (\n",
    "        weighted_model.wv.vectors\n",
    "    )\n",
    "    return node_ids, weighted_node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids, weighted_node_embeddings = node2vec(analyzer.similarity_graph, \n",
    "                                              weight_func=lambda d: PapersAnalyzer.similarity(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "weighted_node_embeddings_2d = tsne.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP \n",
    "\n",
    "# logger.debug('Apply UMAP transformation on node embeddings')\n",
    "# umap = UMAP(n_components=2, random_state=42)\n",
    "# weighted_node_embeddings_2d = umap.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about papers and projected coordinates\n",
    "df = analyzer.df[['id', 'title', 'year', 'type', 'total', 'authors', 'journal', 'comp']].copy()\n",
    "indx = [pid_indx[pid] for pid in node_ids]\n",
    "df['d1'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 0])\n",
    "df['d2'] = pd.Series(index=indx, data=weighted_node_embeddings_2d[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, CustomJS\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "\n",
    "from pysrc.papers.utils import cut_authors_list\n",
    "\n",
    "\n",
    "def plot_embeddings(df, clusters):\n",
    "    cmap = Plotter.factors_colormap(len(set(clusters)))\n",
    "    palette = dict(zip(sorted(set(clusters)), [Plotter.color_to_rgb(cmap(i)).to_hex() \n",
    "                                               for i in range(len(set(clusters)))]))\n",
    "\n",
    "    # Size is based on the citations number, at least 1\n",
    "    df['size'] = 1 + 2 * np.log1p(df['total'])\n",
    "\n",
    "    # Split authors\n",
    "    df['authors'] = df['authors'].apply(lambda authors: cut_authors_list(authors))\n",
    "\n",
    "    ds = ColumnDataSource(df)\n",
    "    # Add clusters coloring\n",
    "    ds.add([palette[c] for c in clusters], 'color')\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'd1'\n",
    "    p.yaxis.axis_label = 'd2'\n",
    "\n",
    "    p.hover.tooltips = plotter._html_tooltips([\n",
    "        (\"Author(s)\", '@authors'),\n",
    "        (\"Journal\", '@journal'),\n",
    "        (\"Year\", '@year'),\n",
    "        (\"Type\", '@type'),\n",
    "        (\"Cited by\", '@total paper(s) total')])\n",
    "    p.circle(x='d1', y='d2', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='comp')\n",
    "    p.legend.location = None\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot default clusters in embeddings coordinates')\n",
    "plot_embeddings(df, analyzer.df['comp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Min cluster size\n",
    "TOPIC_MIN_SIZE = 20\n",
    "# Max number of topics should be \"deliverable\"\n",
    "TOPICS_MAX_NUMBER = 20\n",
    "\n",
    "logger.debug('Looking for an appropriate number of clusters')\n",
    "r = TOPICS_MAX_NUMBER + 1\n",
    "l = 1\n",
    "\n",
    "while l < r - 2:\n",
    "    n_clusters = int((l + r) / 2)\n",
    "    logger.debug(f'l {l} r {r} n_clusters {n_clusters}')\n",
    "    model = AgglomerativeClustering(n_clusters=n_clusters).fit(weighted_node_embeddings)\n",
    "    clusters = model.labels_.astype(int)\n",
    "    clusters_counter = Counter(clusters)\n",
    "    min_size = clusters_counter.most_common()[-1][1]\n",
    "    logger.debug(f'min_size {min_size}')\n",
    "    if min_size < TOPIC_MIN_SIZE:\n",
    "        r = n_clusters + 1\n",
    "    elif min_size > TOPIC_MIN_SIZE:\n",
    "        l = n_clusters\n",
    "    else:\n",
    "        break\n",
    "\n",
    "logger.debug(f'Number of clusters = {n_clusters}')        \n",
    "logger.debug('Reorder clusters by size descending')\n",
    "clusters_reord = np.zeros(len(clusters), dtype=int)    \n",
    "for i, (c, n) in enumerate(clusters_counter.most_common()):\n",
    "    clusters_reord[clusters == c] = i\n",
    "clusters = clusters_reord\n",
    "    \n",
    "print('Cluster sizes')\n",
    "t = pd.DataFrame({'Cluster': clusters, \n",
    "                  'size': np.ones(len(clusters))}).groupby(['Cluster']).sum().astype(int).reset_index()    \n",
    "display(t)\n",
    "sns.barplot(data=t, x='Cluster', y='size')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "\n",
    "print('Computing clusters keywords')\n",
    "clusters_pids = pd.DataFrame(dict(id=node_ids, comp=clusters)).groupby('comp')['id'].apply(list).to_dict()\n",
    "\n",
    "clusters_description = get_topics_description(\n",
    "    analyzer.df, clusters_pids,\n",
    "    analyzer.corpus_terms, analyzer.corpus_counts,\n",
    "    query=analyzer.query,\n",
    "    n_words=analyzer.TOPIC_DESCRIPTION_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "    \n",
    "plot_dendrogram(model, truncate_mode='level', p=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unique_everseen\n",
    "\n",
    "def compute_clusters_dendrogram_children(m):\n",
    "    leaves_map = dict(enumerate(m.labels_))\n",
    "    nodes_map = {}\n",
    "    children = []\n",
    "    for i, (u, v) in enumerate(m.children_):\n",
    "        u_cluster = leaves_map[u] if u in leaves_map else nodes_map[u] if u in nodes_map else None\n",
    "        v_cluster = leaves_map[v] if v in leaves_map else nodes_map[v] if v in nodes_map else None\n",
    "#         print(f'{u}: {u_cluster}, {v}: {v_cluster}')\n",
    "        node = len(leaves_map) + i\n",
    "        if u_cluster is not None and v_cluster is not None:\n",
    "            if u_cluster != v_cluster:\n",
    "                nodes_map[node] = None\n",
    "#                 print('Node', node, None)\n",
    "                children.append((u, v, node))\n",
    "#                 print('Added', (u, v, node))\n",
    "            else:\n",
    "                nodes_map[node] = u_cluster\n",
    "#                 print('Node', node, u_cluster)\n",
    "        else:\n",
    "            nodes_map[node] = u_cluster or v_cluster\n",
    "#             print('Node', node, u_cluster or v_cluster)\n",
    "            children.append((u, v, node))\n",
    "#             print('Added', (u, v, node))\n",
    "    \n",
    "    def rwc(v):\n",
    "        if v in leaves_map:\n",
    "            return leaves_map[v]\n",
    "        elif v in nodes_map:\n",
    "            res = nodes_map[v]\n",
    "            return res if res is not None else v\n",
    "        else:\n",
    "            return v\n",
    "    children = [(rwc(u), rwc(v), rwc(n)) for u, v, n in children]\n",
    "    return children\n",
    "\n",
    "def convert_clusters_paths(m):\n",
    "    logger.debug('Computing dendrogram for clusters')\n",
    "    clusters_dendrogram_children = compute_clusters_dendrogram_children(m)\n",
    "    logger.debug('Converting agglomerative clustering clusters dendrogram format to louvain')\n",
    "    paths = [[p] for p in sorted(set(m.labels_))]\n",
    "    for i, (u, v, n) in enumerate(clusters_dendrogram_children):\n",
    "        for p in paths:\n",
    "            if p[i] == u or p[i] == v:\n",
    "                p.append(n)\n",
    "            else:\n",
    "                p.append(p[i])\n",
    "\n",
    "    logger.debug('Radix sort or paths to ensure no overlaps')\n",
    "    for i in range(len(clusters_dendrogram_children)):\n",
    "        paths.sort(key=lambda p: p[i])\n",
    "        # Reorder next level to keep order of previous if possible\n",
    "        if i != len(clusters_dendrogram_children):\n",
    "            order = dict((v, i) for i, v in enumerate(unique_everseen(p[i + 1] for p in paths)))\n",
    "            for p in paths:\n",
    "                p[i + 1] = order[p[i + 1]]\n",
    "    leaves_order = dict((v, i) for i, v in enumerate(unique_everseen(p[0] for p in paths)))\n",
    "    return paths, leaves_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, leaves_order = convert_clusters_paths(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi, sin, cos, fabs\n",
    "from bokeh.colors import RGB\n",
    "\n",
    "from pysrc.papers.utils import contrast_color\n",
    "\n",
    "def topics_hierarchy_with_keywords(kwd_df, comp_sizes, paths, leaves_order):\n",
    "    # Configure dimensions\n",
    "    p = figure(x_range=[-190, 190],\n",
    "               y_range=[-160, 160],\n",
    "               tools=\"save\",\n",
    "               width=600, height=600)\n",
    "    x_coefficient = 1.5  # Ellipse x coefficient\n",
    "    y_delta = 60  # Extra space near pi / 2 and 3 * pi / 2\n",
    "    n_topics = len(leaves_order)\n",
    "    radius = 80  # Radius of circular dendrogram\n",
    "    dendrogram_len = len(paths[0])\n",
    "    d_radius = radius / (dendrogram_len + 2)\n",
    "    d_degree = 2 * pi / n_topics\n",
    "    char_delta = 2  # Multiplier to compute approximate width of text\n",
    "    delta = 5  # Space between dendrogram and text\n",
    "    max_words = min(5, max(1, int(120 / n_topics)))\n",
    "\n",
    "    # Leaves coordinates\n",
    "    leaves_degrees = dict((v, i * d_degree) for v, i in leaves_order.items())\n",
    "\n",
    "    # Draw levels\n",
    "    for i in range(2, dendrogram_len + 1): \n",
    "        p.ellipse(0, 0, fill_alpha=0, line_color='lightgray', line_alpha=0.5, \n",
    "                    width=2 * d_radius * i,\n",
    "                    height=2 * d_radius * i,\n",
    "                    line_dash='dotted')\n",
    "\n",
    "    # Draw dendrogram - from bottom to top\n",
    "    ds = leaves_degrees.copy()\n",
    "    for i in range(1, dendrogram_len):\n",
    "        next_ds = {}\n",
    "        for path in paths:\n",
    "            if path[i] not in next_ds:\n",
    "                next_ds[path[i]] = []\n",
    "            next_ds[path[i]].append(ds[path[i - 1]])\n",
    "        for v, nds in next_ds.items():\n",
    "            next_ds[v] = np.mean(nds)\n",
    "\n",
    "        for path in paths:\n",
    "            current_d = ds[path[i - 1]]\n",
    "            next_d = next_ds[path[i]]\n",
    "            p.line([cos(current_d) * d_radius * (dendrogram_len + 2 - i),\n",
    "                    cos(next_d) * d_radius * (dendrogram_len + 2 - i - 1)],\n",
    "                   [sin(current_d) * d_radius * (dendrogram_len + 2 - i),\n",
    "                    sin(next_d) * d_radius * (dendrogram_len + 2 - i - 1)],\n",
    "                   line_color='lightgray')\n",
    "        ds = next_ds\n",
    "\n",
    "    # Draw leaves\n",
    "    n_comps = len(comp_sizes)\n",
    "    cmap = Plotter.factors_colormap(n_comps)\n",
    "    topics_colors = dict((i, Plotter.color_to_rgb(cmap(i))) for i in range(n_comps))\n",
    "    xs = [cos(d) * d_radius * (dendrogram_len + 1) for _, d in leaves_degrees.items()]\n",
    "    ys = [sin(d) * d_radius * (dendrogram_len + 1) for _, d in leaves_degrees.items()]\n",
    "    sizes = [20 + int(min(10, math.log(comp_sizes[v]))) for v, _ in leaves_degrees.items()]\n",
    "    comps = [v + 1 for v, _ in leaves_degrees.items()]\n",
    "    colors = [topics_colors[v] for v, _ in leaves_degrees.items()]\n",
    "    ds = ColumnDataSource(data=dict(x=xs, y=ys, size=sizes, comps=comps, color=colors))\n",
    "    p.circle(x='x', y='y', size='size', fill_color='color', line_color='black', source=ds)\n",
    "\n",
    "    def contrast_color_rbg(rgb):\n",
    "        cr, cg, cb = contrast_color(rgb.r, rgb.g, rgb.b)\n",
    "        return RGB(cr, cg, cb)\n",
    "\n",
    "    # Topics labels\n",
    "    p.text(x=[cos(d) * d_radius * (dendrogram_len + 1) - char_delta * len(str(v + 1))\n",
    "              for v, d in leaves_degrees.items()],\n",
    "           y=[sin(d) * d_radius * (dendrogram_len + 1) for _, d in leaves_degrees.items()],\n",
    "           text=[str(v + 1) for v, _ in leaves_degrees.items()],\n",
    "           text_baseline='middle', text_font_size='10pt',\n",
    "           text_color=contrast_color_rbg(topics_colors[v]))\n",
    "\n",
    "    # Show words for components - most popular words per component\n",
    "    topics = leaves_order.keys()\n",
    "    words2show = PlotPreprocessor.topics_words(kwd_df, max_words, topics)\n",
    "\n",
    "    # Visualize words\n",
    "    for v, d in leaves_degrees.items():\n",
    "        if v not in words2show:  # No super-specific words for topic\n",
    "            continue\n",
    "        words = words2show[v]\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i, word in enumerate(words):\n",
    "            wd = d + d_degree * (i - len(words) / 2) / len(words)\n",
    "            # Make word degree in range 0 - 2 * pi\n",
    "            if wd < 0:\n",
    "                wd += 2 * pi\n",
    "            elif wd > 2 * pi:\n",
    "                wd -= 2 * pi\n",
    "            xs.append(cos(wd) * (radius * x_coefficient + delta))\n",
    "            y = sin(wd) * (radius + delta)\n",
    "            # Additional vertical space around pi/2 and 3*pi/2\n",
    "            if pi / 4 <= wd < 3 * pi / 4:\n",
    "                y += (pi / 4 - fabs(pi / 2 - wd)) * y_delta\n",
    "            elif 5 * pi / 4 <= wd < 7 * pi / 4:\n",
    "                y -= (pi / 4 - fabs(3 * pi / 2 - wd)) * y_delta\n",
    "            ys.append(y)\n",
    "\n",
    "        # Different text alignment for left | right parts\n",
    "        p.text(x=[x for x in xs if x > 0], y=[y for i, y in enumerate(ys) if xs[i] > 0],\n",
    "               text=[w for i, w in enumerate(words) if xs[i] > 0],\n",
    "               text_align='left', text_baseline='middle', text_font_size='10pt',\n",
    "               text_color=topics_colors[v])\n",
    "        p.text(x=[x for x in xs if x <= 0], y=[y for i, y in enumerate(ys) if xs[i] <= 0],\n",
    "               text=[w for i, w in enumerate(words) if xs[i] <= 0],\n",
    "               text_align='right', text_baseline='middle', text_font_size='10pt',\n",
    "               text_color=topics_colors[v])\n",
    "\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.axis.major_tick_line_color = None\n",
    "    p.axis.minor_tick_line_color = None\n",
    "    p.axis.major_label_text_color = None\n",
    "    p.axis.major_label_text_font_size = '0pt'\n",
    "    p.axis.axis_line_color = None\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwds = [(comp, ','.join([f'{t}:{v:.3f}' for t, v in vs[:analyzer.TOPIC_DESCRIPTION_WORDS]]))\n",
    "        for comp, vs in clusters_description.items()]\n",
    "kwd_df = pd.DataFrame(kwds, columns=['comp', 'kwd'])\n",
    "comp_sizes = Counter(clusters)\n",
    "show(topics_hierarchy_with_keywords(kwd_df, comp_sizes, paths, leaves_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df_clusters = analyzer.df['comp'].copy()\n",
    "original_partition = analyzer.partition\n",
    "\n",
    "df['comp'] = pd.Series(index=indx, data=clusters)\n",
    "clusters_partition = dict(zip(df['id'], df['comp']))\n",
    "\n",
    "analyzer.df['comp'] = df['comp']\n",
    "analyzer.partition = clusters_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO')\n",
    "# show(plotter.topic_years_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Structure graph visualization with new clusters')\n",
    "show(plotter.structure_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.heatmap_topics_similarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df, topics = PlotPreprocessor.topics_similarity_data(\n",
    "    analyzer.similarity_graph, clusters_partition\n",
    ")\n",
    "\n",
    "similarity_df['type'] = ['Inside' if x == y else 'Outside' \n",
    "                         for (x, y) in zip(similarity_df['comp_x'], similarity_df['comp_y'])]\n",
    "sns.displot(similarity_df, x=\"similarity\", hue=\"type\", kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "\n",
    "def structure_graph(g, df):\n",
    "    nodes = df['id']\n",
    "    graph = GraphRenderer()\n",
    "    comps = df['comp']\n",
    "    cmap = Plotter.factors_colormap(len(set(comps)))\n",
    "    palette = dict(zip(sorted(set(comps)), [Plotter.color_to_rgb(cmap(i)).to_hex()\n",
    "                                            for i in range(len(set(comps)))]))\n",
    "\n",
    "    graph.node_renderer.data_source.add(df['id'], 'index')\n",
    "    graph.node_renderer.data_source.data['id'] = df['id']\n",
    "    graph.node_renderer.data_source.data['title'] = df['title']\n",
    "    graph.node_renderer.data_source.data['authors'] = df['authors']\n",
    "    graph.node_renderer.data_source.data['journal'] = df['journal']\n",
    "    graph.node_renderer.data_source.data['year'] = df['year']\n",
    "    graph.node_renderer.data_source.data['cited'] = df['total']\n",
    "    # Limit size\n",
    "    graph.node_renderer.data_source.data['size'] = df['total'] * 20 / df['total'].max() + 5\n",
    "    graph.node_renderer.data_source.data['topic'] = [c + 1 for c in comps]\n",
    "    graph.node_renderer.data_source.data['color'] = [palette[c] for c in comps]\n",
    "\n",
    "    graph.edge_renderer.data_source.data = dict(start=[u for u, _ in g.edges],\n",
    "                                                end=[v for _, v in g.edges])\n",
    "\n",
    "    # start of layout code\n",
    "    x = df['d1']\n",
    "    y = df['d2']\n",
    "    xrange = max(x) - min(x)\n",
    "    yrange = max(y) - min(y)\n",
    "    p = figure(plot_width=600,\n",
    "               plot_height=600,\n",
    "               x_range=(min(x) - 0.05 * xrange, max(x) + 0.05 * xrange), \n",
    "               y_range=(min(y) - 0.05 * yrange, max(y) + 0.05 * yrange),\n",
    "               tools=\"pan,tap,wheel_zoom,box_zoom,reset,save\")\n",
    "    p.xaxis.major_tick_line_color = None  # turn off x-axis major ticks\n",
    "    p.xaxis.minor_tick_line_color = None  # turn off x-axis minor ticks\n",
    "    p.yaxis.major_tick_line_color = None  # turn off y-axis major ticks\n",
    "    p.yaxis.minor_tick_line_color = None  # turn off y-axis minor ticks\n",
    "    p.xaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.yaxis.major_label_text_font_size = '0pt'  # preferred method for removing tick labels\n",
    "    p.grid.grid_line_color = None\n",
    "    p.outline_line_color = None\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "\n",
    "    tooltip = \"\"\"\n",
    "    <div style=\"max-width: 500px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Author(s)</span>\n",
    "            <span style=\"font-size: 10px;\">@authors</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Journal</span>\n",
    "            <span style=\"font-size: 10px;\">@journal</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Cited</span>\n",
    "            <span style=\"font-size: 10px;\">@cited</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px; font-weight: bold;\">Topic</span>\n",
    "            <span style=\"font-size: 10px;\">@topic</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    p.add_tools(HoverTool(tooltips=tooltip))\n",
    "\n",
    "    graph_layout = dict(zip(nodes, zip(x, y)))\n",
    "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "    graph.node_renderer.glyph = Circle(size='size', fill_color='color')\n",
    "    graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "    graph.edge_renderer.glyph = MultiLine(line_color='grey', line_alpha=0.1, line_width=1)\n",
    "    graph.edge_renderer.hover_glyph = MultiLine(line_color='blue', line_alpha=1.0, line_width=2)\n",
    "\n",
    "    graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "    p.renderers.append(graph)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysrc.papers.analysis.graph import local_sparse\n",
    "\n",
    "print('Visualize structure graph using projected coordinates')\n",
    "show(structure_graph(local_sparse(analyzer.similarity_graph,  0.5), df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original louvain community clusters shown on projection coordinates')\n",
    "t = df.copy()\n",
    "t['comp'] = original_df_clusters # Restore original clusters\n",
    "show(structure_graph(local_sparse(analyzer.similarity_graph,  0.5), t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pysrc.papers.analysis.metadata import popular_authors, popular_journals, build_authors_similarity_graph, \\\n",
    "    compute_authors_citations_and_papers, cluster_authors\n",
    "\n",
    "logging.info(\"Analyzing groups of similar authors\")\n",
    "authors_citations, authors_papers = compute_authors_citations_and_papers(analyzer.df)\n",
    "authors_productivity = {a: np.log1p(authors_citations.get(a, 1)) * p for a, p in authors_papers.items()}\n",
    "min_threshold = np.percentile(list(authors_productivity.values()), 95)\n",
    "min_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "import itertools\n",
    "\n",
    "logger = logging.getLogger('Test')\n",
    "\n",
    "authors_similarity_graph = build_authors_similarity_graph(\n",
    "    analyzer.df, analyzer.texts_similarity, analyzer.citations_graph, \n",
    "    analyzer.cocit_grouped_df, analyzer.bibliographic_coupling_df,\n",
    "    check_author_func=lambda a: authors_productivity[a] >= min_threshold\n",
    ")\n",
    "\n",
    "# authors_similarity_graph = analyzer.authors_similarity_graph\n",
    "logging.info(f'Built authors graph - '\n",
    "             f'{len(authors_similarity_graph.nodes())} nodes and {len(authors_similarity_graph.edges())} edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Compute aggregated similarity using co-authorship')\n",
    "for _, _, d in authors_similarity_graph.edges(data=True):\n",
    "    d['similarity'] = 100 * d.get('authorship', 0) + PapersAnalyzer.similarity(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec embeddings for authors graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_node_ids, authors_weighted_node_embeddings = node2vec(authors_similarity_graph, \n",
    "                                                              weight_func=lambda d: d['similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Apply t-SNE transformation on node embeddings')\n",
    "authors_tsne = TSNE(n_components=2, random_state=42)\n",
    "authors_weighted_node_embeddings_2d = tsne.fit_transform(authors_weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe combining information about authors and projected coordinates\n",
    "authors_df = pd.DataFrame(dict(author=authors_node_ids, \n",
    "                               d1=authors_weighted_node_embeddings_2d[:, 0],\n",
    "                               d2=authors_weighted_node_embeddings_2d[:, 1]))\n",
    "authors_df['cited'] = [authors_citations[a] for a in authors_df['author']]\n",
    "authors_df['papers'] = [authors_papers[a] for a in authors_df['author']]\n",
    "authors_df['size'] = [1 + authors_productivity[a] for a in authors_df['author']]\n",
    "# Limit max size\n",
    "authors_df['size'] = authors_df['size'] * 20 / authors_df['size'].max() + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ColumnDataSource(authors_df)\n",
    "p = figure(plot_width=600, plot_height=600,\n",
    "           tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\", toolbar_location=\"right\",\n",
    "           tooltips=[(\"Author\", '@author'),(\"Papers\", '@papers'), (\"Cited\", '@cited')])\n",
    "p.sizing_mode = 'stretch_width'\n",
    "p.xaxis.axis_label = 'd1'\n",
    "p.yaxis.axis_label = 'd2'\n",
    "\n",
    "p.circle(x='d1', y='d2', fill_alpha=0.8, source=ds, size='size',\n",
    "         line_color='black', fill_color='blue', legend_field='author')\n",
    "p.legend.location = None\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_clusters = cluster_authors(authors_similarity_graph, analyzer.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Graph based visualization of authors graph')\n",
    "show(plotter.authors_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_authors_clusters(authors_df):\n",
    "    cmap = Plotter.factors_colormap(len(set(authors_df['cluster'])))\n",
    "    palette = dict(zip(sorted(set(authors_df['cluster'])), \n",
    "                       [Plotter.color_to_rgb(cmap(i)).to_hex() for i in range(len(set(authors_df['cluster'])))]))\n",
    "    authors_df['color'] = [palette[c] for c in authors_df['cluster']]\n",
    "\n",
    "    ds = ColumnDataSource(authors_df)\n",
    "    p = figure(plot_width=600, plot_height=600,\n",
    "               tools=\"hover,pan,tap,wheel_zoom,box_zoom,reset,save\", toolbar_location=\"right\",\n",
    "               tooltips=[(\"Author\", '@author'),(\"Papers\", '@papers'), (\"Cited\", '@cited'), ('Cluster', '@cluster')])\n",
    "    p.sizing_mode = 'stretch_width'\n",
    "    p.xaxis.axis_label = 'd1'\n",
    "    p.yaxis.axis_label = 'd2'\n",
    "\n",
    "    p.circle(x='d1', y='d2', fill_alpha=0.8, source=ds, size='size',\n",
    "             line_color='black', fill_color='color', legend_field='author')\n",
    "    p.legend.location = None\n",
    "    show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = [authors_clusters[a] for a in authors_df['author']]\n",
    "plot_authors_clusters(authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors clustering based on embeddings and number of clusters in papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f'Clustering authors based on embeddings')\n",
    "model = AgglomerativeClustering(n_clusters=2 * n_clusters).fit(authors_weighted_node_embeddings)\n",
    "authors_clusters = dict(zip(authors_node_ids, model.labels_.astype(int)))\n",
    "authors_clusters_counter = Counter(authors_clusters.values())\n",
    "print(authors_clusters_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df['cluster'] = [authors_clusters[a] for a in authors_df['author']]\n",
    "plot_authors_clusters(authors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORS_PER_COMP = 20\n",
    "group_authors = {}\n",
    "for group in sorted(set(authors_clusters.values())):\n",
    "    authors = [a for a in authors_clusters.keys() if authors_clusters[a] == group]\n",
    "    authors.sort(key=lambda a: authors_productivity[a], reverse=True)\n",
    "    top = authors[:AUTHORS_PER_COMP]\n",
    "    group_authors[group] = \", \".join(top)\n",
    "    print(f'#{group} ({len(authors)}) {\", \".join(top)}' + (', ...' if len(authors) > AUTHORS_PER_COMP else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_sizes = Counter(authors_clusters.values())\n",
    "paper_groups = np.zeros(shape=(len(analyzer.df), len(set(authors_clusters.values()))))\n",
    "for i, row in analyzer.df[['authors']].iterrows():\n",
    "    for a in row[0].split(', '):\n",
    "        if a in authors_clusters:\n",
    "            group = authors_clusters[a]\n",
    "            paper_groups[i, group] += 1 / part_sizes[group]\n",
    "groups = np.argmax(paper_groups, axis=1)\n",
    "papers_assigned = paper_groups.sum(axis=1) > 0\n",
    "groups_partition = {pid: groups[i] for i, pid in enumerate(analyzer.df['id']) if papers_assigned[i]}\n",
    "\n",
    "groups_part_sizes = {c: sum([groups_partition[node] == c for node in groups_partition.keys()]) \n",
    "                     for c in set(groups_partition.values())}\n",
    "logging.info(f'Components: {groups_part_sizes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pysrc.papers.analysis.topics import get_topics_description\n",
    "\n",
    "groups_pids = pd.DataFrame(groups_partition.items(), columns=['id', 'comp']). \\\n",
    "                groupby('comp')['id'].apply(list).to_dict()\n",
    "groups_description = get_topics_description(\n",
    "    analyzer.df.iloc[np.flatnonzero(papers_assigned), :], groups_pids,\n",
    "    analyzer.corpus_terms, analyzer.corpus_counts[np.flatnonzero(papers_assigned), :],\n",
    "    query=analyzer.query,\n",
    "    n_words=analyzer.TOPIC_DESCRIPTION_WORDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_df = pd.DataFrame(columns=['group', 'authors', 'papers', 'keywords'], dtype=object)\n",
    "for g, pids in groups_pids.items():\n",
    "    if g in group_authors and g in groups_description:\n",
    "        groups_df.loc[len(groups_df)] = (g, group_authors[g], len(pids), \n",
    "                                         ', '.join(v[0] for v in groups_description[g][:10]))\n",
    "\n",
    "display(groups_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evolution_data, keywords_data = plotter.topic_evolution()\n",
    "show(evolution_data)\n",
    "print(keywords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(analyzer.citations_graph, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(analyzer.citations_graph, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in analyzer.citations_graph:\n",
    "    for u in analyzer.citations_graph[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['id'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "titles = []\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    sel = analyzer.df[analyzer.df['id'] == node]\n",
    "    year = sel['year'].values[0]\n",
    "    \n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    \n",
    "    titles.append(sel['title'].values[0])\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "TOOLTIPS = \"\"\"\n",
    "    <div style=\"max-width: 320px\">\n",
    "        <div>\n",
    "            <span style=\"font-size: 12px; font-weight: bold;\">@title</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">Year</span>\n",
    "            <span style=\"font-size: 10px;\">@year</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PMID</span>\n",
    "            <span style=\"font-size: 10px;\">@id</span>\n",
    "        </div>\n",
    "        <div>\n",
    "            <span style=\"font-size: 11px;\">PageRank</span>\n",
    "            <span style=\"font-size: 10px;\">@pagerank</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=TOOLTIPS))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['id'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['title'] = titles\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "# graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['id'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['id'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "# graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "# graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.min_border_left = 75\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Papers by PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pmid, pagerank in sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)[:10]:\n",
    "    print(f\"{(100*pagerank):.2f} {analyzer.df[analyzer.df['id'] == pmid]['title'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank and citation ranking correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "analyzer.df['citation_rank'] = analyzer.df['total'].rank(method='first', ascending=False)\n",
    "pagerank_rank = sorted(pr_nx.items(), key=lambda el: el[1], reverse=True)\n",
    "\n",
    "r = np.zeros((len(pagerank_rank), 2))\n",
    "for i, (pmid, pr) in enumerate(pagerank_rank):\n",
    "    sel = analyzer.df[analyzer.df['id'] == pmid]\n",
    "    if len(sel) > 0:\n",
    "        r[i, 0] = i\n",
    "        r[i, 1] = int(sel['citation_rank'].values[0])\n",
    "        \n",
    "TOP_X = [5, 10, 30, 50, 100]\n",
    "for x in TOP_X:\n",
    "    rho, _ = spearmanr(r[:x, 0], r[:x, 1])\n",
    "    print(f'Spearman correlation coefficient for top {x}: {rho}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
