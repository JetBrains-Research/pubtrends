{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = 'dna methylation clock'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.pm_loader import PubmedLoader\n",
    "from pysrc.papers.ss_loader import SemanticScholarLoader\n",
    "from pysrc.papers.analyzer_experimental import ExperimentalAnalyzer\n",
    "from pysrc.papers.plotter import Plotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "loader = SemanticScholarLoader(config)\n",
    "analyzer = KeyPaperAnalyzer(loader, config)\n",
    "\n",
    "try:\n",
    "    ids = analyzer.search_terms(SEARCH_QUERY, sort=SORT_MOST_CITED)\n",
    "    analyzer.analyze_papers(ids, SEARCH_QUERY)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    analyzer.teardown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Pubmed uses neo4j for now \n",
    "loader = PubmedLoader(config)\n",
    "loader.values = analyzer.loader.values\n",
    "\n",
    "with loader.conn.cursor() as cursor:\n",
    "    cursor.execute(f\"\"\"\n",
    "    WITH vals(pmid) AS (VALUES {loader.values})\n",
    "    SELECT pmid INTO temporary table TEMP_PMIDS FROM vals;\n",
    "    SELECT C.pmid_out, C.pmid_in, date_part('year', P.date)\n",
    "    FROM TEMP_PMIDS T\n",
    "    JOIN PMCitations C\n",
    "    ON C.pmid_in = T.pmid\n",
    "    JOIN PMPublications P\n",
    "    ON C.pmid_out = P.pmid;\n",
    "    \"\"\")\n",
    "    data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "citations = pd.DataFrame(data, columns=['id_out', 'id_in', 'year'])\n",
    "citations['id_in'] = citations['id_in'].astype(str)\n",
    "citations['id_out'] = citations['id_out'].astype(str)\n",
    "citations['year'] = citations['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year, max_year = int(citations['year'].min()), int(citations['year'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "pr = {}\n",
    "\n",
    "for year in range(min_year + 1, max_year + 1):\n",
    "    G = analyzer.build_citation_graph(citations[citations['year'] <= year])\n",
    "    pr[year] = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_df = pd.concat([pd.Series(v) for v in pr.values()], axis=1, sort=True)\n",
    "pr_df = pr_df.rename(columns=dict(enumerate(pr.keys()))).reset_index().rename(columns={'index': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_df = pr_df.fillna(0)\n",
    "pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for index, row in pr_df.iterrows():\n",
    "    paper_year = analyzer.df[analyzer.df['id'] == row['id']]['year'].values\n",
    "    if len(paper_year) > 0:\n",
    "        years = range(paper_year[0], max_year + 1)\n",
    "        x = [year - paper_year[0] for year in years]\n",
    "        y = [row[year] if year in row else 0 for year in years]\n",
    "        plt.plot(years, y)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of New Topics in Evolution Diagram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)\n",
    "show(plotter.topic_evolution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prev = 2014\n",
    "now = 2019\n",
    "\n",
    "ct = pd.crosstab(analyzer.evolution_df[prev], analyzer.evolution_df[now])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_ct = ct.to_numpy() / ct.to_numpy().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic is considered as new if at least THRESHOLD values come from component '-1'\n",
    "THRESHOLD = 0.5\n",
    "new_topics = ct.columns[percentage_ct[0, :] > THRESHOLD].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_reversed = analyzer.G.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Node should have at least EXPLAIN_THRESHOLD papers in DFS tree\n",
    "EXPLAIN_THRESHOLD = 0.4\n",
    "\n",
    "for topic in new_topics:\n",
    "    new_ids = analyzer.df[np.logical_and(analyzer.df['comp'] == topic,\n",
    "                                         analyzer.df['year'] > prev)]['id'].values\n",
    "    print(f'Topic {topic}')\n",
    "    for pid in new_ids:\n",
    "        if pid in G_reversed.nodes():\n",
    "            nodes = []\n",
    "            for node in nx.dfs_tree(G_reversed, source=str(pid)):\n",
    "                sel = analyzer.df[analyzer.df['id'] == node]\n",
    "                if len(sel) > 0 and sel['comp'].values[0] == topic:\n",
    "                    nodes.append(node)\n",
    "            if len(nodes) >= EXPLAIN_THRESHOLD * len(new_ids):\n",
    "                print(pid, len(nodes) / len(new_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "loader = PubmedLoader(config)\n",
    "loader.values = analyzer.loader.values\n",
    "query = re.sub(loader.VALUES_REGEX, loader.values, '''\n",
    "SELECT CAST(C.pmid_out AS TEXT), CAST(C.pmid_in AS TEXT)\n",
    "FROM PMCitations C\n",
    "JOIN (VALUES $VALUES$) AS CT(pmid) ON (C.pmid_in = CT.pmid) OR (C.pmid_out = CT.pmid);\n",
    "''')\n",
    "\n",
    "with loader.conn.cursor() as cursor:\n",
    "    cursor.execute(query)\n",
    "\n",
    "    cit_df = pd.DataFrame(cursor.fetchall(), columns=['id_out', 'id_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = set(cit_df['id_out'].values) | set(cit_df['id_in'].values)\n",
    "papers = [int(pid) for pid in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.values = ', '.join(['({})'.format(i) for i in sorted(papers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from papers.utils import tokenize, build_corpus, vectorize, lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = build_corpus(analyzer.pub_df)\n",
    "tfidf, vectorizer = vectorize(corpus, n_words=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering - Cosine Similarity\n",
    "\n",
    "Looks like distances are almost equal, possible curse of dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "sim = cosine_similarity(tfidf)\n",
    "euclidean = euclidean_distances(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(euclidean.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering\n",
    "\n",
    "Latent Dirichlet Allocation + Perplexity-based Selection of Optimal Amoount of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "SEED = 20190816\n",
    "\n",
    "def lda_optimal_topics(counts):    \n",
    "    # Store results during optimization to avoid re-calculation of perplexity\n",
    "    # for the same number of components\n",
    "    results = {}\n",
    "\n",
    "    def lda_evaluate(counts, n_comps, n_times=10):\n",
    "        actual = int(round(n_comps))\n",
    "        logging.info(f'Trying {n_comps} - {actual}')\n",
    "\n",
    "        if actual not in results:\n",
    "            p = 0\n",
    "            for _ in range(n_times):\n",
    "                lda = LatentDirichletAllocation(n_components=actual, random_state=SEED)\n",
    "                lda.fit(counts)\n",
    "                p += lda.perplexity(counts)\n",
    "            results[actual] = p / n_times\n",
    "\n",
    "        return results[actual]\n",
    "\n",
    "    upper_bound = min(counts.shape[0], 100)\n",
    "    res = minimize_scalar(lambda x: lda_evaluate(counts, x), bounds=(1, upper_bound), method='bounded',\n",
    "                          options={'xatol': 1, 'maxiter': 10})\n",
    "    \n",
    "    opt = int(round(res.x))\n",
    "    logging.info(f'Found {opt} topics')\n",
    "    topics, lda = lda_topics(counts, n_topics=opt)\n",
    "    \n",
    "    return topics, lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = build_corpus(analyzer.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, vectorizer = vectorize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, lda = lda_optimal_topics(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from papers.utils import explain_lda_topics\n",
    "\n",
    "explain_lda_topics(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-citation Clusters vs Text-based Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_labels = list([c for c in set(analyzer.df['comp'].values) if c >= 0])\n",
    "text_labels = list(set(labels))\n",
    "\n",
    "cm = np.zeros((len(graph_labels), len(text_labels)))\n",
    "for i in range(len(graph_labels)):\n",
    "    for j in range(len(text_labels)):\n",
    "        ci, cj = graph_labels[i], text_labels[j]\n",
    "        cm[i, j] = np.logical_and(analyzer.df['comp'] == ci, labels == cj).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Evolution based on LDA clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from papers.utils import explain_lda_topics\n",
    "\n",
    "def lda_topic_evolution(df, step=5, min_papers=0, current=0):\n",
    "    min_year = int(df['year'].min())\n",
    "    max_year = int(df['year'].max())\n",
    "    year_range = list(np.arange(max_year, min_year - 1, step=-step).astype(int))\n",
    "\n",
    "    # Cannot analyze evolution\n",
    "    if len(year_range) < 2:\n",
    "        logging.info(f'Year step is too big to analyze evovution of topics in {min_year} - {max_year}')\n",
    "        return None, None\n",
    "\n",
    "    logging.info(f'Studying evolution of topics in {min_year} - {max_year}')\n",
    "    logging.info(f\"Years when topics are studied: {', '.join([str(year) for year in year_range])}\")\n",
    "\n",
    "    years_processed = 0\n",
    "    evolution_series = []\n",
    "    partition = {}\n",
    "    explanation = {}\n",
    "    for i, year in enumerate(year_range):\n",
    "        valid = df[df['year'] <= year]\n",
    "        corpus = build_corpus(valid)\n",
    "        counts, vectorizer = vectorize(corpus, terms=analyzer.terms, n_words=len(valid) * 3)\n",
    "        topics, lda = lda_optimal_topics(counts)\n",
    "        explanation[year] = explain_lda_topics(lda, vectorizer, n_top_words=20)\n",
    "        partition[year] = dict(zip(list(valid['id'].values), topics.argmax(axis=1)))\n",
    "        partition[year], _ = analyzer.merge_components(partition[year])\n",
    "                  \n",
    "    return partition, explanation, year_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p, explanation, year_range = lda_topic_evolution(analyzer.pub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.evolution_year_range = year_range\n",
    "\n",
    "evolution_series = []\n",
    "for year in year_range:\n",
    "    evolution_series.append(pd.Series(p[year]))\n",
    "\n",
    "analyzer.evolution_df = pd.concat(evolution_series, axis=1).rename(\n",
    "    columns=dict(enumerate(year_range)))\n",
    "analyzer.evolution_df = analyzer.evolution_df[list(reversed(list(analyzer.evolution_df.columns)))]\n",
    "\n",
    "# Assign -1 to articles that do not belong to any cluster at some step\n",
    "analyzer.evolution_df = analyzer.evolution_df.fillna(-1.0).astype(int)\n",
    "\n",
    "analyzer.evolution_df = analyzer.evolution_df.reset_index().rename(columns={'index': 'id'})\n",
    "analyzer.evolution_df['id'] = analyzer.evolution_df['id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.components = set(p[2019].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.evolution_kwds = {}\n",
    "for year in year_range:\n",
    "    analyzer.evolution_kwds[year] = {c: [el[1] for el in words] for c, words in explanation[year].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)\n",
    "show(plotter.topic_evolution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Clustering\n",
    "\n",
    "Make clustering more stable by aggregating output of several runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "CG = nx.Graph()\n",
    "nodes = list(analyzer.CG.nodes())\n",
    "n_nodes = len(nodes)\n",
    "N_TIMES = 10\n",
    "THRESHOLD = 0.66\n",
    "for t in range(N_TIMES):\n",
    "    p = community.best_partition(analyzer.CG, randomize=True)\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(i + 1, n_nodes):\n",
    "            v, u = nodes[i], nodes[j]\n",
    "            if p[v] == p[u]:\n",
    "                w = CG.edges[v, u]['weight'] if CG.has_edge(v, u) else 0\n",
    "                CG.add_edge(v, u, weight=w+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_edges = []\n",
    "\n",
    "for e in CG.edges(data=True):\n",
    "    v, u, data = e\n",
    "    w = data['weight']\n",
    "    if w > N_TIMES * THRESHOLD:\n",
    "        reliable_edges.append((v, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reliable_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CG.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG_reliable = CG.edge_subgraph(reliable_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG_reliable.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.CG.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliable_partition = {}\n",
    "\n",
    "n_comps = 0\n",
    "for i, comp in enumerate(nx.connected_components(CG_reliable)):\n",
    "    n_comps += 1\n",
    "    for v in comp:\n",
    "        reliable_partition[v] = i\n",
    "        \n",
    "# Nodes without reliable links\n",
    "for v in CG.nodes():\n",
    "    if v not in reliable_partition:\n",
    "        reliable_partition[v] = n_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = community.best_partition(CG, partition=reliable_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comps = len(set(init))\n",
    "\n",
    "# Nodes without reliable links\n",
    "for v in analyzer.CG.nodes():\n",
    "    if v not in init:\n",
    "        init[v] = n_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = community.best_partition(analyzer.CG, partition=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(reliable_partition.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(community.best_partition(analyzer.CG).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MST with PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "pr = nx.pagerank(analyzer.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undirected = analyzer.G.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, u in G_undirected.edges():\n",
    "    G_undirected[v][u]['weight'] = pr[v] - pr[u] - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = list(nx.connected_components(G_undirected))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_comp = G_undirected.subgraph(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = nx.minimum_spanning_tree(G_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_tree(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.nx_agraph.to_agraph(T)\n",
    "A.layout('sfdp')\n",
    "A.draw('mst.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation Graph for a Certain Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR = 'Horvath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df = analyzer.df[analyzer.df['authors'].apply(lambda x: AUTHOR in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_G = analyzer.G.subgraph(author_df['id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pygraphviz as pgv\n",
    "from IPython.display import Image\n",
    "\n",
    "def draw(graph):\n",
    "    return Image(nx.nx_agraph.to_agraph(graph).draw(format='png', prog='sfdp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(author_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
