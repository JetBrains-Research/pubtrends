{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Extraction\n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Spacy: `conda install -c conda-forge spacy`\n",
    "2. English corpus for spacy: `python -m spacy download en_core_web_sm`\n",
    "3. text2num: `pip install text2num`\n",
    "\n",
    "### Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source Selection\n",
    "\n",
    "### Option 1: Use Search Terms \n",
    "\n",
    "1. Define the `SEARCH_QUERY` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Set `USE_PUBMED_IMPORT` to `False`.\n",
    "\n",
    "### Option 2: Import Search Results from Pubmed \n",
    "\n",
    "1. Set `USE_PUBMED_IMPORT` to `True`.\n",
    "2. Search for terms of interest on the Pubmed website. Use Save with Selection=All Results and Format=PMID to obtain a .txt file with PMIDs of all papers that were found.\n",
    "3. Use widget below to upload a file with IDs from Pubmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Use Search Terms\n",
    "SEARCH_QUERY = 'protein structure'\n",
    "\n",
    "# Option 2 - Import Search Results from Pubmed\n",
    "USE_PUBMED_IMPORT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PMIDs from Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "if USE_PUBMED_IMPORT:\n",
    "    w = widgets.FileUpload(accept='.txt', multiple=False)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Publication Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T11:13:45.558630Z",
     "start_time": "2019-05-20T11:13:20.941831Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pysrc.papers.config import PubtrendsConfig\n",
    "from pysrc.papers.progress import Progress\n",
    "from pysrc.papers.db.pm_loader import PubmedLoader\n",
    "from pysrc.papers.db.ss_loader import SemanticScholarLoader\n",
    "from pysrc.papers.analyzer_experimental import ExperimentalAnalyzer\n",
    "from pysrc.papers.plotter_experimental import ExperimentalPlotter\n",
    "from pysrc.papers.utils import SORT_MOST_CITED, SORT_MOST_RELEVANT, SORT_MOST_RECENT, cut_authors_list\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "SEARCH_SORT = SORT_MOST_CITED\n",
    "SEARCH_PAPERS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:33.794284Z",
     "start_time": "2019-05-20T11:13:45.591588Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "loader = PubmedLoader(config)\n",
    "progress = Progress(1)\n",
    "loader.set_progress(progress)\n",
    "try:\n",
    "    if USE_PUBMED_IMPORT:\n",
    "        filename = list(w.value.keys())[0]\n",
    "        ids = [int(chunk) for chunk in w.value[filename]['content'].decode('utf-8').split('\\r\\n')]\n",
    "    else:\n",
    "        ids = loader.search(SEARCH_QUERY, limit=SEARCH_PAPERS, sort=SEARCH_SORT)\n",
    "    pub_df = loader.load_publications(ids)\n",
    "finally:\n",
    "    loader.close_connection()\n",
    "    logging.info('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function for Metric Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "from spacy import displacy\n",
    "from text_to_num import alpha2digit\n",
    "\n",
    "REAL_NUMBER = re.compile(r'-?[\\d]+(\\.[\\d]+)?')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Test Grobid Quantities\n",
    "\n",
    "def extract_metrics(abstract_text, visualize_dependencies=False):\n",
    "    \"\"\"\n",
    "    Parses abstract and returns a dict of numbers with nouns that could be suitable as a metric.\n",
    "    :return list of tuples (sentence, [metrics]), where metrics is a list of tuples (number, [nouns], sentence_number)\n",
    "    \"\"\"   \n",
    "    metrics = {}\n",
    "    sentences = {}\n",
    "    # Convert textual numbers to digits (three -> 3)\n",
    "    abstract_text = alpha2digit(abstract_text, 'en')\n",
    "    # Split text into sentences and find numbers in sentences\n",
    "    doc = spacy_en(abstract_text)\n",
    "    for idx, sent in enumerate(doc.sents):\n",
    "        sentences[idx] = sent.text\n",
    "        for token in sent:\n",
    "#             print(token.text, token.pos_, list(token.ancestors))\n",
    "            if REAL_NUMBER.fullmatch(token.text):\n",
    "                value = float(token.text) if '.' in token.text else int(token.text)\n",
    "                # Analyze children and siblings, then ancestors if first was not enough\n",
    "                # TODO: is there a better way?\n",
    "                # TODO: use close nouns as a fallback when it is hard to find a dependency?\n",
    "                # TODO: expand nouns with adjectives or other nouns? (rate -> information transfer rate)\n",
    "                candidates = list(token.children) + list(token.head.children) + [token.head]\n",
    "                # Explicitly ignore % (TODO: use as a unit of measurement)\n",
    "                nouns = [token.text for token in filter(lambda t: t.text != '%' and t.pos_ == 'NOUN', candidates)]\n",
    "                if not nouns:\n",
    "                    for t in token.ancestors:\n",
    "                        if t.text != '%' and t.pos_ == 'NOUN':\n",
    "                            nouns = [t.text]\n",
    "                            break\n",
    "                for noun in nouns:\n",
    "                    if noun not in metrics:\n",
    "                        metrics[noun] = []\n",
    "                    metrics[noun].append((value, idx))\n",
    "    if visualize_dependencies:\n",
    "        displacy.render(list(doc.sents), style=\"dep\", jupyter=True)\n",
    "    return metrics, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo `extract_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SENTENCE = \"\"\"Over brief training periods of 3-24 min, four patients then used these signals \n",
    "              to master closed-loop control and to achieve success rates of 74-100% in a \n",
    "              one-dimensional binary task.\"\"\"\n",
    "\n",
    "print(SENTENCE)\n",
    "metrics, _ = extract_metrics(SENTENCE, visualize_dependencies=True)\n",
    "for word, occasions in metrics.items():\n",
    "    print(f\"{word}: {occasions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's apply metric extraction to Pubmed papers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude papers with unwanted terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE = ['\\becog', 'electrocorticograph', '\\bhybrid', 'fNIRS', '\\bSSVEP']\n",
    "EXCLUDE_REGEX = '|'.join(EXCLUDE)\n",
    "EXCLUDE_MASK = np.logical_not(pub_df.abstract.str.contains(EXCLUDE_REGEX, flags=re.IGNORECASE, regex=True))\n",
    "\n",
    "processed_pub_df = pub_df[EXCLUDE_MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ids = set(pub_df['id'])\n",
    "processed_ids = set(processed_pub_df['id'])\n",
    "diff = original_ids - processed_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the list of excluded papers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_df[pub_df['id'].isin(diff)][['id', 'abstract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Metric Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow, currently moved out of the class to speed up fixing & rerunning the code of MetricExtractor\n",
    "\n",
    "metrics_data = []\n",
    "for _, data in processed_pub_df.iterrows():\n",
    "    paper_metrics_data = [data['id'], *extract_metrics(data['abstract'])]\n",
    "    metrics_data.append(paper_metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricExtractor:\n",
    "    def __init__(self, metrics_data):\n",
    "        self.metrics_df = pd.DataFrame(metrics_data, columns=['ID', 'Metrics', 'Sentences'])\n",
    "        \n",
    "    def get_top_metrics(self, number=20):\n",
    "        metrics_counter = Counter()\n",
    "        for metric_dict in self.metrics_df['Metrics']:\n",
    "            for metric, occasions in metric_dict.items():\n",
    "                metrics_counter[metric] += len(occasions)\n",
    "        return metrics_counter.most_common(number)\n",
    "    \n",
    "    def get_metric_values(self, *metrics, min_value=None, max_value=None, detailed=False):\n",
    "        values = []\n",
    "        for _, data in self.metrics_df.iterrows():\n",
    "            metric_dict = data['Metrics']\n",
    "            sentences = data['Sentences']\n",
    "        \n",
    "            for metric in metrics:\n",
    "                if metric in metric_dict:\n",
    "                    for value, sentence_number in metric_dict[metric]:\n",
    "                        if min_value and value < min_value or max_value and value > max_value:\n",
    "                            continue\n",
    "                        if detailed:\n",
    "                            sentence = sentences[sentence_number]\n",
    "                            values.append([data['ID'], value, sentence])\n",
    "                        else:\n",
    "                            values.append(value)\n",
    "        if detailed:\n",
    "            return pd.DataFrame(values, columns=['PMID', ', '.join(metrics), 'Sentence'])\n",
    "        return values\n",
    "    \n",
    "    def filter_papers(self, metrics):\n",
    "        \"\"\"\n",
    "        :param metrics - list of tuples ([list of keywords], min_value, max_value)\n",
    "               e.g. (['subjects', 'participants'], 5, None)\n",
    "        :return list of PMIDs\n",
    "        \"\"\"\n",
    "        selection = []\n",
    "        for _, data in self.metrics_df.iterrows():\n",
    "            suitable = True\n",
    "            metric_dict = data['Metrics']\n",
    "            \n",
    "            for metric in metrics:\n",
    "                metric_suitable = False\n",
    "                words, min_value, max_value = metric\n",
    "                \n",
    "                for word in words:\n",
    "                    if word in metric_dict:\n",
    "                        for value, _ in metric_dict[word]:\n",
    "                            if min_value and value < min_value or max_value and value > max_value:\n",
    "                                continue\n",
    "                            metric_suitable = True\n",
    "                    if metric_suitable:\n",
    "                        break\n",
    "                \n",
    "                suitable &= metric_suitable\n",
    "                    \n",
    "            if suitable:\n",
    "                selection.append(data['ID'])\n",
    "        return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = MetricExtractor(metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See an example of extraction result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me.metrics_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(me.get_top_metrics(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a metric to show distribution and abstract fragments where it was mentioned.\n",
    "\n",
    "Currently synonyms are not processed, so feel free to use a tuple of words that correspond to the same metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = ('subjects', 'patients', 'participants', 'volunteers')\n",
    "METRIC_TEXT = ', '.join(METRIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of selected `METRIC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_subjects = me.get_metric_values(*METRIC)\n",
    "plt.hist(num_subjects)\n",
    "plt.xlabel(METRIC_TEXT)\n",
    "plt.ylabel('Number of papers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me.get_metric_values(*METRIC, detailed=True).sort_values(METRIC_TEXT, ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter papers based on desired metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    (['subjects', 'patients', 'participants', 'volunteers'], 10, None),\n",
    "    (['accuracy'], 65, None),\n",
    "]\n",
    "\n",
    "selection = me.filter_papers(METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_df[pub_df.id.isin(selection)][['id', 'abstract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "\n",
    "Other libraries for extraction of metrics/quantities: quantulum, grobid-quantities.\n",
    "\n",
    "Dependencies:\n",
    "  * stemming `pip install stemming`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantulum3 import parser\n",
    "\n",
    "sent = 'One subject participated and reached 95.4 % mean online accuracy after six runs of 40 trials.'\n",
    "parser.parse(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}