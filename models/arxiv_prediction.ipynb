{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Arxiv to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keypaper.arxiv_loader import ArxivLoader\n",
    "from keypaper.arxiv_analyzer import ArxivAnalyzer\n",
    "from keypaper.config import PubtrendsConfig\n",
    "import html\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "analyzer = ArxivAnalyzer(ArxivLoader(config))\n",
    "SEARCH_TERMS = []\n",
    "log = analyzer.launch(*SEARCH_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.df['total'].hist(bins=[0, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_not_null = analyzer.df[analyzer.df.total > 0]\n",
    "arxiv_df_null = analyzer.df[analyzer.df.total == 0].head(n=10000)\n",
    "arxiv_df = pd.concat([arxiv_df_not_null, arxiv_df_null]).drop(columns=['crc32id', 'aux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df['total'].hist(bins=[0, 5, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only 1975-2015 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years = arxiv_df[arxiv_df.year <= 2015][1975 <= arxiv_df.year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypaper.utils import split_df_list\n",
    "def popular_authors(df, n=20, current=0, task=None):\n",
    "    author_stats = df[['authors']].copy()\n",
    "    author_stats['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_stats.dropna(subset=['authors'], inplace=True)\n",
    "\n",
    "    author_stats = split_df_list(author_stats, target_column='authors', separator=', ')\n",
    "    author_stats.rename(columns={'authors': 'author'}, inplace=True)\n",
    "\n",
    "    author_stats = author_stats.groupby(['author']).size().reset_index(name='counts')\n",
    "\n",
    "    author_stats = author_stats.groupby('author').agg({'counts': ['sum']}).reset_index()\n",
    "\n",
    "    author_stats.columns = author_stats.columns.droplevel(level=1)\n",
    "    author_stats.columns = ['author', 'sum']\n",
    "    author_stats = author_stats.sort_values(by=['sum'], ascending=False)\n",
    "\n",
    "    return author_stats.head(n=n)\n",
    "\n",
    "def popular_journals(df, n=20):\n",
    "    journal_stats = df.groupby(['journal']).size().reset_index(name='counts')\n",
    "    \n",
    "    journal_stats['journal'].replace('', np.nan, inplace=True)\n",
    "    journal_stats.dropna(subset=['journal'], inplace=True)\n",
    "\n",
    "    journal_stats = journal_stats.groupby('journal').agg({'counts': ['sum']}).reset_index()\n",
    "\n",
    "    journal_stats.columns = journal_stats.columns.droplevel(level=1)\n",
    "    journal_stats.columns = ['journal', 'sum']\n",
    "\n",
    "    journal_stats = journal_stats.sort_values(by=['sum'], ascending=False)\n",
    "\n",
    "    return journal_stats.head(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = popular_authors(arxiv_df, n=30000)[['author', 'sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors['author'] = authors['author'].apply(lambda author: author.lower().replace('.', ''))\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_names = pd.read_csv('authorsNames.tsv', sep='\\t')\n",
    "authors_features = pd.read_csv('authorsFeatures.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_year = popular_authors(arxiv_df_years, n=30000)[['author', 'sum']]\n",
    "authors_year['author'] = authors_year['author'].apply(lambda author: author.lower().replace('.', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_authors_year = pd.merge(authors_year, authors_names, on='author', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {merged_authors_year.shape[0]} authors in file out of {authors_year.shape[0]} most popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many papers from Arxiv (1975-2015) have at least one author, that contains in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years = arxiv_df[arxiv_df.year <= 2015][1975 <= arxiv_df.year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_authors = set(authors_names['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_author_detection(row):\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "        \n",
    "    is_detected = list(map(lambda author: True if author.lower().replace('.', '') in set_authors else False,\n",
    "                                    authors_list))\n",
    "    return is_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_any(row):\n",
    "    return any(row.detected_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years['detected_authors'] = arxiv_df_years.apply(lambda row: test_author_detection(row), axis=1)\n",
    "arxiv_df_years['any_detected'] = arxiv_df_years.apply(lambda row: count_any(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_papers_with_detected_authors = arxiv_df_years[arxiv_df_years['any_detected']].shape[0]\n",
    "n_papers = arxiv_df_years.shape[0]\n",
    "print(f'Found {n_papers_with_detected_authors} papers with detected authors (at least one of them) out of {n_papers} papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_features['name'] = authors_names\n",
    "authors_features.set_index('name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_of_needed_authors = authors_features[authors_features.index.isin(merged_authors_year.author)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = features_of_needed_authors.to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypaper.utils import lda_subtopics, explain_lda_subtopics\n",
    "n_topics = 100\n",
    "topic_names = [f'topic{i}' for i in range(n_topics)]\n",
    "topics, lda, vectorizer = lda_subtopics(arxiv_df_years, n_words=1000, n_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(data=topics, columns=topic_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years = pd.concat([arxiv_df_years.reset_index(), topics_df.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = explain_lda_subtopics(lda, vectorizer, n_top_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cits = []\n",
    "for i in range(n_topics):\n",
    "    topic_cits.append(np.dot(arxiv_df_years[f'topic{i}'], arxiv_df_years['total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_after_n_years(row, n):\n",
    "    paper_year = row['year']\n",
    "    cit = 0\n",
    "    for cur_year in range(paper_year, paper_year + n):\n",
    "        if cur_year in row:\n",
    "            cit += row[cur_year]\n",
    "    return cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal_preprocess(df):\n",
    "    journals = popular_journals(df, n=10000)[['journal', 'sum']]\n",
    "    \n",
    "    journals_citations = df[['journal', 'total']].groupby(['journal'])\\\n",
    "            .agg({'total': 'sum'}).reset_index().rename(columns={'total':'journal_citations'})\n",
    "    return journals, journals_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals, journals_citations = journal_preprocess(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authors_preprocess(df):\n",
    "    authors = popular_authors(df, n=10000)[['author', 'sum']]\n",
    "    authors_papers_dict = authors.set_index('author')['sum'].to_dict()\n",
    "    \n",
    "    author_total = df[['authors', 'total']]\n",
    "    author_total['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_total.dropna(subset=['authors'], inplace=True)\n",
    "    author_total = split_df_list(author_total, target_column='authors', separator=', ')\n",
    "    authors_citations = author_total.groupby(['authors']).agg({'total': 'sum'}).reset_index()\n",
    "    authors_citations = authors_citations.loc[authors_citations['authors'] != ''].rename(columns={'authors':'author'})\n",
    "    authors_dict = authors_citations.set_index('author')['total'].to_dict()\n",
    "    return authors_papers_dict, authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_papers_dict, authors_dict = authors_preprocess(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_papers(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    papers_of_given_authors = list(map(lambda author: authors_papers_dict[author] \n",
    "                                    if author in authors_papers_dict else 1,\n",
    "                                   authors_list))\n",
    "\n",
    "    return pd.Series([mean(papers_of_given_authors), max(papers_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_citations(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    cit_of_given_authors = list(map(lambda author: authors_dict[author] if author in authors_dict else 1,\n",
    "                                    authors_list))\n",
    "\n",
    "    return pd.Series([mean(cit_of_given_authors), max(cit_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_diversity(row):\n",
    "    return np.dot(list(row[[f'topic{i}'for i in range(n_topics)]]),\n",
    "           np.log(list(row[[f'topic{i}'for i in range(n_topics)]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_citations_on_topics(row):\n",
    "    return np.dot(list(row[[f'topic{i}'for i in range(n_topics)]]), topic_cits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_authors_features(row):\n",
    "    if not row['any_detected']:\n",
    "        return row\n",
    "    \n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    authors_list = list(map(lambda author: author.lower().replace('.', ''), authors_list))\n",
    "        \n",
    "    # should be authors_features.todict() instead of features_dict but it is already too slow\n",
    "    features_of_paper_authors = pd.DataFrame(filter(lambda x: x is not None, \n",
    "                                                    map(lambda author: features_dict[author] \n",
    "                                                        if author in features_dict else None, authors_list)))\n",
    "    for feature in features_of_paper_authors:\n",
    "        row[feature + '_mean'] = features_of_paper_authors[feature].mean()\n",
    "        row[feature + '_min'] = features_of_paper_authors[feature].min()\n",
    "        row[feature + '_max'] = features_of_paper_authors[feature].max()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df2, step=5, current_year=2019):\n",
    "    df = df2.copy()\n",
    "    df['n_authors'] = df['authors'].apply(lambda authors: len(authors.split(', ')))\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['recency'] = current_year - df['year']\n",
    "    df['title_len'] = df['title'].apply(lambda title: 0 if pd.isnull(title) else len(title))\n",
    "    df['abstract_len'] = df['abstract'].apply(lambda abstract: 0 if pd.isnull(abstract) else len(abstract))\n",
    "    min_year, max_year = analyzer.min_year, analyzer.max_year\n",
    "    \n",
    "#   early citations (cumulative)\n",
    "    for i in range(1, step + 2):\n",
    "        feature_name = f'c{i}'\n",
    "        df[feature_name] = df.apply(lambda row: citations_after_n_years(row, n=i), axis=1)\n",
    "    \n",
    "#   early citations (deltas)\n",
    "    for i in range(1, step + 1):\n",
    "        df[f'd{i}'] = df[f'c{i + 1}'] - df[f'c{i}'] \n",
    "        \n",
    "    logging.info(\"Done counting early citations\")\n",
    "    \n",
    "#   how many papers were published by author/journal that published this paper \n",
    "    df = pd.merge(df, journals, on='journal', how='left').rename(columns={'sum':'journal_papers'})\n",
    "    logging.info(\"Done counting how many papers were published by the journal\")\n",
    "    \n",
    "    df['author_mean_papers'], df['author_max_papers'] = df['authors']\\\n",
    "        .apply(lambda authors: get_authors_papers(authors))\n",
    "    \n",
    "    logging.info(\"Done counting how many papers were published by each of the authors\")\n",
    "\n",
    "#   how many citations journal that published this paper has\n",
    "    df = pd.merge(df, journals_citations, on='journal', how='left')\n",
    "    \n",
    "    logging.info(\"Done counting how cited each journal was\")\n",
    "\n",
    "#   how many citations author of this paper got (mean/max)\n",
    "    df['author_mean_citations'], df['author_max_citations'] = df['authors']\\\n",
    "        .apply(lambda authors: get_authors_citations(authors))\n",
    "    logging.info(\"Done counting how cited each author was\")\n",
    "    \n",
    "#   topics\n",
    "    df['diversity'] = df.apply(lambda row: count_diversity(row), axis=1)\n",
    "    df['citations_on_topics'] = df.apply(lambda row: count_citations_on_topics(row), axis=1)\n",
    "    logging.info(\"Done adding features based on topics\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_arxiv_df_years = preprocess(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ~250 features about authors and venues based on information from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_author_features = []\n",
    "for feature in author_columns:\n",
    "    final_author_features += [feature + '_mean', feature + '_min', feature + '_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is super slow\n",
    "authors_features_to_add = arxiv_df_years[['any_detected', 'authors']]\\\n",
    "                            .apply(lambda row: add_all_authors_features(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_features_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_arxiv_df_years = pd.concat([authors_features_to_add.drop(columns=['any_detected', 'authors']), \n",
    "                                         preprocessed_arxiv_df_years], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_arxiv_df_years.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with different features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, features, target, model=LinearRegression(), dropna=False):\n",
    "    if dropna:\n",
    "        df.dropna(inplace=True)\n",
    "    else:\n",
    "        df.fillna(0, inplace=True)\n",
    "    train_validate = df[df.recency > step + 1][features + [target]]\n",
    "    train = train_validate[train_validate.recency > 11] \n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "    \n",
    "    X = train.iloc[:,:-1]\n",
    "    y = train.iloc[:,-1]\n",
    "    X_validate = validate.iloc[:,:-1]\n",
    "    y_validate = validate.iloc[:,-1]\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_validate = scaler.transform(X_validate)\n",
    "\n",
    "    reg = model.fit(X, y)\n",
    "    \n",
    "    print(f\"R^2 train: {reg.score(X, y)} validate: {reg.score(X_validate, y_validate)}\")\n",
    "    print(f\"RMSE train: {sqrt(mse(reg.predict(X), y))} validate: {sqrt(mse(reg.predict(X_validate), y_validate))}\")\n",
    "    \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_influencers(df, features, reg):\n",
    "    maxcoef = np.argsort(-np.abs(reg.coef_))\n",
    "    coef = reg.coef_[maxcoef]\n",
    "    for i in range(0, 10):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict c6 given c1-c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'citations_on_topics' is linear combination of topic distribution ('topic_names' columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_features = ['diversity', 'n_authors', 'recency', 'title_len', 'abstract_len', 'citations_on_topics']  # + topic_names\n",
    "# calculated\n",
    "authors_journals = ['journal_papers','author_mean_papers', 'author_max_papers', \n",
    "                      'journal_citations', 'author_mean_citations', 'author_max_citations']\n",
    "\n",
    "basic_features += authors_journals\n",
    "features = (basic_features +['c1', 'c2', 'c3', 'c4', 'c5'])\n",
    "target = 'c6'\n",
    "reg = predict(preprocessed_arxiv_df_years, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_influencers(preprocessed_arxiv_df_years, features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = []\n",
    "# for a, b in zip(list(y_validate), list(reg.predict(X_validate))):\n",
    "#     if a != 0:\n",
    "#         x.append(b - a)\n",
    "# x = temp\n",
    "# plt.hist(x, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict d5 given c1, d1-d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = basic_features + ['c1', 'd1', 'd2', 'd3', 'd4'] \n",
    "features += authors_journals\n",
    "target = 'd5'\n",
    "reg = predict(preprocessed_arxiv_df_years, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_influencers(preprocessed_arxiv_df_years, features, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict c_i without any early citations info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    print('\\n' + f\"predict citations after {i} years after paper was published\")\n",
    "    features = basic_features + authors_journals\n",
    "    target = f'c{i}'\n",
    "    reg = predict(preprocessed_arxiv_df_years, features, target)\n",
    "    print_top_influencers(preprocessed_arxiv_df_years, features, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with regularisation (L1 or L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lasso regularisation; target {target}\")\n",
    "Ls = predict(preprocessed_arxiv_df_years, features, target, model=LassoCV(cv=5))\n",
    "print_top_influencers(preprocessed_arxiv_df_years, features, Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ridge regularisation; target {target}\")\n",
    "Rr = predict(preprocessed_arxiv_df_years, features, target, model=RidgeCV(cv=5))\n",
    "print_top_influencers(preprocessed_arxiv_df_years, features, Rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using all features extracted from file about authors and venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    print('\\n' + f\"predict citations after {i} years after paper was published\")\n",
    "    features = basic_features + final_author_features\n",
    "    target = f'c{i}'\n",
    "    reg = predict(preprocessed_arxiv_df_years, features, target, LassoCV(cv=5, tol=0.1), dropna=True)\n",
    "    print_top_influencers(preprocessed_arxiv_df_years, features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
