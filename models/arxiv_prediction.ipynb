{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Arxiv to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from keypaper.arxiv_loader import ArxivLoader\n",
    "from keypaper.arxiv_analyzer import ArxivAnalyzer\n",
    "from keypaper.config import PubtrendsConfig\n",
    "import html\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PubtrendsConfig(test=False)\n",
    "analyzer = ArxivAnalyzer(ArxivLoader(config))\n",
    "SEARCH_TERMS = []\n",
    "log = analyzer.launch(*SEARCH_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.df['total'].hist(bins=[0, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_not_null = analyzer.df[analyzer.df.total > 0]\n",
    "arxiv_df_null = analyzer.df[analyzer.df.total == 0].head(n=10000)\n",
    "arxiv_df = pd.concat([arxiv_df_not_null, arxiv_df_null]).drop(columns=['crc32id', 'aux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df['total'].hist(bins=[0, 10, 20, 30, 40, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter only 1975-2015 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years = arxiv_df[arxiv_df.year <= 2015][1975 <= arxiv_df.year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypaper.utils import split_df_list\n",
    "def popular_authors(df, n=None):\n",
    "    author_stats = df[['authors']].copy()\n",
    "    author_stats['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_stats.dropna(subset=['authors'], inplace=True)\n",
    "\n",
    "    author_stats = split_df_list(author_stats, target_column='authors', separator=', ')\n",
    "    author_stats.rename(columns={'authors': 'author'}, inplace=True)\n",
    "\n",
    "    author_stats = author_stats.groupby(['author']).size().reset_index(name='counts')\n",
    "\n",
    "    author_stats = author_stats.groupby('author').agg({'counts': ['sum']}).reset_index()\n",
    "\n",
    "    author_stats.columns = author_stats.columns.droplevel(level=1)\n",
    "    author_stats.columns = ['author', 'sum']\n",
    "    author_stats = author_stats.sort_values(by=['sum'], ascending=False)\n",
    "\n",
    "    if n:\n",
    "        return author_stats.head(n=n)\n",
    "    else:\n",
    "        return author_stats\n",
    "\n",
    "def popular_journals(df, n=None):\n",
    "    journal_stats = df.groupby(['journal']).size().reset_index(name='counts')\n",
    "    \n",
    "    journal_stats['journal'].replace('', np.nan, inplace=True)\n",
    "    journal_stats.dropna(subset=['journal'], inplace=True)\n",
    "\n",
    "    journal_stats = journal_stats.groupby('journal').agg({'counts': ['sum']}).reset_index()\n",
    "\n",
    "    journal_stats.columns = journal_stats.columns.droplevel(level=1)\n",
    "    journal_stats.columns = ['journal', 'sum']\n",
    "\n",
    "    journal_stats = journal_stats.sort_values(by=['sum'], ascending=False)\n",
    "\n",
    "    if n:\n",
    "        return author_stats.head(n=n)\n",
    "    else:\n",
    "        return journal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years['authors'] = arxiv_df_years['authors'].apply(lambda authors: authors.lower().replace('.', ' '))\n",
    "arxiv_df_years['authors'] = arxiv_df_years['authors'].apply(lambda authors: ' '.join(authors.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years['journal'] = arxiv_df_years['journal'].apply(lambda journal: journal.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topics to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keypaper.utils import lda_subtopics, explain_lda_subtopics\n",
    "n_topics = 100\n",
    "topic_names = [f'topic{i}' for i in range(n_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year, end_year = 1995, 2016 # end year exclusive\n",
    "topics_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topic(row):\n",
    "    index = np.argmax(np.array(row))\n",
    "    probs.append(row[f'topic{index}'])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Find topics for year {year}\")\n",
    "    topics, lda, vectorizer = lda_subtopics(arxiv_df_years[arxiv_df_years.year <= year], \n",
    "                                                        n_words=1000, n_topics=n_topics)\n",
    "    \n",
    "    topics_df = pd.DataFrame(data=topics, columns=topic_names)\n",
    "    topics_df.index = arxiv_df_years[arxiv_df_years.year <= year].index\n",
    "    topics_df['main_topic'] = topics_df[topic_names].apply(lambda row: find_topic(row), axis=1)\n",
    "    \n",
    "    explanations = explain_lda_subtopics(lda, vectorizer, n_top_words=20)\n",
    "    \n",
    "    topics_info[year] = {'topics': topics_df, 'lda': lda, 'vectorizer': vectorizer, 'explanations': explanations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count citations before each year (including this year)\n",
    "def before_year_citations(df):\n",
    "    df[f'before_{start_year}'] = df[start_year]\n",
    "    for year in range(start_year + 1, end_year):\n",
    "        df[f'before_{year}'] = df[f'before_{year - 1}'] + df[year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_year_citations(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_citations = {}\n",
    "topic_ranks = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Counting topics citations for year {year}\")\n",
    "    # count citations of each topic\n",
    "    topic_citations[year] = []\n",
    "    for i in range(n_topics):\n",
    "        p_topic_i = topics_info[year]['topics'][f'topic{i}']\n",
    "        \n",
    "        cit_documents = arxiv_df_years[arxiv_df_years.year <= year][f'before_{year}']\n",
    "        assert p_topic_i.shape[0] == cit_documents.shape[0]\n",
    "        \n",
    "        topic_citations[year].append(np.dot(p_topic_i, cit_documents))\n",
    "    topic_ranks[year] = pd.Series(topic_citations[year]).rank(ascending=False, method='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_rank(row):\n",
    "    df = topics_info[row.year]['topics']\n",
    "    main_topic = df.loc[row.name,:]['main_topic']\n",
    "    topics_rank = topic_ranks[row.year][main_topic]\n",
    "    return topics_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years['topic_rank'] = arxiv_df_years[arxiv_df_years.year >= 1995].apply(lambda row: get_topic_rank(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diversity(row):\n",
    "    topic_probs = topics_info[row.year]['topics'].loc[row.name,:][topic_names]\n",
    "    return np.dot(list(topic_probs), np.log(list(topic_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years['diversity'] = arxiv_df_years[arxiv_df_years.year >= 1995].apply(lambda row: get_diversity(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_years.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (aurhors and journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from scholarmetrics import hindex, gindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_after_n_years(row, n):\n",
    "    paper_year = row['year']\n",
    "    cit = 0\n",
    "    for cur_year in range(paper_year, paper_year + n):\n",
    "        if cur_year in row:\n",
    "            cit += row[cur_year]\n",
    "    return cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def journal_preprocess(df):\n",
    "#     journals = popular_journals(df)[['journal', 'sum']]\n",
    "#     journals_citations = df[['journal', 'total']].groupby(['journal'])\\\n",
    "#             .agg({'total': 'sum'}).reset_index().rename(columns={'total':'journal_citations'})\n",
    "    \n",
    "#     return journals, journals_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# journals, journals_citations = journal_preprocess(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def journal_rank_and_mean(df):\n",
    "    journals_citations_years = pd.DataFrame()\n",
    "    for year in range(start_year, end_year):\n",
    "        journals_citations_cur = df[df.year <= year][['journal', 'total']]\\\n",
    "                                                .groupby(['journal'])\\\n",
    "                                                .agg({'total': 'mean'}).reset_index()\\\n",
    "                                                .rename(columns={'total':'journal_citations'})\n",
    "        \n",
    "        journals_citations_cur['journal'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "        journals_citations_cur.dropna(subset=['journal'], inplace=True)\n",
    "        \n",
    "        journals_citations_cur['rank'] = journals_citations_cur['journal_citations'].rank(ascending=False, \n",
    "                                                                                          method='min')\n",
    "        journals_citations_cur['year'] = year\n",
    "        journals_citations_years = pd.concat([journals_citations_years, journals_citations_cur], axis=0)\n",
    "        \n",
    "    return journals_citations_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_journals_graph(df, cit_df):\n",
    "    with_journal_out = pd.merge(cit_df, df[['id', 'journal']], left_on='id_out', right_on='id')\\\n",
    "                                .rename(columns={'journal': 'journal_out'})\\\n",
    "                                .drop(columns=['id'])\n",
    "    \n",
    "    journal_edges = pd.merge(with_journal_out, df[['id', 'journal']], left_on='id_in', right_on='id')\\\n",
    "                                .rename(columns={'journal': 'journal_in'})\\\n",
    "                                .drop(columns=['id'])[['journal_out', 'journal_in']]\n",
    "    # clear empty journals\n",
    "    journal_edges.replace({'': np.nan}, inplace=True)\n",
    "    journal_edges.dropna(inplace=True)\n",
    "    \n",
    "    journal_edges = journal_edges.groupby(['journal_out', 'journal_in']).size().reset_index(name='weight')\n",
    "    \n",
    "    # build graph\n",
    "    journal_graph = nx.from_pandas_edgelist(journal_edges, 'journal_out', 'journal_in', 'weight')\n",
    "    \n",
    "    return journal_graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_journals = {}\n",
    "pagerank_journals_df = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Count pagerank of each journal for year {year}\")\n",
    "    journals_graph = build_journals_graph(arxiv_df_years[arxiv_df_years.year <= year], analyzer.cit_df)\n",
    "    pagerank_journals[year] = nx.pagerank(journals_graph, alpha=0.85)\n",
    "    pagerank_journals_df[year] = pd.DataFrame([pagerank_journals[year]]).transpose().reset_index()\n",
    "    pagerank_journals_df[year].columns = ['journal', 'pagerank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years = journal_rank_and_mean(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_authors_graph(df, cit_df):\n",
    "    authors_df = df[['authors', 'id']]\n",
    "    authors_df['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    authors_df.dropna(subset=['authors'], inplace=True)\n",
    "    \n",
    "    authors_df = split_df_list(authors_df, target_column='authors', separator=', ')\\\n",
    "                        .rename(columns={'authors':'author'})\n",
    "    \n",
    "    with_author_out = pd.merge(cit_df, authors_df, left_on='id_out', right_on='id')\\\n",
    "                                .rename(columns={'author': 'author_out'})\\\n",
    "                                .drop(columns=['id'])\n",
    "    \n",
    "    author_edges = pd.merge(with_author_out, authors_df, left_on='id_in', right_on='id')\\\n",
    "                                .rename(columns={'author': 'author_in'})\\\n",
    "                                .drop(columns=['id'])[['author_out', 'author_in']]\n",
    "    \n",
    "    # clear empty authors\n",
    "    author_edges.replace({'': np.nan}, inplace=True)\n",
    "    author_edges.dropna(inplace=True)\n",
    "    \n",
    "    author_edges = author_edges.groupby(['author_out', 'author_in']).size().reset_index(name='weight')\n",
    "    \n",
    "    # build graph\n",
    "    author_graph = nx.from_pandas_edgelist(author_edges, 'author_out', 'author_in', 'weight')\n",
    "    \n",
    "    return author_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count pagerank for graph of authors citation and productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_graph_features = {}\n",
    "for year in range(start_year, end_year):\n",
    "    logging.info(f\"Started counting graph of authors citations for year {year}\")\n",
    "    authors_graph = build_authors_graph(arxiv_df_years[arxiv_df_years.year <= year], analyzer.cit_df)\n",
    "    author_graph_features[year] = nx.pagerank(authors_graph, alpha=0.85)\n",
    "    \n",
    "    author_graph_features[year] = {k: {'pagerank': v, 'productivity': 0} \n",
    "                                   for k, v in author_graph_features[year].items()}\n",
    "    for author, _, weight in authors_graph.selfloop_edges(data='weight'):\n",
    "        author_graph_features[year][author]['productivity'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_features(df):\n",
    "    author_total = df[['authors', 'total', 'year']]\n",
    "    author_total['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "    author_total.dropna(subset=['authors'], inplace=True)\n",
    "    author_total['co_authors'] = author_total['authors'].apply(lambda authors: len(authors.split(', ')) - 1)\n",
    "    \n",
    "    author_total = split_df_list(author_total, target_column='authors', separator=', ')\n",
    "    \n",
    "    authors_dict_years = {}\n",
    "    for year in range(start_year, end_year):\n",
    "        logging.info(f\"Started counting authors ranks and mean number citations for year {year}\")\n",
    "        authors_citations_groupped = author_total[author_total.year <= year].groupby(['authors'])\n",
    "        authors_citations = authors_citations_groupped.agg({'total': ['mean', hindex, gindex], 'co_authors': 'mean'}).reset_index()\n",
    "        authors_citations.columns = authors_citations.columns.droplevel(level=1)\n",
    "        authors_citations.columns = ['author', 'total', 'hindex', 'gindex', 'co_authors']\n",
    "\n",
    "        authors_citations = authors_citations.loc[authors_citations['author'] != '']\n",
    "        authors_citations['rank'] = authors_citations['total'].rank(ascending=False, method='min')\n",
    "        cur_authors_dict = authors_citations.set_index('author')[['total', 'rank', 'hindex', 'gindex', 'co_authors']]\\\n",
    "                                                            .to_dict(orient='index')\n",
    "\n",
    "        authors_dict_years[year] = cur_authors_dict\n",
    "\n",
    "    return authors_dict_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_dict_years = author_features(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None, None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    default_features = {'rank': None, 'hindex': None, 'gindex': None, 'co_authors': None}\n",
    "    features_of_given_authors = list(map(lambda author: authors_dict_years[year][author] \n",
    "                                    if author in authors_dict_years[year] else default_features,\n",
    "                                    authors_list))\n",
    "    \n",
    "    ranks = filter(None.__ne__, map(lambda features: features['rank'], features_of_given_authors))\n",
    "    hindexes = filter(None.__ne__, map(lambda features: features['hindex'], features_of_given_authors))\n",
    "    gindexes = filter(None.__ne__, map(lambda features: features['gindex'], features_of_given_authors))\n",
    "    socialities = filter(None.__ne__, map(lambda features: features['co_authors'], features_of_given_authors))\n",
    "\n",
    "    return pd.Series([mean(ranks), mean(hindexes), mean(gindexes), mean(socialities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_graph_features(row):\n",
    "    if not row.authors:\n",
    "        return pd.Series([None, None])\n",
    "    year = row['year']\n",
    "    authors_list = row['authors'].split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "        \n",
    "    default_features = {'pagerank': np.nan, 'productivity': np.nan}\n",
    "    features_of_given_authors = list(map(lambda author: author_graph_features[year][author] \n",
    "                                    if author in author_graph_features[year] else default_features,\n",
    "                                    authors_list))\n",
    "    \n",
    "    pageranks = filter(None.__ne__, map(lambda features: features['pagerank'], features_of_given_authors))\n",
    "    productivities = filter(None.__ne__, map(lambda features: features['productivity'], features_of_given_authors))\n",
    "    return pd.Series([mean(pageranks), mean(productivities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def authors_preprocess(df):\n",
    "#     authors = popular_authors(df)[['author', 'sum']]\n",
    "#     authors_papers_dict = authors.set_index('author')['sum'].to_dict()\n",
    "    \n",
    "#     author_total = df[['authors', 'total']]\n",
    "#     author_total['authors'].replace({'': np.nan, -1: np.nan}, inplace=True)\n",
    "#     author_total.dropna(subset=['authors'], inplace=True)\n",
    "#     author_total = split_df_list(author_total, target_column='authors', separator=', ')\n",
    "#     authors_citations = author_total.groupby(['authors']).agg({'total': 'sum'}).reset_index()\n",
    "#     authors_citations = authors_citations.loc[authors_citations['authors'] != ''].rename(columns={'authors':'author'})\n",
    "#     authors_dict = authors_citations.set_index('author')['total'].to_dict()\n",
    "#     return authors_papers_dict, authors_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors_papers_dict, authors_dict = authors_preprocess(arxiv_df_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_papers(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    papers_of_given_authors = list(map(lambda author: authors_papers_dict[author] \n",
    "                                    if author in authors_papers_dict else 1,\n",
    "                                   authors_list))\n",
    "\n",
    "    return pd.Series([mean(papers_of_given_authors), max(papers_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_citations(authors_str):\n",
    "    authors_list = authors_str.split(', ')\n",
    "    if len(authors_list) > 10:\n",
    "        authors_list = authors_list[:10] + [authors_list[-1]]\n",
    "    cit_of_given_authors = list(map(lambda author: authors_dict[author] if author in authors_dict else 0,\n",
    "                                    authors_list))\n",
    "\n",
    "    return pd.Series([mean(cit_of_given_authors), max(cit_of_given_authors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_citations_years.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_as_in_paper(df2, step=5, current_year=2019):\n",
    "    df = df2.copy()\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['recency'] = current_year - df['year']\n",
    "    final_features = ['recency', 'topic_rank', 'diversity']\n",
    "    \n",
    "#   early citations (cumulative)\n",
    "    for i in range(1, step + 2):\n",
    "        feature_name = f'c{i}'\n",
    "        df[feature_name] = df.apply(lambda row: citations_after_n_years(row, n=i), axis=1)\n",
    "        \n",
    "    logging.info(\"Done counting early citations\")\n",
    "    \n",
    "    \n",
    "    features_to_add = ['authors_mean_rank', 'authors_mean_hindex', 'authors_mean_gindex', 'authors_mean_sociality']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_features(row), axis=1)\n",
    "    \n",
    "    logging.info(\"Done counting author rank, h-indexes, g-indexes and sociality\")\n",
    "\n",
    "    features_to_add = ['authors_mean_pagerank', 'authors_mean_productivity']\n",
    "    final_features += features_to_add\n",
    "    df[features_to_add] = df[['authors', 'year']].apply(lambda row: get_authors_graph_features(row), axis=1)\n",
    "    \n",
    "    logging.info(\"Done counting author pagerank and productivity\")\n",
    "    \n",
    "    df = pd.merge(df, pagerank_journals_df[year], on='journal', how='left')\\\n",
    "                                        .rename(columns={'pagerank': 'journal_pagerank'})\n",
    "    df = pd.merge(df, journals_citations_years[['journal', 'rank', 'year']], on=['journal', 'year'], how='left')\\\n",
    "                                            .rename(columns={'rank': 'journal_rank'})\n",
    "    final_features += ['journal_pagerank', 'journal_rank']\n",
    "    logging.info(\"Done counting rank and pagerank of each journal\")\n",
    "\n",
    "#   extra features\n",
    "    df['title_len'] = df['title'].apply(lambda title: 0 if pd.isnull(title) else len(title))\n",
    "    df['abstract_len'] = df['abstract'].apply(lambda abstract: 0 if pd.isnull(abstract) else len(abstract))\n",
    "    df['n_authors'] = df['authors'].apply(lambda authors: len(authors.split(', ')))\n",
    "    final_features += ['title_len', 'abstract_len', 'n_authors']\n",
    "\n",
    "    final_targets = ['c1', 'c5']\n",
    "    \n",
    "    return df[final_features + final_targets], final_features, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, features, targets = preprocess_as_in_paper(arxiv_df_years[arxiv_df_years.year >= 1995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_features = ['recency', 'topic_rank', 'diversity', 'authors_mean_rank', 'authors_mean_hindex',\n",
    "                'authors_mean_sociality', 'authors_mean_pagerank', 'authors_mean_productivity',\n",
    "                'journal_pagerank', 'journal_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version of preprocessing, contains features that are not used in current preprocessing\n",
    "# def preprocess(df2, step=5, current_year=2019):\n",
    "#     df = df2.copy()\n",
    "#     -df['n_authors'] = df['authors'].apply(lambda authors: len(authors.split(', ')))\n",
    "#     df['year'] = df['year'].astype(int)\n",
    "#     df['recency'] = current_year - df['year']\n",
    "#     -df['title_len'] = df['title'].apply(lambda title: 0 if pd.isnull(title) else len(title))\n",
    "#     -df['abstract_len'] = df['abstract'].apply(lambda abstract: 0 if pd.isnull(abstract) else len(abstract))\n",
    "    \n",
    "# #   early citations (deltas)\n",
    "#     for i in range(1, step + 1):\n",
    "#         df[f'd{i}'] = df[f'c{i + 1}'] - df[f'c{i}'] \n",
    "        \n",
    "#     logging.info(\"Done counting early citations\")\n",
    "    \n",
    "# #   how many papers were published by author/journal that published this paper \n",
    "#     df = pd.merge(df, journals, on='journal', how='left').rename(columns={'sum':'journal_papers'})\n",
    "#     logging.info(\"Done counting how many papers were published by the journal\")\n",
    "    \n",
    "#     df[['author_mean_papers','author_max_papers']] = df['authors']\\\n",
    "#         .apply(lambda authors: get_authors_papers(authors))\n",
    "    \n",
    "#     logging.info(\"Done counting how many papers were published by each of the authors\")\n",
    "\n",
    "# #   how many citations journal that published this paper has\n",
    "#     df = pd.merge(df, journals_citations, on='journal', how='left')\n",
    "    \n",
    "#     logging.info(\"Done counting how cited each journal was\")\n",
    "\n",
    "# #   how many citations author of this paper got (mean/max)\n",
    "#     df[['author_mean_citations', 'author_max_citations']] = df['authors']\\\n",
    "#         .apply(lambda authors: get_authors_citations(authors))\n",
    "#     logging.info(\"Done counting how cited each author was\")\n",
    "    \n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.hist(figsize=(40, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur_features = list(test_df.columns[:-2]) # all\n",
    "# logged_features = []\n",
    "# for f in cur_features:\n",
    "#     if test_df[f].min(skipna=True) >= 0:\n",
    "#         test_df['log_' + f] = test_df[f].apply(lambda x: np.log(x + 1))\n",
    "#         logged_features.append('log_' + f)\n",
    "#         if f in ten_features:\n",
    "#             logged_ten_features.append('log_' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_log = ['authors_mean_pagerank', 'authors_mean_sociality']\n",
    "logged_features = []\n",
    "for f in features_to_log:\n",
    "    if test_df[f].min(skipna=True) >= 0:\n",
    "        test_df['log_' + f] = test_df[f].apply(lambda x: np.log1p(x))\n",
    "        logged_features.append('log_' + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with different features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df2, features, target, model=LinearRegression(), show_plt=False, log_target=False, n=1):\n",
    "    df = df2[features + [target]].copy()\n",
    "    df = df.astype(np.float32)\n",
    "        \n",
    "    print(\"start fill nans\")\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp.fit(df)\n",
    "    df = pd.DataFrame(imp.transform(df))\n",
    "    df.columns = features + [target]\n",
    "    print(\"tranform done\")\n",
    "    \n",
    "    train_validate = df[df.recency > step + 1][features + [target]]\n",
    "    train = train_validate[train_validate.recency > 11] \n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "        \n",
    "    coefs = []\n",
    "    r_squared = []\n",
    "    rmse = []\n",
    "    for i in range(n):\n",
    "        frac = 0.8\n",
    "        train_sample = train.sample(frac=frac)\n",
    "        X = train_sample.iloc[:,:-1]\n",
    "        y = train_sample.iloc[:,-1]\n",
    "        val_sample = validate.sample(frac=frac)\n",
    "        X_validate = val_sample.iloc[:,:-1]\n",
    "        y_validate = val_sample.iloc[:,-1]\n",
    "\n",
    "        if log_target:\n",
    "            y = np.log(y + 1)\n",
    "            y_validate = np.log(y_validate + 1)\n",
    "\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        X_validate = scaler.transform(X_validate)\n",
    "\n",
    "        reg = model.fit(X, y)\n",
    "        coefs.append(reg.coef_)\n",
    "        \n",
    "        if show_plt:\n",
    "            x = []\n",
    "            for a, b in zip(list(y_validate), list(reg.predict(X_validate))):\n",
    "                if a != 0:\n",
    "                    x.append(b - a)\n",
    "            plt.hist(x, bins=50)\n",
    "            plt.show()\n",
    "\n",
    "        r_squared.append((reg.score(X, y), reg.score(X_validate, y_validate)))\n",
    "        rmse.append((sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate))))\n",
    "        print(f\"R^2 train: {reg.score(X, y)} validate: {reg.score(X_validate, y_validate)}\")\n",
    "        print(f\"RMSE train: {sqrt(mse(reg.predict(X), y))} validate: {sqrt(mse(reg.predict(X_validate), y_validate))}\")\n",
    "    \n",
    "    return reg, (X, y), (X_validate, y_validate), (coefs, r_squared, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_influencers(df, features, reg, coefs=None, n=10):\n",
    "    if coefs:\n",
    "        maxcoef = np.argsort(-np.abs(coefs))\n",
    "        coef = np.array(coefs)[maxcoef]\n",
    "    else:\n",
    "        maxcoef = np.argsort(-np.abs(reg.coef_))\n",
    "        coef = reg.coef_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predict c5 given c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'c5'\n",
    "all_features = features + ['c1'] #+ logged_features\n",
    "all_features.remove('authors_mean_gindex')\n",
    "reg, (X, y), (X_val, y_val), extras = predict(test_df, all_features, target, model=LassoCV(cv=5), show_plt=True, log_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_influencers(test_df, all_features, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reg.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Predict c_5 without any early citations info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.remove('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, (X, y), (X_val, y_val), extras = predict(test_df, all_features, 'c5', show_plt=True, log_target=True)\n",
    "print_top_influencers(test_df, features + squared_features, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or with regularisation (L1 or L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lasso regularisation; target {target}\")\n",
    "Ls, (X, y), (X_val, y_val), extras = predict(test_df, all_features, target, model=LassoCV(cv=5), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ls.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ridge regularisation; target {target}\")\n",
    "Rr, (X, y), (X_val, y_val), extras = predict(test_df, all_features, target, model=RidgeCV(), log_target=True)\n",
    "print_top_influencers(test_df, all_features, Rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_features = all_features\n",
    "# for f in logged_features:\n",
    "#     mixed_features.remove(f[4:])\n",
    "#     mixed_features.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg, (X, y), (X_val, y_val), (coefs, r_squared, rmse) = predict(test_df, features, target, \n",
    "                                                                model=LassoCV(cv=5, tol=0.4), log_target=True, n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(features)):\n",
    "    print(features[j])\n",
    "    plt.hist(np.transpose(coefs)[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_coef = []\n",
    "for j in range(len(features)):\n",
    "    average_coef.append(mean(np.transpose(coefs)[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = print_top_influencers(test_df, features, reg, coefs=average_coef, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression:\n",
    "    def __init__(self, coef=[]):\n",
    "        self.coef = coef\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = preprocessing.scale(X)\n",
    "        self.y = y\n",
    "        self.coef = []\n",
    "        b = sum(y) / len(y)\n",
    "        a = np.dot(np.dot(np.linalg.pinv(np.dot(self.X.transpose(), self.X)), self.X.transpose()), y)\n",
    "        self.coef = [b] + a.tolist()\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted_y = np.dot(x, self.coef[1:]) + self.coef[0]\n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = mean(np.log1p(test_df['c5']))\n",
    "all_average_coef = [b] + average_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_features)):\n",
    "    if abs(average_coef[i]) < 0.002:\n",
    "        average_coef[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validate(df2):\n",
    "    df = df2[features + [target]].copy()\n",
    "    df = df.astype(np.float32)\n",
    "    print(\"start fill nans\")\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    imp.fit(df)\n",
    "    df = pd.DataFrame(imp.transform(df))\n",
    "    df.columns = features + [target]\n",
    "\n",
    "    print(\"tranform done\")\n",
    "    \n",
    "    train_validate = df[df.recency > step + 1][features + [target]]\n",
    "    train = train_validate[train_validate.recency > 11] \n",
    "    validate = train_validate[train_validate.recency <= 11]\n",
    "    return train, validate\n",
    "    \n",
    "train, validate = get_train_validate(test_df)\n",
    "X = train.iloc[:,:-1]\n",
    "y = np.log1p(train.iloc[:,-1])\n",
    "X_validate = validate.iloc[:,:-1]\n",
    "y_validate = np.log1p(validate.iloc[:,-1])\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_validate = scaler.transform(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_average_coef[0] = mean(y)\n",
    "reg_test = linear_regression(coef=all_average_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = reg_test.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y, reg_test.predict(X)), r2_score(y_validate, reg_test.predict(X_validate)))\n",
    "print(r2_score(np.exp(y), np.exp(reg_test.predict(X))), r2_score(np.exp(y_validate), np.exp(reg_test.predict(X_validate))))\n",
    "print(sqrt(mse(reg.predict(X), y)), sqrt(mse(reg.predict(X_validate), y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reg_test.predict(X_val), y_val, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use desicion tree instead of linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df[all_features + ['c5']].astype(np.float32)\n",
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start fill nans\")\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(df)\n",
    "df = pd.DataFrame(imp.transform(df))\n",
    "df.columns = columns\n",
    "\n",
    "print(\"tranform done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate = df[df.recency > step + 1][all_features + [target]] \n",
    "train = train_validate[train_validate.recency > 11] \n",
    "validate = train_validate[train_validate.recency <= 11]\n",
    "\n",
    "train_sample = train.sample(frac=0.7)\n",
    "X = train_sample.iloc[:,:-1]\n",
    "y = np.log(train_sample.iloc[:,-1] + 1)\n",
    "validate_sample = validate.sample(frac=0.7)\n",
    "X_validate = validate_sample.iloc[:,:-1]\n",
    "y_validate = np.log(validate_sample.iloc[:,-1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=6, min_samples_split=40, n_estimators=1000, n_jobs=-1, verbose=4)\n",
    "regr.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"RMSE train: {sqrt(mse(regr.predict(X), y))} validate: {sqrt(mse(regr.predict(X_validate), y_validate))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2: train {regr.score(X, y)} validate: {regr.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(np.exp(y_validate), np.exp(regr.predict(X_validate))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regr.predict(X_validate), y_validate, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"R^2: train {r2_score(y, regr.predict(X))} validate: {r2_score(y_validate, regr.predict(X_validate))}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = print_top_influencers_tree(test_df, features, regr, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_influencers_tree(df, features, reg, n=10):\n",
    "    maxcoef = np.argsort(-np.abs(reg.feature_importances_))\n",
    "    coef = reg.feature_importances_[maxcoef]\n",
    "    top_features = []\n",
    "    for i in range(0, min(n, len(features))):\n",
    "        print(\"{:.<060} {:< 010.4e}\".format(df[features].columns[maxcoef[i]], coef[i]))\n",
    "        top_features.append(df[features].columns[maxcoef[i]])\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_regressor = RandomForestRegressor()\n",
    "\n",
    "parameters = {'max_features':np.arange(5,15),'n_estimators': [300],'min_samples_leaf': (10, 20, 30), 'max_depth':np.arange(5,10)}\n",
    "\n",
    "random_grid = GridSearchCV(random_regressor, parameters, cv=5, verbose=4, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_rbf = SVR(kernel='rbf', C=10, gamma='auto', epsilon=.1, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr_lin = SVR(kernel='linear', C=100, gamma='auto', verbose=4)\n",
    "# svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = vr_rbf.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_rbf_val = vr_rbf.predict(X_validate)\n",
    "print(r2_score(y_validate, y_predicted_rbf_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "mi = mutual_info_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxcoef = np.argsort(-np.abs(mi))\n",
    "coef = np.array(mi)[maxcoef]\n",
    "\n",
    "top_features = []\n",
    "for i in range(0, min(30, len(coef))):\n",
    "    print(\"{:.<060} {:< 010.4e}\".format(str(X.columns[maxcoef[i]]), coef[i]))\n",
    "    top_features.append(X.columns[maxcoef[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostRegressor, CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Pool(data=X, label=y)\n",
    "eval_dataset = Pool(data=X_validate, label=y_validate)\n",
    "model = CatBoostRegressor(iterations=1400, use_best_model=True, learning_rate=0.02, max_depth=6, loss_function='RMSE')\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          use_best_model=True,\n",
    "          eval_set=eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2: {model.score(X, y)} validate: {model.score(X_validate, y_validate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.predict(X_validate), y_validate, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.get_feature_importance(data=train_dataset,\n",
    "                       prettified=True,\n",
    "                       thread_count=-1,\n",
    "                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = importances.sort_values(by=['Importances'], ascending=False)\n",
    "importances_df['Feature name'] = importances_df['Feature Id'].apply(lambda id: features[int(id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(train, validate):\n",
    "    X = train[features + ['is_top']].iloc[:,:-1]\n",
    "    y = train[features + ['is_top']].iloc[:,-1]\n",
    "    X_validate = validate[features + ['is_top']].iloc[:,:-1]\n",
    "    y_validate = validate[features + ['is_top']].iloc[:,-1]\n",
    "\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X_validate = scaler.transform(X_validate)\n",
    "    return X, y, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best = train.nlargest(columns=['c5'], n = int(part * train.shape[0]))\n",
    "train_min = train_best['c5'].min()\n",
    "validate_best = validate.nlargest(columns=['c5'], n = int(part * validate.shape[0]))\n",
    "val_min = validate_best['c5'].min()\n",
    "train['is_top'] = train['c5'].apply(lambda x: 1 if x > train_min else 0)\n",
    "validate['is_top'] = validate['c5'].apply(lambda x: 1 if x > val_min else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best.groupby(by=['recency'])['c5'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = train[train['is_top'] == 1]\n",
    "not_top = train[train['is_top'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"top size: {top.shape[0]} not top size: {not_top.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_top_downsampled = resample(not_top, replace = False, n_samples = len(top), random_state = 27)\n",
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([not_top_downsampled, top])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_upsampled = resample(top, replace=True, n_samples=len(not_top), random_state=27) \n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_top, top_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [downsampled, upsampled]:\n",
    "    X, y, X_validate, y_validate = train_validate(dataset, validate)\n",
    "    target = 'is_top'\n",
    "    for weight in ([1, 3, 5]):\n",
    "        print(\"weight =\", weight)\n",
    "        clf = RandomForestClassifier(max_depth=7, n_estimators=1000, class_weight={0: 1, 1: weight})\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        print(\"score train: \", clf.score(X, y), \"validate :\", clf.score(X_validate, y_validate))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_validate, clf.predict(X_validate)).ravel()\n",
    "        print(tn, fp, fn, tp)\n",
    "        print(\"recall :\", recall_score(y_validate,  clf.predict(X_validate)))\n",
    "        print(\"precision :\", precision_score(y_validate,  clf.predict(X_validate)))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
