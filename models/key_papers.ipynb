{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key papers\n",
    "\n",
    "This Jupyter Notebook can be used to perform basic publication analysis for a science branch. \n",
    "\n",
    "**Features:**\n",
    "\n",
    "1. Subtopic analysis based on co-citation graph clustering:\n",
    "    * Chord diagram for co-citation graph\n",
    "    * Comparison of subtopics by size\n",
    "    * Timeline of each subtopic\n",
    "    * Extraction of 1,2,3-grams describing each subtopic\n",
    "2. Detection of highlight papers:\n",
    "    * Top cited papers overall\n",
    "    * Detection of most cited papers for each year\n",
    "    * Detection of papers with max relative citation gain for each year\n",
    "3. Citation dynamics visualization for highlight papers\n",
    "4. Subtopic evolution tracking based on co-citation graph clustering for different time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "1. Define the `SEARCH_TERMS` variable in the cell below with a list of keywords that describe the science branch of your interest.\n",
    "2. Run all cells & see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T11:13:20.757962Z",
     "start_time": "2019-05-20T11:13:19.650470Z"
    }
   },
   "outputs": [],
   "source": [
    "SEARCH_TERMS = ['dna', 'methylation', 'clock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T11:13:45.558630Z",
     "start_time": "2019-05-20T11:13:20.941831Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from bokeh.plotting import show, output_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keypaper.pm_loader import PubmedLoader\n",
    "from keypaper.analysis import KeyPaperAnalyzer\n",
    "from keypaper.visualization import Plotter\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "output_notebook()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:33.794284Z",
     "start_time": "2019-05-20T11:13:45.591588Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analyzer = KeyPaperAnalyzer(PubmedLoader(test=False))\n",
    "log = analyzer.launch(*SEARCH_TERMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:35.056256Z",
     "start_time": "2019-05-20T12:00:34.356360Z"
    }
   },
   "outputs": [],
   "source": [
    "plotter = Plotter(analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtopics a.k.a. Clusters in the Co-citation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:45.632292Z",
     "start_time": "2019-05-20T12:00:35.064248Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(plotter.chord_diagram_components())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:00:48.074797Z",
     "start_time": "2019-05-20T12:00:45.760212Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(plotter.component_size_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:26.334327Z",
     "start_time": "2019-05-20T12:00:48.085780Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in plotter.subtopic_timeline_graphs():\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(plotter.component_ratio())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Cited Papers Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:27.394835Z",
     "start_time": "2019-05-20T12:01:26.342323Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show(plotter.top_cited_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Cited Papers for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:28.107789Z",
     "start_time": "2019-05-20T12:01:27.588861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show(plotter.max_gain_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top by Relative Gain for Each Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:28.573501Z",
     "start_time": "2019-05-20T12:01:28.118778Z"
    }
   },
   "outputs": [],
   "source": [
    "show(plotter.max_relative_gain_papers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation per Year Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:31.239272Z",
     "start_time": "2019-05-20T12:01:28.583495Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotter.article_citation_dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.562979Z",
     "start_time": "2019-05-20T12:15:27.328Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:31.263273Z",
     "start_time": "2019-05-20T12:01:31.256277Z"
    }
   },
   "outputs": [],
   "source": [
    "# plt = plotter.subtopic_evolution()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:01:41.155189Z",
     "start_time": "2019-05-20T12:01:31.273261Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:02:46.342768Z",
     "start_time": "2019-05-20T12:01:41.190167Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer.subtopic_evolution_analysis(step=2)\n",
    "evol = plotter.analyzer.evolution_df.copy()\n",
    "for col in evol:\n",
    "    evol[col] = evol[col].apply(lambda x: math.floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:02:49.166657Z",
     "start_time": "2019-05-20T12:02:46.353758Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = evol.columns[1:]\n",
    "pairs = list(zip(cols, cols[1:]))\n",
    "nodes = []\n",
    "edges = []\n",
    "\n",
    "for now, then in pairs:\n",
    "    nodes_now = [f'{now} {c}' for c in evol[now].unique()]\n",
    "    nodes_then = [f'{then} {c}' for c in evol[then].unique()]\n",
    "    inner = {node : 0 for node in nodes_then}\n",
    "    changes = {node : inner.copy() for node in nodes_now}\n",
    "    for pmid, comp in evol.iterrows():\n",
    "        c_now, c_then = comp[now], comp[then]\n",
    "        changes[f'{now} {c_now}'][f'{then} {c_then}'] += 1\n",
    "    \n",
    "#     nodes.extend(nodes_now)\n",
    "    for v in nodes_now:\n",
    "        for u in nodes_then:\n",
    "            if changes[v][u] > 0:\n",
    "                edges.append([v, u, changes[v][u]])\n",
    "    \n",
    "#     edges.append([str(now), str(then), 1])\n",
    "    \n",
    "# nodes.extend(nodes_then)\n",
    "    \n",
    "# print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:02:52.682050Z",
     "start_time": "2019-05-20T12:02:49.182634Z"
    }
   },
   "outputs": [],
   "source": [
    "sankey = hv.Sankey(edges)\n",
    "sankey.opts(width=960, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank for Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.548987Z",
     "start_time": "2019-05-20T12:15:08.630Z"
    }
   },
   "outputs": [],
   "source": [
    "analyzer.load_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.552985Z",
     "start_time": "2019-05-20T12:15:08.834Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Apply PageRank algorithm with damping factor of 0.5\n",
    "pr_nx = nx.pagerank(analyzer.G, alpha=0.5, tol=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.078073Z",
     "start_time": "2019-05-20T11:13:20.732Z"
    }
   },
   "outputs": [],
   "source": [
    "ancestor = dict.fromkeys(analyzer.G, (0, 0))\n",
    "\n",
    "# Select ancestor with highest PR for each node\n",
    "for v in analyzer.G:\n",
    "    for u in analyzer.G[v]:\n",
    "        anc, pr = ancestor[u]\n",
    "        if pr_nx[v] > pr:\n",
    "            ancestor[u] = (v, pr_nx[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.118051Z",
     "start_time": "2019-05-20T11:13:20.740Z"
    }
   },
   "outputs": [],
   "source": [
    "PRG = nx.DiGraph()\n",
    "for v, anc in ancestor.items():\n",
    "    u, pr = anc\n",
    "    if pr > 0:\n",
    "        PRG.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.145031Z",
     "start_time": "2019-05-20T11:13:20.832Z"
    }
   },
   "outputs": [],
   "source": [
    "start, end = zip(*list(PRG.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.179011Z",
     "start_time": "2019-05-20T11:13:25.347Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Circle, HoverTool, MultiLine\n",
    "from bokeh.models.graphs import NodesAndLinkedEdges\n",
    "\n",
    "node_indices = list(filter(lambda node: len(analyzer.df[analyzer.df['pmid'] == node]) > 0, list(PRG.nodes())))\n",
    "\n",
    "years = []\n",
    "year_counts = {}\n",
    "pageranks = []\n",
    "size = []\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['pmid'] == node]['year'].values[0]\n",
    "    if not year in year_counts:\n",
    "        year_counts[year] = 1\n",
    "    else:\n",
    "        year_counts[year] += 1\n",
    "    years.append(year)\n",
    "    pageranks.append(pr_nx[node] * 100)\n",
    "    size.append(pr_nx[node] * 1000)\n",
    "max_year_count = max(list(year_counts.values()))\n",
    "min_year, max_year = min(years), max(years)\n",
    "\n",
    "plot = figure(title=\"PageRank applied to citation filtering\", \n",
    "              x_range=(min_year - 1, max_year+1), y_range=(0, max_year_count + 1),\n",
    "              tools=\"\", toolbar_location=None)\n",
    "\n",
    "plot.add_tools(HoverTool(tooltips=[(\"PMID\", \"@pmid\"), (\"Year\", \"@year\"), (\"PageRank\", \"@pagerank %\")]))\n",
    "\n",
    "graph = GraphRenderer()\n",
    "\n",
    "graph.node_renderer.data_source.add(node_indices, 'index')\n",
    "graph.node_renderer.data_source.data['pmid'] = node_indices\n",
    "graph.node_renderer.data_source.data['year'] = years\n",
    "graph.node_renderer.data_source.data['pagerank'] = pageranks\n",
    "graph.node_renderer.data_source.data['size'] = size\n",
    "graph.edge_renderer.data_source.data = dict(start=start, end=end)\n",
    "\n",
    "### start of layout code   \n",
    "x = [analyzer.df[analyzer.df['pmid'] == pmid]['year'].values[0] for pmid in node_indices]\n",
    "y = []\n",
    "tmp_year_counts = {}\n",
    "for node in node_indices:\n",
    "    year = analyzer.df[analyzer.df['pmid'] == node]['year'].values[0]\n",
    "    if not year in tmp_year_counts:\n",
    "        tmp_year_counts[year] = 1\n",
    "    else:\n",
    "        tmp_year_counts[year] += 1\n",
    "    y.append(tmp_year_counts[year])\n",
    "\n",
    "graph_layout = dict(zip(node_indices, zip(x, y)))\n",
    "graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "graph.node_renderer.glyph = Circle(size='size', fill_color='blue')\n",
    "graph.node_renderer.hover_glyph = Circle(size='size', fill_color='green')\n",
    "\n",
    "graph.edge_renderer.glyph = MultiLine(line_color='black', line_alpha=1, line_width=1)\n",
    "graph.edge_renderer.hover_glyph = MultiLine(line_color='green', line_width=2)\n",
    "\n",
    "graph.inspection_policy = NodesAndLinkedEdges()\n",
    "\n",
    "plot.renderers.append(graph)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom PageRank version for testing\n",
    "\n",
    "This section is devoted to experiments with PageRank on the basis of networkx source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.362154Z",
     "start_time": "2019-05-20T11:13:25.396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adopted from networkx source code\n",
    "# https://networkx.github.io/documentation/networkx-1.10/_modules/networkx/algorithms/link_analysis/pagerank_alg.html#pagerank\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pagerank(G, alpha=0.85, personalization=None,\n",
    "             max_iter=100, tol=1.0e-6, nstart=None, weight='weight',\n",
    "             dangling=None):\n",
    "    \"\"\"Return the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    alpha : float, optional\n",
    "      Damping parameter for PageRank, default=0.85.\n",
    "\n",
    "    personalization: dict, optional\n",
    "      The \"personalization vector\" consisting of a dictionary with a\n",
    "      key for every graph node and nonzero personalization value for each node.\n",
    "      By default, a uniform distribution is used.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method eigenvalue solver.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "\n",
    "    nstart : dictionary, optional\n",
    "      Starting value of PageRank iteration for each node.\n",
    "\n",
    "    weight : key, optional\n",
    "      Edge data key to use as weight.  If None weights are set to 1.\n",
    "\n",
    "    dangling: dict, optional\n",
    "      The outedges to be assigned to any \"dangling\" nodes, i.e., nodes without\n",
    "      any outedges. The dict key is the node the outedge points to and the dict\n",
    "      value is the weight of that outedge. By default, dangling nodes are given\n",
    "      outedges according to the personalization vector (uniform if not\n",
    "      specified). This must be selected to result in an irreducible transition\n",
    "      matrix (see notes under google_matrix). It may be common to have the\n",
    "      dangling dict to be the same as the personalization dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pagerank : dictionary\n",
    "       Dictionary of nodes with PageRank as value\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> G = nx.DiGraph(nx.path_graph(4))\n",
    "    >>> pr = nx.pagerank(G, alpha=0.9)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The eigenvector calculation is done by the power iteration method\n",
    "    and has no guarantee of convergence.  The iteration will stop\n",
    "    after max_iter iterations or an error tolerance of\n",
    "    number_of_nodes(G)*tol has been reached.\n",
    "\n",
    "    The PageRank algorithm was designed for directed graphs but this\n",
    "    algorithm does not check if the input graph is directed and will\n",
    "    execute on undirected graphs by converting each edge in the\n",
    "    directed graph to two edges.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    pagerank_numpy, pagerank_scipy, google_matrix\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] A. Langville and C. Meyer,\n",
    "       \"A survey of eigenvector methods of web information retrieval.\"\n",
    "       http://citeseer.ist.psu.edu/713792.html\n",
    "    .. [2] Page, Lawrence; Brin, Sergey; Motwani, Rajeev and Winograd, Terry,\n",
    "       The PageRank citation ranking: Bringing order to the Web. 1999\n",
    "       http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1999-66&format=pdf\n",
    "    \"\"\"\n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "\n",
    "    if not G.is_directed():\n",
    "        D = G.to_directed()\n",
    "    else:\n",
    "        D = G\n",
    "\n",
    "    # Create a copy in (right) stochastic form\n",
    "    W = nx.stochastic_graph(D, weight=weight)\n",
    "    N = W.number_of_nodes()\n",
    "    E = W.number_of_edges()\n",
    "       \n",
    "    # Number of references for each node and average for graph\n",
    "    NR = D.out_degree(list(D.nodes()))\n",
    "    NR_avg = E * 2 / N\n",
    "\n",
    "    # Choose fixed starting vector if not given\n",
    "    if nstart is None:\n",
    "        x = dict.fromkeys(W, 1.0 / N)\n",
    "    else:\n",
    "        # Normalized nstart vector\n",
    "        s = float(sum(nstart.values()))\n",
    "        x = dict((k, v / s) for k, v in nstart.items())\n",
    "\n",
    "    if personalization is None:\n",
    "        # Assign uniform personalization vector if not given\n",
    "        p = dict.fromkeys(W, 1.0 / N)\n",
    "    else:\n",
    "        missing = set(G) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        s = float(sum(personalization.values()))\n",
    "        p = dict((k, v / s) for k, v in personalization.items())\n",
    "\n",
    "    if dangling is None:\n",
    "        # Use personalization vector if dangling vector not specified\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(G) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        s = float(sum(dangling.values()))\n",
    "        dangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "    \n",
    "    # power iteration: make up to max_iter iterations\n",
    "    n_iter = 0\n",
    "    for _ in range(max_iter):\n",
    "        n_iter += 1\n",
    "        xlast = x\n",
    "        x = dict.fromkeys(xlast.keys(), 0)\n",
    "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
    "        for n in x:\n",
    "            # this matrix multiply looks odd because it is\n",
    "            # doing a left multiply x^T=xlast^T*W\n",
    "            for nbr in W[n]:\n",
    "                x[nbr] += alpha * xlast[n] / (1 + np.sqrt(NR[n]))\n",
    "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
    "        # check convergence, l1 norm\n",
    "        err = sum([abs(x[n] - xlast[n]) for n in x])\n",
    "        if err < N*tol:\n",
    "            print(f'PageRank converged in {n_iter} iterations.')\n",
    "            return x\n",
    "\n",
    "    raise NetworkXError('pagerank: power iteration failed to converge '\n",
    "                        'in %d iterations.' % max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.370150Z",
     "start_time": "2019-05-20T11:13:25.415Z"
    }
   },
   "outputs": [],
   "source": [
    "pr = pagerank(analyzer.G, alpha=0.5, tol=1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T12:20:18.384142Z",
     "start_time": "2019-05-20T11:13:25.423Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(pr.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
